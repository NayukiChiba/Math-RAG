"""
å¿«é€Ÿæ£€ç´¢æµ‹è¯•ç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. å¿«é€Ÿè¯„ä¼°æ£€ç´¢ç³»ç»Ÿçš„å¬å›ç‡
2. æ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥å¯¹æ¯”
3. ç”Ÿæˆç®€æ´çš„è¯„æµ‹æŠ¥å‘Š
4. æ”¯æŒæŠ½æ ·æµ‹è¯•ï¼ˆæ— éœ€å…¨é‡è¯„æµ‹ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¿«é€Ÿæµ‹è¯•ï¼ˆé»˜è®¤ 20 æ¡æŸ¥è¯¢ï¼‰
    python evaluation/quickEval.py

    # æŒ‡å®šæµ‹è¯•æ•°é‡
    python evaluation/quickEval.py --num-queries 50

    # æµ‹è¯•ç‰¹å®šæ£€ç´¢æ–¹æ³•
    python evaluation/quickEval.py --methods bm25plus hybrid_plus

    # å…³é—­æŠ½æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æŸ¥è¯¢
    python evaluation/quickEval.py --all-queries
"""

import argparse
import json
import math
import os
import random
import sys
import time
from collections.abc import Callable
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# Windows ç»ˆç«¯ UTF-8 æ”¯æŒ
if sys.platform == "win32":
    import codecs

    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
    sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")

import config

# ==================== æŒ‡æ ‡è®¡ç®—å‡½æ•° ====================


def calculateRecallAtK(results: list[dict], relevantTerms: list[str], k: int) -> float:
    """è®¡ç®— Recall@K"""
    if not relevantTerms:
        return 0.0

    topkResults = results[:k]
    topkTerms = {r["term"] for r in topkResults}
    found = sum(1 for term in relevantTerms if term in topkTerms)

    return found / len(relevantTerms)


def calculateMRR(results: list[dict], relevantTerms: list[str]) -> float:
    """è®¡ç®— MRR (Mean Reciprocal Rank)"""
    for rank, result in enumerate(results, 1):
        if result["term"] in relevantTerms:
            return 1.0 / rank
    return 0.0


def calculateMAP(results: list[dict], relevantTerms: list[str]) -> float:
    """è®¡ç®— MAP (Mean Average Precision)"""
    if not relevantTerms:
        return 0.0

    precisionSum = 0.0
    hitCount = 0

    for rank, result in enumerate(results, 1):
        if result["term"] in relevantTerms:
            hitCount += 1
            precision = hitCount / rank
            precisionSum += precision

    return precisionSum / len(relevantTerms) if hitCount > 0 else 0.0


def calculateNDCG(results: list[dict], relevantTerms: list[str], k: int) -> float:
    """è®¡ç®— nDCG@K"""

    def dcg(results, k):
        score = 0.0
        for i, result in enumerate(results[:k]):
            if result["term"] in relevantTerms:
                rel = len(relevantTerms) - relevantTerms.index(result["term"])
                score += rel / math.log2(i + 2)
        return score

    def idcg(k):
        score = 0.0
        for i in range(min(k, len(relevantTerms))):
            score += (len(relevantTerms) - i) / math.log2(i + 2)
        return score

    dcgScore = dcg(results, k)
    idcgScore = idcg(k)

    return dcgScore / idcgScore if idcgScore > 0 else 0.0


# ==================== æ•°æ®åŠ è½½å‡½æ•° ====================


def loadQueries(
    filepath: str, numQueries: int | None = None, allQueries: bool = False
) -> list[dict]:
    """
    åŠ è½½æŸ¥è¯¢é›†

    Args:
        filepath: æŸ¥è¯¢æ–‡ä»¶è·¯å¾„
        numQueries: æŠ½æ ·æ•°é‡
        allQueries: ä½¿ç”¨å…¨éƒ¨æŸ¥è¯¢

    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    queries = []
    try:
        with open(filepath, encoding="utf-8") as f:
            for line in f:
                try:
                    query = json.loads(line.strip())
                    if all(k in query for k in ["query", "relevant_terms", "subject"]):
                        if (
                            isinstance(query["relevant_terms"], list)
                            and query["relevant_terms"]
                        ):
                            queries.append(query)
                except json.JSONDecodeError:
                    pass
    except FileNotFoundError:
        print(f"âŒ æŸ¥è¯¢é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼š{filepath}")
        return []

    print(f"âœ… åŠ è½½äº† {len(queries)} æ¡æŸ¥è¯¢")

    # æŠ½æ ·
    if not allQueries and numQueries and numQueries < len(queries):
        print(f"ğŸ“Š éšæœºæŠ½æ · {numQueries} æ¡æŸ¥è¯¢è¿›è¡Œæµ‹è¯•")
        random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
        queries = random.sample(queries, numQueries)

    return queries


def loadCorpus(filepath: str) -> list[dict]:
    """åŠ è½½è¯­æ–™åº“"""
    corpus = []
    try:
        with open(filepath, encoding="utf-8") as f:
            for line in f:
                corpus.append(json.loads(line.strip()))
        print(f"âœ… åŠ è½½äº† {len(corpus)} æ¡è¯­æ–™")
    except FileNotFoundError:
        print(f"âš ï¸  è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼š{filepath}")
    return corpus


# ==================== æ£€ç´¢å™¨åŠ è½½å‡½æ•° ====================


def createBM25Retriever(corpusFile: str, indexFile: str):
    """åˆ›å»º BM25 æ£€ç´¢å™¨"""
    from retrieval.retrievalBM25 import BM25Retriever

    retriever = BM25Retriever(corpusFile, indexFile)
    if not retriever.loadIndex():
        print("âš ï¸  BM25 ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
        retriever.buildIndex()
        retriever.saveIndex()
    return retriever


def createBM25PlusRetriever(corpusFile: str, indexFile: str, termsFile: str):
    """åˆ›å»º BM25+ æ£€ç´¢å™¨"""
    from retrieval.retrievalBM25Plus import BM25PlusRetriever

    retriever = BM25PlusRetriever(corpusFile, indexFile, termsFile)
    if not retriever.loadIndex():
        print("âš ï¸  BM25+ ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
        retriever.loadTermsMap()
        retriever.buildIndex()
        retriever.saveIndex()
    return retriever


def createVectorRetriever(
    corpusFile: str, indexFile: str, embeddingFile: str, model: str
):
    """åˆ›å»ºå‘é‡æ£€ç´¢å™¨"""
    from retrieval.retrievalVector import VectorRetriever

    retriever = VectorRetriever(corpusFile, model, indexFile, embeddingFile)
    if not retriever.loadIndex():
        print("âš ï¸  å‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
        retriever.buildIndex()
        retriever.saveIndex()
    return retriever


def createHybridPlusRetriever(
    corpusFile: str,
    bm25Index: str,
    vectorIndex: str,
    vectorEmbedding: str,
    termsFile: str,
    model: str,
):
    """åˆ›å»ºæ”¹è¿›çš„æ··åˆæ£€ç´¢å™¨"""
    from retrieval.retrievalHybridPlus import HybridPlusRetriever

    retriever = HybridPlusRetriever(
        corpusFile, bm25Index, vectorIndex, vectorEmbedding, model, termsFile
    )
    return retriever


# ==================== è¯„æµ‹å‡½æ•° ====================


def evaluateMethod(
    method: str,
    retriever: Any,
    queries: list[dict],
    topK: int = 10,
    searchFunc: str | Callable = "search",
    **searchKwargs,
) -> dict[str, Any]:
    """è¯„æµ‹å•ä¸ªæ£€ç´¢æ–¹æ³•"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š è¯„æµ‹æ–¹æ³•ï¼š{method}")
    print(f"{'=' * 60}")

    metrics = {
        "method": method,
        "total_queries": len(queries),
        "recall@1": [],
        "recall@3": [],
        "recall@5": [],
        "recall@10": [],
        "mrr": [],
        "map": [],
        "ndcg@3": [],
        "ndcg@5": [],
        "ndcg@10": [],
        "avg_query_time": 0.0,
    }

    queryTimes = []

    for i, query in enumerate(queries, 1):
        queryText = query["query"]
        relevantTerms = query["relevant_terms"]

        # æ‰§è¡Œæ£€ç´¢
        startTime = time.time()

        # æ”¯æŒå¯è°ƒç”¨å‡½æ•°ä½œä¸º searchFunc
        if callable(searchFunc):
            results = searchFunc(queryText, topK * 2)
        elif searchFunc == "search":
            results = retriever.search(queryText, topK * 2, **searchKwargs)
        elif searchFunc == "batchSearch":
            results = retriever.batchSearch([queryText], topK * 2, **searchKwargs).get(
                queryText, []
            )
        else:
            results = getattr(retriever, searchFunc)(
                queryText, topK * 2, **searchKwargs
            )

        endTime = time.time()
        queryTime = endTime - startTime
        queryTimes.append(queryTime)

        # è®¡ç®—æŒ‡æ ‡
        metrics["recall@1"].append(calculateRecallAtK(results, relevantTerms, 1))
        metrics["recall@3"].append(calculateRecallAtK(results, relevantTerms, 3))
        metrics["recall@5"].append(calculateRecallAtK(results, relevantTerms, 5))
        metrics["recall@10"].append(calculateRecallAtK(results, relevantTerms, 10))
        metrics["mrr"].append(calculateMRR(results, relevantTerms))
        metrics["map"].append(calculateMAP(results, relevantTerms))
        metrics["ndcg@3"].append(calculateNDCG(results, relevantTerms, 3))
        metrics["ndcg@5"].append(calculateNDCG(results, relevantTerms, 5))
        metrics["ndcg@10"].append(calculateNDCG(results, relevantTerms, 10))

        # è¿›åº¦æ˜¾ç¤º
        if i % 10 == 0 or i == len(queries):
            print(f"  è¿›åº¦ï¼š{i}/{len(queries)} ({i / len(queries) * 100:.1f}%)")

    # è®¡ç®—å¹³å‡å€¼
    metrics["avg_query_time"] = sum(queryTimes) / len(queryTimes) if queryTimes else 0.0
    metrics["avg_metrics"] = {
        "recall@1": sum(metrics["recall@1"]) / len(metrics["recall@1"]),
        "recall@3": sum(metrics["recall@3"]) / len(metrics["recall@3"]),
        "recall@5": sum(metrics["recall@5"]) / len(metrics["recall@5"]),
        "recall@10": sum(metrics["recall@10"]) / len(metrics["recall@10"]),
        "mrr": sum(metrics["mrr"]) / len(metrics["mrr"]),
        "map": sum(metrics["map"]) / len(metrics["map"]),
        "ndcg@3": sum(metrics["ndcg@3"]) / len(metrics["ndcg@3"]),
        "ndcg@5": sum(metrics["ndcg@5"]) / len(metrics["ndcg@5"]),
        "ndcg@10": sum(metrics["ndcg@10"]) / len(metrics["ndcg@10"]),
    }

    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“ˆ {method} è¯„æµ‹æ‘˜è¦:")
    print(f"  Recall@5: {metrics['avg_metrics']['recall@5']:.2%}")
    print(f"  Recall@10: {metrics['avg_metrics']['recall@10']:.2%}")
    print(f"  MRR: {metrics['avg_metrics']['mrr']:.4f}")
    print(f"  nDCG@5: {metrics['avg_metrics']['ndcg@5']:.4f}")
    print(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´ï¼š{metrics['avg_query_time']:.4f}s")

    return metrics


def runQuickEval(
    methods: list[str] | None = None,
    numQueries: int = 20,
    allQueries: bool = False,
    topK: int = 10,
) -> dict[str, Any]:
    """
    è¿è¡Œå¿«é€Ÿè¯„æµ‹

    Args:
        methods: è¯„æµ‹æ–¹æ³•åˆ—è¡¨
        numQueries: æŠ½æ ·æŸ¥è¯¢æ•°é‡
        allQueries: ä½¿ç”¨å…¨éƒ¨æŸ¥è¯¢
        topK: è¯„ä¼°çš„ TopK å€¼

    Returns:
        è¯„æµ‹æŠ¥å‘Š
    """
    print("=" * 60)
    print("ğŸš€ å¿«é€Ÿæ£€ç´¢è¯„æµ‹ç³»ç»Ÿ")
    print("=" * 60)

    # é»˜è®¤æ–¹æ³•
    if methods is None:
        methods = ["bm25", "bm25plus", "hybrid_plus"]

    # åŠ è½½æ•°æ®
    # æ³¨æ„ï¼šæŸ¥è¯¢é›†åœ¨ data/evaluation è€Œé data/processed/evaluation
    queriesFile = config.EVALUATION_DIR
    if not os.path.exists(queriesFile):
        queriesFile = os.path.join(config.PROCESSED_DIR, "evaluation")
    queriesFile = os.path.join(queriesFile, "queries.jsonl")
    corpusFile = os.path.join(config.PROCESSED_DIR, "retrieval", "corpus.jsonl")
    bm25Index = os.path.join(config.PROCESSED_DIR, "retrieval", "bm25_index.pkl")
    bm25PlusIndex = os.path.join(
        config.PROCESSED_DIR, "retrieval", "bm25plus_index.pkl"
    )
    vectorIndex = os.path.join(config.PROCESSED_DIR, "retrieval", "vector_index.faiss")
    vectorEmbedding = os.path.join(
        config.PROCESSED_DIR, "retrieval", "vector_embeddings.npz"
    )
    termsFile = os.path.join(config.PROCESSED_DIR, "terms", "all_terms.json")

    print(f"\nğŸ“‚ æŸ¥è¯¢æ–‡ä»¶ï¼š{queriesFile}")
    print(f"ğŸ“‚ è¯­æ–™æ–‡ä»¶ï¼š{corpusFile}")

    queries = loadQueries(
        queriesFile, numQueries if not allQueries else None, allQueries
    )

    if not queries:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æŸ¥è¯¢ï¼Œé€€å‡ºè¯„æµ‹")
        return {}

    # åŠ è½½è¯­æ–™ï¼ˆç”¨äºæ£€æŸ¥ï¼‰
    loadCorpus(corpusFile)

    # è¯„æµ‹æ¯ç§æ–¹æ³•
    allMetrics = {}

    for method in methods:
        if method == "bm25":
            retriever = createBM25Retriever(corpusFile, bm25Index)
            metrics = evaluateMethod("BM25", retriever, queries, topK)
        elif method == "bm25plus":
            retriever = createBM25PlusRetriever(corpusFile, bm25PlusIndex, termsFile)
            metrics = evaluateMethod(
                "BM25+", retriever, queries, topK, expandQuery=True
            )
        elif method == "vector":
            retriever = createVectorRetriever(
                corpusFile,
                vectorIndex,
                vectorEmbedding,
                "paraphrase-multilingual-MiniLM-L12-v2",
            )
            metrics = evaluateMethod("Vector", retriever, queries, topK)
        elif method == "hybrid_plus":
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜çš„ BM25 æƒé‡å’Œæ›´å¤§çš„å¬å›å› å­
            retriever = createHybridPlusRetriever(
                corpusFile,
                bm25PlusIndex,
                vectorIndex,
                vectorEmbedding,
                termsFile,
                "paraphrase-multilingual-MiniLM-L12-v2",
            )
            metrics = evaluateMethod(
                "Hybrid+",
                retriever,
                queries,
                topK,
                strategy="weighted",
                alpha=0.85,  # BM25 æƒé‡æé«˜åˆ° 0.85
                beta=0.15,  # Vector æƒé‡é™ä½åˆ° 0.15
                recallFactor=8,  # å¢åŠ å¬å›å› å­åˆ° 8
                expandQuery=False,  # ç¦ç”¨æŸ¥è¯¢æ‰©å±•
            )
        elif method == "hybrid_rrf":
            retriever = createHybridPlusRetriever(
                corpusFile,
                bm25PlusIndex,
                vectorIndex,
                vectorEmbedding,
                termsFile,
                "paraphrase-multilingual-MiniLM-L12-v2",
            )
            metrics = evaluateMethod(
                "Hybrid+RRF",
                retriever,
                queries,
                topK,
                strategy="rrf",
                recallFactor=8,  # å¢åŠ å¬å›å› å­
            )
        elif method == "advanced":
            # é«˜çº§æ£€ç´¢ï¼šä½¿ç”¨ RRF èåˆç­–ç•¥ + æ›´é«˜å¬å›
            from retrieval.retrievalHybridPlus import HybridPlusRetriever

            retriever = HybridPlusRetriever(
                corpusFile,
                bm25PlusIndex,  # ä½¿ç”¨ BM25+ ç´¢å¼•
                vectorIndex,
                vectorEmbedding,
                "paraphrase-multilingual-MiniLM-L12-v2",
                termsFile,
            )

            def advancedSearch(query, topK):
                return retriever.search(
                    query,
                    topK,
                    strategy="rrf",  # ä½¿ç”¨ RRF èåˆ
                    recallFactor=8,  # å¢åŠ å¬å›å› å­
                    expandQuery=True,
                )

            metrics = evaluateMethod(
                "Advanced", retriever, queries, topK, searchFunc=advancedSearch
            )
        else:
            print(f"âš ï¸  æœªçŸ¥æ–¹æ³•ï¼š{method}ï¼Œè·³è¿‡")
            continue

        allMetrics[method] = metrics

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„æµ‹å¯¹æ¯”æŠ¥å‘Š")
    print("=" * 60)

    print(
        f"\n{'æ–¹æ³•':<15} {'R@1':>8} {'R@3':>8} {'R@5':>8} {'R@10':>8} {'MRR':>8} {'nDCG@5':>8} {'æ—¶é—´ (s)':>8}"
    )
    print("-" * 75)

    for _, metrics in allMetrics.items():
        avg = metrics["avg_metrics"]
        timeStr = (
            f"{metrics['avg_query_time']:.3f}" if "avg_query_time" in metrics else "N/A"
        )
        print(
            f"{metrics['method']:<15} "
            f"{avg['recall@1']:.2%}  "
            f"{avg['recall@3']:.2%}  "
            f"{avg['recall@5']:.2%}  "
            f"{avg['recall@10']:.2%}  "
            f"{avg['mrr']:.4f}  "
            f"{avg['ndcg@5']:.4f}  "
            f"{timeStr:>8}"
        )

    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    bestMethod = max(
        allMetrics.keys(), key=lambda m: allMetrics[m]["avg_metrics"]["recall@5"]
    )
    print(
        f"\nğŸ† Recall@5 æœ€ä½³æ–¹æ³•ï¼š{allMetrics[bestMethod]['method']} ({allMetrics[bestMethod]['avg_metrics']['recall@5']:.2%})"
    )

    return allMetrics


def saveReport(metrics: dict[str, Any], outputFile: str) -> None:
    """ä¿å­˜è¯„æµ‹æŠ¥å‘Š"""
    dirname = os.path.dirname(outputFile)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "metrics": metrics,
    }

    with open(outputFile, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜ï¼š{outputFile}")


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¿«é€Ÿæ£€ç´¢è¯„æµ‹ç³»ç»Ÿ")
    parser.add_argument(
        "--num-queries", type=int, default=20, help="æŠ½æ ·æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ 20ï¼‰"
    )
    parser.add_argument(
        "--all-queries", action="store_true", help="ä½¿ç”¨å…¨éƒ¨æŸ¥è¯¢ï¼ˆä¸æŠ½æ ·ï¼‰"
    )
    parser.add_argument(
        "--methods",
        type=str,
        nargs="+",
        choices=[
            "bm25",
            "bm25plus",
            "vector",
            "hybrid_plus",
            "hybrid_rrf",
            "advanced",
        ],
        help="è¯„æµ‹æ–¹æ³•åˆ—è¡¨",
    )
    parser.add_argument(
        "--topk", type=int, default=10, help="è¯„ä¼°çš„ TopK å€¼ï¼ˆé»˜è®¤ 10ï¼‰"
    )
    parser.add_argument("--output", type=str, help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    # è¿è¡Œè¯„æµ‹
    metrics = runQuickEval(
        methods=args.methods,
        numQueries=args.num_queries,
        allQueries=args.all_queries,
        topK=args.topk,
    )

    # ä¿å­˜æŠ¥å‘Š
    if metrics and args.output:
        saveReport(metrics, args.output)
    elif metrics:
        # é»˜è®¤ä¿å­˜åˆ° outputs/reports/quick_eval.json
        defaultOutput = os.path.join(
            config.PROJECT_ROOT, "outputs", "reports", "quick_eval.json"
        )
        saveReport(metrics, defaultOutput)


if __name__ == "__main__":
    main()
