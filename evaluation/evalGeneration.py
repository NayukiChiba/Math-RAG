"""
ç”Ÿæˆè´¨é‡è¯„æµ‹è„šæœ¬

åŠŸèƒ½ï¼š
1. è¯„ä¼° RAG ç³»ç»Ÿçš„ç”Ÿæˆè´¨é‡
2. è®¡ç®—æœ¯è¯­å‘½ä¸­ç‡ã€æ¥æºå¼•ç”¨ç‡ã€å›ç­”éç©ºç‡
3. å¯é€‰è®¡ç®— BLEU/ROUGE åˆ†æ•°
4. ç”Ÿæˆè¯„æµ‹æŠ¥å‘Šï¼ˆå«æœ€å¥½/æœ€å·®ç¤ºä¾‹å¯¹æ¯”ï¼‰

è¯„æµ‹æŒ‡æ ‡è¯´æ˜ï¼š
- æœ¯è¯­å‘½ä¸­ç‡ï¼šå›ç­”ä¸­æ˜¯å¦åŒ…å«é»„é‡‘é›†ä¸­çš„ç›¸å…³æœ¯è¯­
- æ¥æºå¼•ç”¨ç‡ï¼šå›ç­”ä¸­æ˜¯å¦åŒ…å«æ­£ç¡®çš„ä¹¦å/é¡µç å¼•ç”¨
- å›ç­”éç©ºç‡ï¼šç”Ÿæˆæœªæ‹’ç»/æœªå´©æºƒçš„æ¯”ä¾‹
- BLEU/ROUGEï¼šä¸å‚è€ƒç­”æ¡ˆçš„è¯æ±‡é‡å åˆ†æ•°ï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluation/evalGeneration.py
    python evaluation/evalGeneration.py --results outputs/rag_results.jsonl
    python evaluation/evalGeneration.py --bleu --rouge
"""

import argparse
import json
import os
import re
import sys
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config


def loadRagResults(filepath: str) -> list[dict[str, Any]]:
    """
    åŠ è½½ RAG é—®ç­”ç»“æœ

    Args:
        filepath: ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰

    Returns:
        ç»“æœåˆ—è¡¨
    """
    results = []
    try:
        with open(filepath, encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    result = json.loads(line)
                    results.append(result)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        print(f"âœ… åŠ è½½äº† {len(results)} æ¡ RAG ç»“æœ")
        return results
    except FileNotFoundError:
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return []
    except Exception as e:
        print(f"âŒ åŠ è½½ç»“æœå¤±è´¥: {e}")
        return []


def loadGoldQueries(filepath: str) -> dict[str, dict[str, Any]]:
    """
    åŠ è½½é»„é‡‘æµ‹è¯•é›†ï¼Œæ„å»º query -> gold æ˜ å°„

    Args:
        filepath: é»„é‡‘æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„

    Returns:
        query åˆ° gold æ•°æ®çš„æ˜ å°„
    """
    goldMap = {}
    try:
        with open(filepath, encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    gold = json.loads(line)
                    query = gold.get("query", "")
                    if query:
                        goldMap[query] = gold
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ é»„é‡‘é›†ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        print(f"âœ… åŠ è½½äº† {len(goldMap)} æ¡é»„é‡‘æµ‹è¯•æ•°æ®")
        return goldMap
    except FileNotFoundError:
        print(f"âš ï¸ é»„é‡‘æµ‹è¯•é›†ä¸å­˜åœ¨: {filepath}")
        return {}
    except Exception as e:
        print(f"âŒ åŠ è½½é»„é‡‘æµ‹è¯•é›†å¤±è´¥: {e}")
        return {}


# ---- è¯„æµ‹æŒ‡æ ‡è®¡ç®— ----


def calculateTermHitRate(answer: str, relevantTerms: list[str]) -> dict[str, Any]:
    """
    è®¡ç®—æœ¯è¯­å‘½ä¸­ç‡

    Args:
        answer: ç”Ÿæˆçš„å›ç­”
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨

    Returns:
        å‘½ä¸­ç‡ä¿¡æ¯
    """
    if not relevantTerms:
        return {"hit_count": 0, "total": 0, "rate": 0.0, "hit_terms": []}

    hitTerms = []
    for term in relevantTerms:
        # ç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        if term.lower() in answer.lower():
            hitTerms.append(term)

    return {
        "hit_count": len(hitTerms),
        "total": len(relevantTerms),
        "rate": len(hitTerms) / len(relevantTerms),
        "hit_terms": hitTerms,
    }


def calculateSourceCitationRate(
    answer: str, sources: list[dict[str, Any]]
) -> dict[str, Any]:
    """
    è®¡ç®—æ¥æºå¼•ç”¨ç‡

    Args:
        answer: ç”Ÿæˆçš„å›ç­”
        sources: æ¥æºåˆ—è¡¨ï¼ˆå« source å’Œ pageï¼‰

    Returns:
        å¼•ç”¨ç‡ä¿¡æ¯
    """
    if not sources:
        return {"cited_count": 0, "total": 0, "rate": 0.0, "cited_sources": []}

    citedSources = []
    uniqueSources = []

    # å»é‡
    for s in sources:
        sourceName = s.get("source", "")
        if sourceName and sourceName not in [x["source"] for x in uniqueSources]:
            uniqueSources.append(s)

    for s in uniqueSources:
        sourceName = s.get("source", "")
        page = s.get("page")

        if not sourceName:
            continue

        # æ£€æŸ¥ä¹¦åæ˜¯å¦åœ¨å›ç­”ä¸­
        sourceFound = sourceName in answer

        # æ£€æŸ¥é¡µç æ˜¯å¦åœ¨å›ç­”ä¸­ï¼ˆå¦‚æœæœ‰é¡µç ï¼‰
        pageFound = False
        if page:
            # åŒ¹é…å¤šç§é¡µç æ ¼å¼ï¼šç¬¬Xé¡µã€p.Xã€Page X ç­‰
            pagePatterns = [
                f"ç¬¬{page}é¡µ",
                f"p.{page}",
                f"p{page}",
                f"Page {page}",
                f"page {page}",
                f"ç¬¬ {page} é¡µ",
            ]
            for pattern in pagePatterns:
                if pattern in answer:
                    pageFound = True
                    break

        if sourceFound:
            citedSources.append(
                {"source": sourceName, "page": page, "page_cited": pageFound}
            )

    return {
        "cited_count": len(citedSources),
        "total": len(uniqueSources),
        "rate": len(citedSources) / len(uniqueSources) if uniqueSources else 0.0,
        "cited_sources": citedSources,
    }


def isAnswerValid(answer: str) -> dict[str, Any]:
    """
    æ£€æŸ¥å›ç­”æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºã€éæ‹’ç»ï¼‰

    Args:
        answer: ç”Ÿæˆçš„å›ç­”

    Returns:
        æœ‰æ•ˆæ€§ä¿¡æ¯
    """
    if not answer or not answer.strip():
        return {"valid": False, "reason": "empty"}

    # æ£€æŸ¥å¸¸è§çš„æ‹’ç»/å¤±è´¥æ¨¡å¼
    refusalPatterns = [
        r"^ç”Ÿæˆå¤±è´¥",
        r"^æŠ±æ­‰.*æ— æ³•",
        r"^æˆ‘æ— æ³•",
        r"^å¯¹ä¸èµ·.*ä¸èƒ½",
        r"^å¾ˆæŠ±æ­‰",
        r"^æ— æ³•å›ç­”",
        r"^æ²¡æœ‰æ‰¾åˆ°ç›¸å…³",
    ]

    for pattern in refusalPatterns:
        if re.search(pattern, answer.strip()):
            return {"valid": False, "reason": "refusal"}

    # æ£€æŸ¥å›ç­”é•¿åº¦ï¼ˆè¿‡çŸ­å¯èƒ½æ— æ•ˆï¼‰
    if len(answer.strip()) < 10:
        return {"valid": False, "reason": "too_short"}

    return {"valid": True, "reason": None}


def calculateBleuScore(answer: str, reference: str) -> float:
    """
    è®¡ç®— BLEU åˆ†æ•°

    Args:
        answer: ç”Ÿæˆçš„å›ç­”
        reference: å‚è€ƒç­”æ¡ˆ

    Returns:
        BLEU åˆ†æ•°
    """
    try:
        from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu

        # åˆ†è¯ï¼ˆç®€å•æŒ‰å­—ç¬¦åˆ†è¯ï¼Œé€‚åˆä¸­æ–‡ï¼‰
        hypothesis = list(answer)
        referenceTokens = [list(reference)]

        # ä½¿ç”¨å¹³æ»‘å‡½æ•°é¿å…é›¶åˆ†
        smoothie = SmoothingFunction().method1
        score = sentence_bleu(referenceTokens, hypothesis, smoothing_function=smoothie)
        return score
    except ImportError:
        return -1.0  # è¡¨ç¤ºæœªå®‰è£… nltk
    except Exception:
        return 0.0


def calculateRougeScores(answer: str, reference: str) -> dict[str, float]:
    """
    è®¡ç®— ROUGE åˆ†æ•°

    Args:
        answer: ç”Ÿæˆçš„å›ç­”
        reference: å‚è€ƒç­”æ¡ˆ

    Returns:
        ROUGE åˆ†æ•°å­—å…¸
    """
    try:
        from rouge_score import rouge_scorer

        scorer = rouge_scorer.RougeScorer(
            ["rouge1", "rouge2", "rougeL"], use_stemmer=False
        )
        scores = scorer.score(reference, answer)
        return {
            "rouge1": scores["rouge1"].fmeasure,
            "rouge2": scores["rouge2"].fmeasure,
            "rougeL": scores["rougeL"].fmeasure,
        }
    except ImportError:
        return {"rouge1": -1.0, "rouge2": -1.0, "rougeL": -1.0}  # è¡¨ç¤ºæœªå®‰è£…
    except Exception:
        return {"rouge1": 0.0, "rouge2": 0.0, "rougeL": 0.0}


# ---- ä¸»è¯„æµ‹é€»è¾‘ ----


def evaluateGeneration(
    ragResults: list[dict[str, Any]],
    goldMap: dict[str, dict[str, Any]],
    calculateBleu: bool = False,
    calculateRouge: bool = False,
) -> dict[str, Any]:
    """
    è¯„æµ‹ç”Ÿæˆè´¨é‡

    Args:
        ragResults: RAG é—®ç­”ç»“æœåˆ—è¡¨
        goldMap: é»„é‡‘æµ‹è¯•é›†æ˜ å°„
        calculateBleu: æ˜¯å¦è®¡ç®— BLEU
        calculateRouge: æ˜¯å¦è®¡ç®— ROUGE

    Returns:
        è¯„æµ‹ç»“æœ
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š å¼€å§‹ç”Ÿæˆè´¨é‡è¯„æµ‹")
    print("=" * 60)

    # é€æ¡è¯„æµ‹ç»“æœ
    detailedResults = []
    termHitRates = []
    sourceCitationRates = []
    validAnswers = 0
    totalAnswers = len(ragResults)

    bleuScores = []
    rougeScores = {"rouge1": [], "rouge2": [], "rougeL": []}

    for i, result in enumerate(ragResults, 1):
        query = result.get("query", "")
        answer = result.get("answer", "")
        sources = result.get("sources", [])

        print(f"  è¯„æµ‹ {i}/{totalAnswers}: {query[:30]}...")

        # è·å–é»„é‡‘æ•°æ®
        gold = goldMap.get(query, {})
        relevantTerms = gold.get("relevant_terms", [])
        referenceAnswer = gold.get("reference_answer", "")

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        termHit = calculateTermHitRate(answer, relevantTerms)
        sourceCitation = calculateSourceCitationRate(answer, sources)
        answerValidity = isAnswerValid(answer)

        termHitRates.append(termHit["rate"])
        sourceCitationRates.append(sourceCitation["rate"])
        if answerValidity["valid"]:
            validAnswers += 1

        # å¯é€‰ï¼šBLEU/ROUGE
        bleu = -1.0
        rouge = {"rouge1": -1.0, "rouge2": -1.0, "rougeL": -1.0}

        if calculateBleu and referenceAnswer:
            bleu = calculateBleuScore(answer, referenceAnswer)
            if bleu >= 0:
                bleuScores.append(bleu)

        if calculateRouge and referenceAnswer:
            rouge = calculateRougeScores(answer, referenceAnswer)
            if rouge["rouge1"] >= 0:
                rougeScores["rouge1"].append(rouge["rouge1"])
                rougeScores["rouge2"].append(rouge["rouge2"])
                rougeScores["rougeL"].append(rouge["rougeL"])

        # è®°å½•è¯¦ç»†ç»“æœ
        detailedResults.append(
            {
                "query": query,
                "answer_length": len(answer),
                "term_hit": termHit,
                "source_citation": sourceCitation,
                "answer_validity": answerValidity,
                "bleu": bleu,
                "rouge": rouge,
                # ç”¨äºæ’åºçš„ç»¼åˆåˆ†æ•°
                "composite_score": (
                    termHit["rate"] * 0.4
                    + sourceCitation["rate"] * 0.3
                    + (1.0 if answerValidity["valid"] else 0.0) * 0.3
                ),
            }
        )

    # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
    avgTermHitRate = sum(termHitRates) / len(termHitRates) if termHitRates else 0.0
    avgSourceCitationRate = (
        sum(sourceCitationRates) / len(sourceCitationRates)
        if sourceCitationRates
        else 0.0
    )
    answerValidRate = validAnswers / totalAnswers if totalAnswers > 0 else 0.0

    summary = {
        "total_queries": totalAnswers,
        "term_hit_rate": {
            "mean": avgTermHitRate,
            "min": min(termHitRates) if termHitRates else 0.0,
            "max": max(termHitRates) if termHitRates else 0.0,
        },
        "source_citation_rate": {
            "mean": avgSourceCitationRate,
            "min": min(sourceCitationRates) if sourceCitationRates else 0.0,
            "max": max(sourceCitationRates) if sourceCitationRates else 0.0,
        },
        "answer_valid_rate": answerValidRate,
        "valid_answers": validAnswers,
        "invalid_answers": totalAnswers - validAnswers,
    }

    # BLEU/ROUGE æ±‡æ€»
    if bleuScores:
        summary["bleu"] = {
            "mean": sum(bleuScores) / len(bleuScores),
            "min": min(bleuScores),
            "max": max(bleuScores),
        }

    if rougeScores["rouge1"]:
        summary["rouge"] = {
            "rouge1_mean": sum(rougeScores["rouge1"]) / len(rougeScores["rouge1"]),
            "rouge2_mean": sum(rougeScores["rouge2"]) / len(rougeScores["rouge2"]),
            "rougeL_mean": sum(rougeScores["rougeL"]) / len(rougeScores["rougeL"]),
        }

    return {
        "summary": summary,
        "detailed_results": detailedResults,
    }


def findBestWorstExamples(
    detailedResults: list[dict[str, Any]], ragResults: list[dict[str, Any]], n: int = 3
) -> dict[str, list[dict[str, Any]]]:
    """
    æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„ç¤ºä¾‹

    Args:
        detailedResults: è¯¦ç»†è¯„æµ‹ç»“æœ
        ragResults: åŸå§‹ RAG ç»“æœ
        n: ç¤ºä¾‹æ•°é‡

    Returns:
        æœ€å¥½å’Œæœ€å·®ç¤ºä¾‹
    """
    # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
    sortedResults = sorted(
        enumerate(detailedResults), key=lambda x: x[1]["composite_score"], reverse=True
    )

    best = []
    worst = []

    # æœ€å¥½çš„ n ä¸ª
    for idx, detail in sortedResults[:n]:
        original = ragResults[idx]
        best.append(
            {
                "query": detail["query"],
                "answer": original.get("answer", "")[:500],  # æˆªæ–­
                "term_hit_rate": detail["term_hit"]["rate"],
                "source_citation_rate": detail["source_citation"]["rate"],
                "composite_score": detail["composite_score"],
            }
        )

    # æœ€å·®çš„ n ä¸ª
    for idx, detail in sortedResults[-n:]:
        original = ragResults[idx]
        worst.append(
            {
                "query": detail["query"],
                "answer": original.get("answer", "")[:500],
                "term_hit_rate": detail["term_hit"]["rate"],
                "source_citation_rate": detail["source_citation"]["rate"],
                "composite_score": detail["composite_score"],
                "validity_reason": detail["answer_validity"].get("reason"),
            }
        )

    return {"best": best, "worst": list(reversed(worst))}


def printSummary(summary: dict[str, Any]) -> None:
    """æ‰“å°è¯„æµ‹æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆè´¨é‡è¯„æµ‹ç»“æœ")
    print("=" * 60)

    print(f"\næ€»æŸ¥è¯¢æ•°: {summary['total_queries']}")

    print("\nğŸ“ˆ æœ¯è¯­å‘½ä¸­ç‡:")
    print(f"  å¹³å‡: {summary['term_hit_rate']['mean']:.4f}")
    print(f"  æœ€å°: {summary['term_hit_rate']['min']:.4f}")
    print(f"  æœ€å¤§: {summary['term_hit_rate']['max']:.4f}")

    print("\nğŸ“š æ¥æºå¼•ç”¨ç‡:")
    print(f"  å¹³å‡: {summary['source_citation_rate']['mean']:.4f}")
    print(f"  æœ€å°: {summary['source_citation_rate']['min']:.4f}")
    print(f"  æœ€å¤§: {summary['source_citation_rate']['max']:.4f}")

    print("\nâœ… å›ç­”æœ‰æ•ˆç‡:")
    print(f"  æœ‰æ•ˆç‡: {summary['answer_valid_rate']:.4f}")
    print(f"  æœ‰æ•ˆå›ç­”: {summary['valid_answers']}")
    print(f"  æ— æ•ˆå›ç­”: {summary['invalid_answers']}")

    if "bleu" in summary:
        print("\nğŸ“ BLEU åˆ†æ•°:")
        print(f"  å¹³å‡: {summary['bleu']['mean']:.4f}")
        print(f"  æœ€å°: {summary['bleu']['min']:.4f}")
        print(f"  æœ€å¤§: {summary['bleu']['max']:.4f}")

    if "rouge" in summary:
        print("\nğŸ“ ROUGE åˆ†æ•°:")
        print(f"  ROUGE-1: {summary['rouge']['rouge1_mean']:.4f}")
        print(f"  ROUGE-2: {summary['rouge']['rouge2_mean']:.4f}")
        print(f"  ROUGE-L: {summary['rouge']['rougeL_mean']:.4f}")


def printExamples(examples: dict[str, list[dict[str, Any]]]) -> None:
    """æ‰“å°ç¤ºä¾‹å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("ğŸ† æœ€ä½³ç¤ºä¾‹")
    print("=" * 60)

    for i, ex in enumerate(examples["best"], 1):
        print(f"\n[{i}] æŸ¥è¯¢: {ex['query']}")
        print(f"    æœ¯è¯­å‘½ä¸­ç‡: {ex['term_hit_rate']:.4f}")
        print(f"    æ¥æºå¼•ç”¨ç‡: {ex['source_citation_rate']:.4f}")
        print(f"    ç»¼åˆåˆ†æ•°: {ex['composite_score']:.4f}")
        print(f"    å›ç­”: {ex['answer'][:200]}...")

    print("\n" + "=" * 60)
    print("âš ï¸ æœ€å·®ç¤ºä¾‹")
    print("=" * 60)

    for i, ex in enumerate(examples["worst"], 1):
        print(f"\n[{i}] æŸ¥è¯¢: {ex['query']}")
        print(f"    æœ¯è¯­å‘½ä¸­ç‡: {ex['term_hit_rate']:.4f}")
        print(f"    æ¥æºå¼•ç”¨ç‡: {ex['source_citation_rate']:.4f}")
        print(f"    ç»¼åˆåˆ†æ•°: {ex['composite_score']:.4f}")
        if ex.get("validity_reason"):
            print(f"    æ— æ•ˆåŸå› : {ex['validity_reason']}")
        print(f"    å›ç­”: {ex['answer'][:200]}...")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè´¨é‡è¯„æµ‹è„šæœ¬")
    parser.add_argument(
        "--results",
        type=str,
        default=os.path.join(config.PROJECT_ROOT, "outputs", "rag_results.jsonl"),
        help="RAG é—®ç­”ç»“æœæ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--gold",
        type=str,
        default=os.path.join(
            config.PROJECT_ROOT, "data", "evaluation", "queries.jsonl"
        ),
        help="é»„é‡‘æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=os.path.join(
            config.PROJECT_ROOT, "outputs", "reports", "generation_metrics.json"
        ),
        help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„",
    )
    parser.add_argument(
        "--bleu",
        action="store_true",
        help="è®¡ç®— BLEU åˆ†æ•°ï¼ˆéœ€è¦ nltkï¼‰",
    )
    parser.add_argument(
        "--rouge",
        action="store_true",
        help="è®¡ç®— ROUGE åˆ†æ•°ï¼ˆéœ€è¦ rouge-scoreï¼‰",
    )
    parser.add_argument(
        "--examples",
        type=int,
        default=3,
        help="æœ€å¥½/æœ€å·®ç¤ºä¾‹æ•°é‡",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ“Š Math-RAG ç”Ÿæˆè´¨é‡è¯„æµ‹")
    print("=" * 60)
    print(f"RAG ç»“æœ: {args.results}")
    print(f"é»„é‡‘æµ‹è¯•é›†: {args.gold}")
    print(f"BLEU: {'å¯ç”¨' if args.bleu else 'ç¦ç”¨'}")
    print(f"ROUGE: {'å¯ç”¨' if args.rouge else 'ç¦ç”¨'}")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    ragResults = loadRagResults(args.results)
    if not ragResults:
        print("âŒ æ—  RAG ç»“æœï¼Œé€€å‡º")
        return

    goldMap = loadGoldQueries(args.gold)

    # æ‰§è¡Œè¯„æµ‹
    evalResult = evaluateGeneration(
        ragResults,
        goldMap,
        calculateBleu=args.bleu,
        calculateRouge=args.rouge,
    )

    # æ‰¾å‡ºæœ€å¥½/æœ€å·®ç¤ºä¾‹
    examples = findBestWorstExamples(
        evalResult["detailed_results"], ragResults, n=args.examples
    )

    # æ‰“å°ç»“æœ
    printSummary(evalResult["summary"])
    printExamples(examples)

    # ä¿å­˜æŠ¥å‘Š
    import time

    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "results_file": args.results,
        "gold_file": args.gold,
        "summary": evalResult["summary"],
        "examples": examples,
        "detailed_results": evalResult["detailed_results"],
    }

    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜: {args.output}")
    print("\nâœ… è¯„æµ‹å®Œæˆï¼")


if __name__ == "__main__":
    main()
