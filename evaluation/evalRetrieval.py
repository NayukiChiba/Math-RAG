"""
æ£€ç´¢è¯„æµ‹è„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½è¯„æµ‹æŸ¥è¯¢é›†
2. è°ƒç”¨å¤šç§æ£€ç´¢æ–¹æ³•ï¼ˆBM25ã€Vectorã€Hybridï¼‰
3. è®¡ç®—è¯„æµ‹æŒ‡æ ‡ï¼šRecall@Kã€MRRã€nDCG@Kã€MAP
4. ç”Ÿæˆè¯„æµ‹æŠ¥å‘Šå’Œå¯¹æ¯”å›¾è¡¨

è¯„æµ‹æŒ‡æ ‡è¯´æ˜ï¼š
- Recall@Kï¼šå‰ K ä¸ªç»“æœä¸­æ‰¾åˆ°çš„ç›¸å…³æ–‡æ¡£æ¯”ä¾‹
- MRR (Mean Reciprocal Rank)ï¼šç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ’åå€’æ•°çš„å¹³å‡å€¼
- nDCG@K (Normalized Discounted Cumulative Gain)ï¼šè€ƒè™‘æ’åä½ç½®çš„ç›¸å…³æ€§è¯„åˆ†
- MAP (Mean Average Precision)ï¼šæ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„ Precision å¹³å‡å€¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluation/evalRetrieval.py
    python evaluation/evalRetrieval.py --methods bm25 vector
    python evaluation/evalRetrieval.py --topk 20
    python evaluation/evalRetrieval.py --visualize
"""

import argparse
import json
import math
import os
import sys
import time
from collections import defaultdict
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config


def loadQueries(filepath: str) -> list[dict[str, Any]]:
    """
    åŠ è½½è¯„æµ‹æŸ¥è¯¢é›†

    Args:
        filepath: æŸ¥è¯¢é›†æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰

    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    queries = []
    try:
        with open(filepath, encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                try:
                    query = json.loads(line.strip())
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    if not all(
                        k in query for k in ["query", "relevant_terms", "subject"]
                    ):
                        print(f"âš ï¸  ç¬¬ {i} è¡Œç¼ºå°‘å¿…éœ€å­—æ®µï¼Œè·³è¿‡")
                        continue
                    if (
                        not isinstance(query["relevant_terms"], list)
                        or not query["relevant_terms"]
                    ):
                        print(f"âš ï¸  ç¬¬ {i} è¡Œ relevant_terms æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                        continue
                    queries.append(query)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  ç¬¬ {i} è¡Œ JSON è§£æå¤±è´¥: {e}")
        print(f"âœ… åŠ è½½äº† {len(queries)} æ¡æŸ¥è¯¢")
        return queries
    except FileNotFoundError:
        print(f"âŒ æŸ¥è¯¢é›†æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return []
    except Exception as e:
        print(f"âŒ åŠ è½½æŸ¥è¯¢é›†å¤±è´¥: {e}")
        return []


def calculateRecallAtK(results: list[dict], relevantTerms: list[str], k: int) -> float:
    """
    è®¡ç®— Recall@K

    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆæ¯é¡¹åŒ…å« term å­—æ®µï¼‰
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨
        k: TopK é˜ˆå€¼

    Returns:
        Recall@K å€¼ï¼ˆ0-1ï¼‰
    """
    if not relevantTerms:
        return 0.0

    topkResults = results[:k]
    topkTerms = {r["term"] for r in topkResults}
    found = sum(1 for term in relevantTerms if term in topkTerms)

    return found / len(relevantTerms)


def calculateMRR(results: list[dict], relevantTerms: list[str]) -> float:
    """
    è®¡ç®— MRR (Mean Reciprocal Rank)

    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨

    Returns:
        MRR å€¼ï¼ˆ0-1ï¼‰
    """
    for rank, result in enumerate(results, 1):
        if result["term"] in relevantTerms:
            return 1.0 / rank
    return 0.0


def calculateAP(results: list[dict], relevantTerms: list[str]) -> float:
    """
    è®¡ç®— AP (Average Precision)

    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨

    Returns:
        AP å€¼ï¼ˆ0-1ï¼‰
    """
    if not relevantTerms:
        return 0.0

    precisionSum = 0.0
    hitCount = 0

    for rank, result in enumerate(results, 1):
        if result["term"] in relevantTerms:
            hitCount += 1
            precision = hitCount / rank
            precisionSum += precision

    return precisionSum / len(relevantTerms) if hitCount > 0 else 0.0


def calculateDCG(results: list[dict], relevantTerms: list[str], k: int) -> float:
    """
    è®¡ç®— DCG@K (Discounted Cumulative Gain)

    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨
        k: TopK é˜ˆå€¼

    Returns:
        DCG@K å€¼
    """
    dcg = 0.0
    for i, result in enumerate(results[:k], 1):
        term = result["term"]
        # ç›¸å…³æ€§è¯„åˆ†ï¼šåœ¨ relevant_terms ä¸­çš„ä½ç½®è¶Šé å‰ï¼Œç›¸å…³æ€§è¶Šé«˜
        if term in relevantTerms:
            # ç¬¬ä¸€ä¸ªç›¸å…³æœ¯è¯­å¾—åˆ†æœ€é«˜ï¼Œåç»­é€’å‡
            relevance = len(relevantTerms) - relevantTerms.index(term)
        else:
            relevance = 0

        # DCG å…¬å¼ï¼šrel / log2(i+1)
        dcg += relevance / math.log2(i + 1)

    return dcg


def calculateIDCG(relevantTerms: list[str], k: int) -> float:
    """
    è®¡ç®— IDCG@K (Ideal DCG)

    Args:
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨ï¼ˆå·²æŒ‰ç›¸å…³æ€§æ’åºï¼‰
        k: TopK é˜ˆå€¼

    Returns:
        IDCG@K å€¼
    """
    idcg = 0.0
    for i in range(min(k, len(relevantTerms))):
        relevance = len(relevantTerms) - i
        idcg += relevance / math.log2(i + 2)  # i+2 å› ä¸ºä» i=0 å¼€å§‹

    return idcg


def calculateNDCG(results: list[dict], relevantTerms: list[str], k: int) -> float:
    """
    è®¡ç®— nDCG@K (Normalized DCG)

    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨
        relevantTerms: ç›¸å…³æœ¯è¯­åˆ—è¡¨
        k: TopK é˜ˆå€¼

    Returns:
        nDCG@K å€¼ï¼ˆ0-1ï¼‰
    """
    dcg = calculateDCG(results, relevantTerms, k)
    idcg = calculateIDCG(relevantTerms, k)

    return dcg / idcg if idcg > 0 else 0.0


def evaluateMethod(
    method: str,
    retriever: Any,
    queries: list[dict],
    topK: int = 10,
) -> dict[str, Any]:
    """
    è¯„æµ‹å•ä¸ªæ£€ç´¢æ–¹æ³•

    Args:
        method: æ–¹æ³•åç§°ï¼ˆBM25/Vector/Hybrid-Weighted/Hybrid-RRFï¼‰
        retriever: æ£€ç´¢å™¨å®ä¾‹
        queries: æŸ¥è¯¢åˆ—è¡¨
        topK: TopK é˜ˆå€¼

    Returns:
        è¯„æµ‹ç»“æœå­—å…¸
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š è¯„æµ‹æ–¹æ³•: {method}")
    print(f"{'=' * 60}")

    metrics = {
        "method": method,
        "total_queries": len(queries),
        "recall@1": [],
        "recall@3": [],
        "recall@5": [],
        "recall@10": [],
        "mrr": [],
        "map": [],
        "ndcg@3": [],
        "ndcg@5": [],
        "ndcg@10": [],
        "avg_query_time": 0.0,
    }

    queryTimes = []

    for i, query in enumerate(queries, 1):
        queryText = query["query"]
        relevantTerms = query["relevant_terms"]

        print(
            f"  æŸ¥è¯¢ {i}/{len(queries)}: {queryText} (ç›¸å…³æœ¯è¯­: {len(relevantTerms)})"
        )

        # æ‰§è¡Œæ£€ç´¢
        startTime = time.time()
        try:
            if method.startswith("Hybrid-"):
                strategy = "weighted" if method == "Hybrid-Weighted" else "rrf"
                results = retriever.search(
                    queryText,
                    topK=topK,
                    strategy=strategy,
                    alpha=0.5,
                    beta=0.5,
                )
            else:
                results = retriever.search(queryText, topK=topK)
        except Exception as e:
            print(f"    âŒ æ£€ç´¢å¤±è´¥: {e}")
            continue

        queryTime = time.time() - startTime
        queryTimes.append(queryTime)

        # è®¡ç®—æŒ‡æ ‡
        metrics["recall@1"].append(calculateRecallAtK(results, relevantTerms, 1))
        metrics["recall@3"].append(calculateRecallAtK(results, relevantTerms, 3))
        metrics["recall@5"].append(calculateRecallAtK(results, relevantTerms, 5))
        metrics["recall@10"].append(calculateRecallAtK(results, relevantTerms, 10))
        metrics["mrr"].append(calculateMRR(results, relevantTerms))
        metrics["map"].append(calculateAP(results, relevantTerms))
        metrics["ndcg@3"].append(calculateNDCG(results, relevantTerms, 3))
        metrics["ndcg@5"].append(calculateNDCG(results, relevantTerms, 5))
        metrics["ndcg@10"].append(calculateNDCG(results, relevantTerms, 10))

        print(f"    â±ï¸  æŸ¥è¯¢æ—¶é—´: {queryTime * 1000:.2f}ms")

    # è®¡ç®—å¹³å‡å€¼
    if queryTimes:
        metrics["avg_query_time"] = sum(queryTimes) / len(queryTimes)

    avgMetrics = {
        "recall@1": sum(metrics["recall@1"]) / len(metrics["recall@1"])
        if metrics["recall@1"]
        else 0.0,
        "recall@3": sum(metrics["recall@3"]) / len(metrics["recall@3"])
        if metrics["recall@3"]
        else 0.0,
        "recall@5": sum(metrics["recall@5"]) / len(metrics["recall@5"])
        if metrics["recall@5"]
        else 0.0,
        "recall@10": sum(metrics["recall@10"]) / len(metrics["recall@10"])
        if metrics["recall@10"]
        else 0.0,
        "mrr": sum(metrics["mrr"]) / len(metrics["mrr"]) if metrics["mrr"] else 0.0,
        "map": sum(metrics["map"]) / len(metrics["map"]) if metrics["map"] else 0.0,
        "ndcg@3": sum(metrics["ndcg@3"]) / len(metrics["ndcg@3"])
        if metrics["ndcg@3"]
        else 0.0,
        "ndcg@5": sum(metrics["ndcg@5"]) / len(metrics["ndcg@5"])
        if metrics["ndcg@5"]
        else 0.0,
        "ndcg@10": sum(metrics["ndcg@10"]) / len(metrics["ndcg@10"])
        if metrics["ndcg@10"]
        else 0.0,
    }

    print("\nğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
    print(f"  Recall@1:  {avgMetrics['recall@1']:.4f}")
    print(f"  Recall@3:  {avgMetrics['recall@3']:.4f}")
    print(f"  Recall@5:  {avgMetrics['recall@5']:.4f}")
    print(f"  Recall@10: {avgMetrics['recall@10']:.4f}")
    print(f"  MRR:       {avgMetrics['mrr']:.4f}")
    print(f"  MAP:       {avgMetrics['map']:.4f}")
    print(f"  nDCG@3:    {avgMetrics['ndcg@3']:.4f}")
    print(f"  nDCG@5:    {avgMetrics['ndcg@5']:.4f}")
    print(f"  nDCG@10:   {avgMetrics['ndcg@10']:.4f}")
    print(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´: {metrics['avg_query_time'] * 1000:.2f}ms")

    metrics["avg_metrics"] = avgMetrics
    return metrics


def generateComparisonChart(allMetrics: list[dict], outputDir: str) -> None:
    """
    ç”Ÿæˆå¯¹æ¯”å›¾è¡¨

    Args:
        allMetrics: æ‰€æœ‰æ–¹æ³•çš„è¯„æµ‹ç»“æœ
        outputDir: è¾“å‡ºç›®å½•
    """
    try:
        import matplotlib.pyplot as plt
        import numpy as np

        plt.rcParams["font.sans-serif"] = ["SimHei"]  # ä¸­æ–‡å­—ä½“
        plt.rcParams["axes.unicode_minus"] = False  # è´Ÿå·æ˜¾ç¤º

        # æå–æ•°æ®
        methods = [m["method"] for m in allMetrics]
        recallK = [1, 3, 5, 10]
        ndcgK = [3, 5, 10]

        # å›¾1: Recall@K å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle("æ£€ç´¢è¯„æµ‹æŒ‡æ ‡å¯¹æ¯”", fontsize=16, fontweight="bold")

        # Recall@K
        ax = axes[0, 0]
        x = np.arange(len(methods))
        width = 0.2
        for i, k in enumerate(recallK):
            values = [m["avg_metrics"][f"recall@{k}"] for m in allMetrics]
            ax.bar(x + i * width, values, width, label=f"Recall@{k}")
        ax.set_ylabel("Recall")
        ax.set_title("Recall@K")
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(methods, rotation=15, ha="right")
        ax.legend()
        ax.grid(axis="y", alpha=0.3)

        # nDCG@K
        ax = axes[0, 1]
        x = np.arange(len(methods))
        width = 0.25
        for i, k in enumerate(ndcgK):
            values = [m["avg_metrics"][f"ndcg@{k}"] for m in allMetrics]
            ax.bar(x + i * width, values, width, label=f"nDCG@{k}")
        ax.set_ylabel("nDCG")
        ax.set_title("nDCG@K")
        ax.set_xticks(x + width)
        ax.set_xticklabels(methods, rotation=15, ha="right")
        ax.legend()
        ax.grid(axis="y", alpha=0.3)

        # MRR å’Œ MAP
        ax = axes[1, 0]
        x = np.arange(len(methods))
        width = 0.35
        mrr = [m["avg_metrics"]["mrr"] for m in allMetrics]
        map_scores = [m["avg_metrics"]["map"] for m in allMetrics]
        ax.bar(x - width / 2, mrr, width, label="MRR")
        ax.bar(x + width / 2, map_scores, width, label="MAP")
        ax.set_ylabel("Score")
        ax.set_title("MRR å’Œ MAP å¯¹æ¯”")
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=15, ha="right")
        ax.legend()
        ax.grid(axis="y", alpha=0.3)

        # æŸ¥è¯¢æ—¶é—´
        ax = axes[1, 1]
        x = np.arange(len(methods))
        times = [m["avg_query_time"] * 1000 for m in allMetrics]  # è½¬æ¢ä¸ºæ¯«ç§’
        bars = ax.bar(x, times, color="skyblue")
        ax.set_ylabel("æ—¶é—´ (ms)")
        ax.set_title("å¹³å‡æŸ¥è¯¢æ—¶é—´")
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=15, ha="right")
        ax.grid(axis="y", alpha=0.3)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, time_val in zip(bars, times):
            height = bar.get_height()
            ax.text(
                bar.get_x() + bar.get_width() / 2.0,
                height,
                f"{time_val:.1f}",
                ha="center",
                va="bottom",
                fontsize=9,
            )

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        os.makedirs(outputDir, exist_ok=True)
        chartPath = os.path.join(outputDir, "retrieval_comparison.png")
        plt.savefig(chartPath, dpi=300, bbox_inches="tight")
        print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chartPath}")

    except ImportError:
        print("âš ï¸  è·³è¿‡å›¾è¡¨ç”Ÿæˆï¼šmatplotlib æœªå®‰è£…")
    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ£€ç´¢è¯„æµ‹è„šæœ¬")
    parser.add_argument(
        "--queries",
        type=str,
        default=os.path.join(
            config.PROJECT_ROOT, "data", "evaluation", "queries.jsonl"
        ),
        help="æŸ¥è¯¢é›†æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--methods",
        nargs="+",
        default=["bm25", "vector", "hybrid-weighted", "hybrid-rrf"],
        choices=["bm25", "vector", "hybrid-weighted", "hybrid-rrf"],
        help="è¦è¯„æµ‹çš„æ£€ç´¢æ–¹æ³•",
    )
    parser.add_argument(
        "--topk",
        type=int,
        default=10,
        help="TopK é˜ˆå€¼",
    )
    parser.add_argument(
        "--visualize",
        action="store_true",
        help="ç”Ÿæˆå¯¹æ¯”å›¾è¡¨",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=os.path.join(
            config.PROJECT_ROOT, "outputs", "reports", "retrieval_metrics.json"
        ),
        help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ“Š Math-RAG æ£€ç´¢è¯„æµ‹")
    print("=" * 60)
    print(f"æŸ¥è¯¢é›†: {args.queries}")
    print(f"è¯„æµ‹æ–¹æ³•: {', '.join(args.methods)}")
    print(f"TopK: {args.topk}")
    print("=" * 60)

    # åŠ è½½æŸ¥è¯¢é›†
    queries = loadQueries(args.queries)
    if not queries:
        print("âŒ æ— æœ‰æ•ˆæŸ¥è¯¢ï¼Œé€€å‡º")
        return

    # æŒ‰å­¦ç§‘ç»Ÿè®¡
    subjectCount = defaultdict(int)
    for q in queries:
        subjectCount[q["subject"]] += 1
    print("\nğŸ“š å­¦ç§‘åˆ†å¸ƒ:")
    for subject, count in sorted(subjectCount.items()):
        print(f"  {subject}: {count} æ¡")

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retrievers = {}
    corpusPath = os.path.join(config.PROCESSED_DIR, "retrieval", "corpus.jsonl")

    for method in args.methods:
        print(f"\nğŸ”„ åˆå§‹åŒ–æ£€ç´¢å™¨: {method.upper()}")
        try:
            if method == "bm25":
                from retrieval.retrievalBM25 import BM25Retriever

                retriever = BM25Retriever(corpusPath)
                # å°è¯•åŠ è½½ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ„å»º
                if not retriever.loadIndex():
                    print("  ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º...")
                    retriever.buildIndex()
                    retriever.saveIndex()
                retrievers["BM25"] = retriever
            elif method == "vector":
                from retrieval.retrievalVector import VectorRetriever

                retriever = VectorRetriever(corpusPath)
                # å°è¯•åŠ è½½ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ„å»º
                if not retriever.loadIndex():
                    print("  ç´¢å¼•ä¸å­˜åœ¨ï¼Œå¼€å§‹æ„å»º...")
                    retriever.buildIndex()
                    retriever.saveIndex()
                retrievers["Vector"] = retriever
            elif method == "hybrid-weighted":
                from retrieval.retrievalHybrid import HybridRetriever

                retriever = HybridRetriever(corpusPath)
                # Hybrid ä¼šè‡ªåŠ¨åˆå§‹åŒ–å­æ£€ç´¢å™¨
                retrievers["Hybrid-Weighted"] = retriever
            elif method == "hybrid-rrf":
                from retrieval.retrievalHybrid import HybridRetriever

                retriever = HybridRetriever(corpusPath)
                # Hybrid ä¼šè‡ªåŠ¨åˆå§‹åŒ–å­æ£€ç´¢å™¨
                retrievers["Hybrid-RRF"] = retriever
        except ImportError as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥ï¼ˆç¼ºå°‘ä¾èµ–ï¼‰: {e}")
            print(f"ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ {method.upper()} æ‰€éœ€çš„ä¾èµ–åº“æ˜¯å¦å·²å®‰è£…")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()

    if not retrievers:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ£€ç´¢å™¨ï¼Œé€€å‡º")
        return

    # è¯„æµ‹å„æ–¹æ³•
    allMetrics = []
    for methodName, retriever in retrievers.items():
        try:
            metrics = evaluateMethod(methodName, retriever, queries, args.topk)
            allMetrics.append(metrics)
        except Exception as e:
            print(f"âŒ è¯„æµ‹ {methodName} å¤±è´¥: {e}")

    if not allMetrics:
        print("âŒ è¯„æµ‹å¤±è´¥ï¼Œæ— ç»“æœ")
        return

    # ä¿å­˜æŠ¥å‘Š
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "queries_file": args.queries,
        "total_queries": len(queries),
        "subject_distribution": dict(subjectCount),
        "topk": args.topk,
        "results": allMetrics,
    }

    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜: {args.output}")

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    print(f"\n{'=' * 60}")
    print("ğŸ“Š è¯„æµ‹ç»“æœæ±‡æ€»")
    print(f"{'=' * 60}")

    # è¡¨å¤´
    print(
        f"{'æ–¹æ³•':<20} {'Recall@1':<10} {'Recall@10':<10} {'MRR':<10} {'MAP':<10} {'nDCG@10':<10} {'æŸ¥è¯¢æ—¶é—´':<10}"
    )
    print("-" * 90)

    # æ•°æ®è¡Œ
    for m in allMetrics:
        avg = m["avg_metrics"]
        print(
            f"{m['method']:<20} "
            f"{avg['recall@1']:<10.4f} "
            f"{avg['recall@10']:<10.4f} "
            f"{avg['mrr']:<10.4f} "
            f"{avg['map']:<10.4f} "
            f"{avg['ndcg@10']:<10.4f} "
            f"{m['avg_query_time'] * 1000:<10.2f}ms"
        )

    # ç”Ÿæˆå›¾è¡¨
    if args.visualize and len(allMetrics) > 1:
        outputDir = os.path.dirname(args.output)
        generateComparisonChart(allMetrics, outputDir)

    print("\nâœ… è¯„æµ‹å®Œæˆï¼")


if __name__ == "__main__":
    main()
