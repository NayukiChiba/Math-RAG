"""
è‡ªåŠ¨ç”Ÿæˆè¯„æµ‹æŸ¥è¯¢æ•°æ®

åŠŸèƒ½ï¼š
1. ä»æœ¯è¯­åº“ä¸­æ™ºèƒ½é‡‡æ ·é«˜é¢‘æœ¯è¯­
2. è‡ªåŠ¨æå–ç›¸å…³æœ¯è¯­ï¼ˆaliases + related_termsï¼‰
3. åˆ†å­¦ç§‘ç”Ÿæˆ queries.jsonl
4. ä¸ç°æœ‰æ•°æ®åˆå¹¶ï¼Œå»é‡

ä½¿ç”¨æ–¹æ³•ï¼š
    # é»˜è®¤ï¼šæŒ‰å›ºå®šæ•°é‡ç”Ÿæˆï¼ˆæ•°å­¦åˆ†æ35ï¼Œé«˜ç­‰ä»£æ•°20ï¼Œæ¦‚ç‡è®º20ï¼‰
    python evaluation/generateQueries.py

    # ç”Ÿæˆæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­
    python evaluation/generateQueries.py --all

    # æŒ‰æ¯”ä¾‹é‡‡æ ·ï¼ˆå¦‚50%ï¼‰
    python evaluation/generateQueries.py --ratio 0.5

    # æŒ‡å®šæ¯ä¸ªå­¦ç§‘çš„æ•°é‡
    python evaluation/generateQueries.py --num-ma 50 --num-gd 30 --num-gl 30

    # è°ƒæ•´è´¨é‡é˜ˆå€¼ï¼ˆæœ€å°‘ç›¸å…³æœ¯è¯­æ•°ï¼‰
    python evaluation/generateQueries.py --min-related 2
"""

import argparse
import json
import os
import random
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config


def loadJsonFile(filepath: str) -> dict[str, Any]:
    """åŠ è½½ JSON æ–‡ä»¶"""
    try:
        with open(filepath, encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {e}")
        return None


def loadAllTerms(chunkDir: str) -> list[dict[str, Any]]:
    """
    åŠ è½½æ‰€æœ‰æœ¯è¯­æ•°æ®

    Args:
        chunkDir: æœ¯è¯­æ•°æ®ç›®å½•

    Returns:
        æœ¯è¯­åˆ—è¡¨
    """
    terms = []

    print("ğŸ“š åŠ è½½æœ¯è¯­åº“...")
    for bookName in os.listdir(chunkDir):
        bookPath = os.path.join(chunkDir, bookName)

        if not os.path.isdir(bookPath):
            continue

        jsonFiles = [f for f in os.listdir(bookPath) if f.endswith(".json")]
        for jsonFile in jsonFiles:
            filepath = os.path.join(bookPath, jsonFile)
            data = loadJsonFile(filepath)

            if data and "term" in data and "subject" in data:
                terms.append(
                    {
                        "term": data["term"],
                        "aliases": data.get("aliases", []),
                        "related_terms": data.get("related_terms", []),
                        "subject": data["subject"],
                        "book": bookName,
                    }
                )

    print(f"âœ… åŠ è½½ {len(terms)} ä¸ªæœ¯è¯­")
    return terms


def normalizeSubject(subject: str) -> str:
    """æ ‡å‡†åŒ–å­¦ç§‘åç§°"""
    if "æ•°å­¦åˆ†æ" in subject:
        return "æ•°å­¦åˆ†æ"
    elif "é«˜ç­‰ä»£æ•°" in subject:
        return "é«˜ç­‰ä»£æ•°"
    elif "æ¦‚ç‡" in subject or "ç»Ÿè®¡" in subject:
        return "æ¦‚ç‡è®º"
    else:
        return subject


def generateQueries(
    terms: list[dict[str, Any]],
    numPerSubject: dict[str, int] = None,
    minRelatedTerms: int = 1,
    useAll: bool = False,
    sampleRatio: float = None,
) -> list[dict[str, Any]]:
    """
    ç”ŸæˆæŸ¥è¯¢æ•°æ®

    Args:
        terms: æœ¯è¯­åˆ—è¡¨
        numPerSubject: æ¯ä¸ªå­¦ç§‘ç”Ÿæˆæ•°é‡ï¼Œé»˜è®¤ {'æ•°å­¦åˆ†æ': 30, 'é«˜ç­‰ä»£æ•°': 20, 'æ¦‚ç‡è®º': 20}
        minRelatedTerms: æœ€å°‘ç›¸å…³æœ¯è¯­æ•°é‡
        useAll: æ˜¯å¦ä½¿ç”¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­ï¼ˆå¿½ç•¥ numPerSubjectï¼‰
        sampleRatio: é‡‡æ ·æ¯”ä¾‹ (0-1)ï¼Œå¦‚ 0.5 è¡¨ç¤ºä½¿ç”¨ 50% çš„æœ¯è¯­

    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    if numPerSubject is None and not useAll and sampleRatio is None:
        numPerSubject = {"æ•°å­¦åˆ†æ": 30, "é«˜ç­‰ä»£æ•°": 20, "æ¦‚ç‡è®º": 20}

    # æŒ‰å­¦ç§‘åˆ†ç»„
    termsBySubject = defaultdict(list)
    for term in terms:
        subject = normalizeSubject(term["subject"])
        termsBySubject[subject].append(term)

    queries = []

    for subject, subjectTerms in termsBySubject.items():
        print(f"\nğŸ“Š å¤„ç†å­¦ç§‘: {subject} (å…± {len(subjectTerms)} ä¸ªæœ¯è¯­)")

        # è¿‡æ»¤å‡ºæœ‰ç›¸å…³æœ¯è¯­çš„é«˜è´¨é‡æœ¯è¯­
        candidateTerms = []
        for term in subjectTerms:
            relatedCount = len(term["aliases"]) + len(term["related_terms"])
            if relatedCount >= minRelatedTerms:
                candidateTerms.append(
                    {
                        "term": term,
                        "score": relatedCount,  # æŒ‰ç›¸å…³æœ¯è¯­æ•°é‡æ’åº
                    }
                )

        # æŒ‰è´¨é‡æ’åº
        candidateTerms.sort(key=lambda x: x["score"], reverse=True)

        print(f"  - ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­: {len(candidateTerms)} ä¸ª")

        # é€‰æ‹©æœ¯è¯­ç­–ç•¥
        if useAll:
            # ä½¿ç”¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­
            selectedTerms = [c["term"] for c in candidateTerms]
            print("  - ç”Ÿæˆç­–ç•¥: ä½¿ç”¨å…¨éƒ¨ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­")
        elif sampleRatio is not None:
            # æŒ‰æ¯”ä¾‹é‡‡æ ·
            targetNum = int(len(candidateTerms) * sampleRatio)
            # 80% é«˜è´¨é‡ + 20% éšæœº
            numHigh = int(targetNum * 0.8)
            numRandom = targetNum - numHigh
            highQuality = [c["term"] for c in candidateTerms[:numHigh]]
            remainingTerms = [c["term"] for c in candidateTerms[numHigh:]]
            randomTerms = random.sample(
                remainingTerms, min(numRandom, len(remainingTerms))
            )
            selectedTerms = highQuality + randomTerms
            print(
                f"  - ç”Ÿæˆç­–ç•¥: æŒ‰æ¯”ä¾‹é‡‡æ · {sampleRatio * 100:.0f}% = {len(selectedTerms)} æ¡"
            )
        else:
            # æŒ‰å›ºå®šæ•°é‡é‡‡æ ·
            targetNum = numPerSubject.get(subject, 20)
            numHigh = int(targetNum * 0.8)
            numRandom = targetNum - numHigh
            highQuality = [c["term"] for c in candidateTerms[:numHigh]]
            remainingTerms = [c["term"] for c in candidateTerms[numHigh:]]
            randomTerms = random.sample(
                remainingTerms, min(numRandom, len(remainingTerms))
            )
            selectedTerms = highQuality + randomTerms
            print(f"  - ç”Ÿæˆç­–ç•¥: å›ºå®šæ•°é‡ {targetNum} æ¡")

        # ç”ŸæˆæŸ¥è¯¢
        for term in selectedTerms:
            # æ„å»º relevant_termsï¼šterm + aliases + éƒ¨åˆ† related_terms
            relevantTerms = [term["term"]]
            relevantTerms.extend(term["aliases"][:3])  # æœ€å¤š3ä¸ªåˆ«å

            # ä» related_terms ä¸­é€‰æ‹©ç›¸å…³æ€§é«˜çš„
            if term["related_terms"]:
                # ä¼˜å…ˆé€‰æ‹©åŒ…å«æŸ¥è¯¢è¯çš„ç›¸å…³æœ¯è¯­
                relatedWithQuery = [
                    rt for rt in term["related_terms"] if term["term"][:2] in rt
                ]
                relatedOthers = [
                    rt for rt in term["related_terms"] if term["term"][:2] not in rt
                ]

                selectedRelated = relatedWithQuery[:2] + relatedOthers[:1]
                relevantTerms.extend(selectedRelated)

            # å»é‡å¹¶ä¿æŒé¡ºåº
            uniqueRelevant = []
            seen = set()
            for rt in relevantTerms:
                if rt not in seen:
                    uniqueRelevant.append(rt)
                    seen.add(rt)

            query = {
                "query": term["term"],
                "relevant_terms": uniqueRelevant,
                "subject": subject,
            }
            queries.append(query)

        print(f"âœ… ç”Ÿæˆ {len(selectedTerms)} æ¡æŸ¥è¯¢")

    return queries


def loadExistingQueries(filepath: str) -> list[dict[str, Any]]:
    """åŠ è½½ç°æœ‰æŸ¥è¯¢æ•°æ®"""
    queries = []
    if not os.path.exists(filepath):
        return queries

    try:
        with open(filepath, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    queries.append(json.loads(line))
        print(f"ğŸ“‹ åŠ è½½ç°æœ‰æŸ¥è¯¢: {len(queries)} æ¡")
    except Exception as e:
        print(f"âŒ åŠ è½½ç°æœ‰æŸ¥è¯¢å¤±è´¥: {e}")

    return queries


def mergeQueries(
    existing: list[dict[str, Any]], generated: list[dict[str, Any]]
) -> list[dict[str, Any]]:
    """
    åˆå¹¶æŸ¥è¯¢ï¼Œå»é‡

    ä¼˜å…ˆä¿ç•™äººå·¥æ ‡æ³¨çš„æ•°æ®
    """
    # ä½¿ç”¨ query ä½œä¸ºå”¯ä¸€é”®
    existingQueries = {q["query"]: q for q in existing}

    merged = list(existingQueries.values())
    newCount = 0

    for gq in generated:
        if gq["query"] not in existingQueries:
            merged.append(gq)
            newCount += 1

    print("\nğŸ“Š åˆå¹¶ç»“æœ:")
    print(f"  - ç°æœ‰: {len(existing)} æ¡")
    print(f"  - æ–°å¢: {newCount} æ¡")
    print(f"  - æ€»è®¡: {len(merged)} æ¡")

    return merged


def saveQueries(queries: list[dict[str, Any]], filepath: str):
    """ä¿å­˜æŸ¥è¯¢æ•°æ®åˆ° JSONL"""
    # æŒ‰å­¦ç§‘åˆ†ç»„ç»Ÿè®¡
    bySubject = defaultdict(int)
    for q in queries:
        bySubject[q["subject"]] += 1

    print("\nğŸ“Š å­¦ç§‘åˆ†å¸ƒ:")
    for subject, count in sorted(bySubject.items()):
        print(f"  - {subject}: {count} æ¡")

    # ä¿å­˜
    with open(filepath, "w", encoding="utf-8") as f:
        for query in queries:
            f.write(json.dumps(query, ensure_ascii=False) + "\n")

    print(f"\nâœ… ä¿å­˜åˆ°: {filepath}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨ç”Ÿæˆè¯„æµ‹æŸ¥è¯¢æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # é»˜è®¤ï¼šæŒ‰å›ºå®šæ•°é‡ç”Ÿæˆ
  python evaluation/generateQueries.py
  
  # ç”Ÿæˆæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­
  python evaluation/generateQueries.py --all
  
  # æŒ‰50%æ¯”ä¾‹é‡‡æ ·
  python evaluation/generateQueries.py --ratio 0.5
  
  # è‡ªå®šä¹‰å„å­¦ç§‘æ•°é‡
  python evaluation/generateQueries.py --num-ma 50 --num-gd 30 --num-gl 30
        """,
    )

    parser.add_argument(
        "--all", action="store_true", help="ä½¿ç”¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­ï¼ˆå¿½ç•¥æ•°é‡é™åˆ¶ï¼‰"
    )

    parser.add_argument(
        "--ratio", type=float, help="é‡‡æ ·æ¯”ä¾‹ (0-1)ï¼Œå¦‚ 0.5 è¡¨ç¤ºä½¿ç”¨ 50%% çš„æœ¯è¯­"
    )

    parser.add_argument(
        "--num-ma", type=int, default=35, help="æ•°å­¦åˆ†æç”Ÿæˆæ•°é‡ï¼ˆé»˜è®¤35ï¼‰"
    )

    parser.add_argument(
        "--num-gd", type=int, default=20, help="é«˜ç­‰ä»£æ•°ç”Ÿæˆæ•°é‡ï¼ˆé»˜è®¤20ï¼‰"
    )

    parser.add_argument(
        "--num-gl", type=int, default=20, help="æ¦‚ç‡è®ºç”Ÿæˆæ•°é‡ï¼ˆé»˜è®¤20ï¼‰"
    )

    parser.add_argument(
        "--min-related", type=int, default=1, help="æœ€å°‘ç›¸å…³æœ¯è¯­æ•°é‡é˜ˆå€¼ï¼ˆé»˜è®¤1ï¼‰"
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šdata/evaluation/queries.jsonlï¼‰",
    )

    parser.add_argument(
        "--no-merge", action="store_true", help="ä¸ä¸ç°æœ‰æ•°æ®åˆå¹¶ï¼Œç›´æ¥è¦†ç›–"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ¤– è‡ªåŠ¨ç”Ÿæˆè¯„æµ‹æŸ¥è¯¢æ•°æ®")
    print("=" * 60)

    # é…ç½®
    chunkDir = config.CHUNK_DIR
    outputFile = args.output or os.path.join(
        config.PROJECT_ROOT, "data", "evaluation", "queries.jsonl"
    )

    # è®¾ç½®éšæœºç§å­ä¿è¯å¯å¤ç°
    random.seed(42)

    # Step 1: åŠ è½½æ‰€æœ‰æœ¯è¯­
    terms = loadAllTerms(chunkDir)
    if not terms:
        print("âŒ æœªæ‰¾åˆ°æœ¯è¯­æ•°æ®")
        return

    # Step 2: ç”ŸæˆæŸ¥è¯¢
    print("\n" + "=" * 60)
    print("ğŸ”§ ç”Ÿæˆæ–°æŸ¥è¯¢")
    print("=" * 60)

    if args.all:
        print("ğŸ“Œ æ¨¡å¼: ä½¿ç”¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æœ¯è¯­")
        generatedQueries = generateQueries(
            terms, minRelatedTerms=args.min_related, useAll=True
        )
    elif args.ratio is not None:
        print(f"ğŸ“Œ æ¨¡å¼: æŒ‰æ¯”ä¾‹é‡‡æ · ({args.ratio * 100:.0f}%)")
        generatedQueries = generateQueries(
            terms, minRelatedTerms=args.min_related, sampleRatio=args.ratio
        )
    else:
        print("ğŸ“Œ æ¨¡å¼: å›ºå®šæ•°é‡")
        print(f"  - æ•°å­¦åˆ†æ: {args.num_ma} æ¡")
        print(f"  - é«˜ç­‰ä»£æ•°: {args.num_gd} æ¡")
        print(f"  - æ¦‚ç‡è®º: {args.num_gl} æ¡")
        generatedQueries = generateQueries(
            terms,
            numPerSubject={
                "æ•°å­¦åˆ†æ": args.num_ma,
                "é«˜ç­‰ä»£æ•°": args.num_gd,
                "æ¦‚ç‡è®º": args.num_gl,
            },
            minRelatedTerms=args.min_related,
        )

    # Step 3: åŠ è½½ç°æœ‰æŸ¥è¯¢
    if not args.no_merge:
        print("\n" + "=" * 60)
        print("ğŸ“‹ åˆå¹¶ç°æœ‰æŸ¥è¯¢")
        print("=" * 60)

        existingQueries = loadExistingQueries(outputFile)
        mergedQueries = mergeQueries(existingQueries, generatedQueries)
    else:
        print("\n" + "=" * 60)
        print("âš ï¸  ç›´æ¥è¦†ç›–æ¨¡å¼ï¼ˆä¸ä¿ç•™ç°æœ‰æ•°æ®ï¼‰")
        print("=" * 60)
        mergedQueries = generatedQueries

    # Step 4: ä¿å­˜
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜ç»“æœ")
    print("=" * 60)

    saveQueries(mergedQueries, outputFile)

    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)
    print(f"æ€»æŸ¥è¯¢æ•°: {len(mergedQueries)}")
    if not args.no_merge:
        print(f"å»ºè®®äººå·¥å®¡æ ¸: {outputFile}")


if __name__ == "__main__":
    main()
