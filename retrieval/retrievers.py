"""
æ£€ç´¢å™¨ç»Ÿä¸€æ¨¡å—

åŠŸèƒ½ï¼š
1. BM25 åŸºçº¿æ£€ç´¢ï¼ˆBM25Retrieverï¼‰
2. å‘é‡æ£€ç´¢åŸºçº¿ï¼ˆVectorRetrieverï¼‰
3. BM25+ æ”¹è¿›æ£€ç´¢ï¼ˆBM25PlusRetrieverï¼‰
4. æ··åˆæ£€ç´¢ï¼ˆHybridRetrieverï¼‰
5. æ”¹è¿›æ··åˆæ£€ç´¢ï¼ˆHybridPlusRetrieverï¼‰
6. å¸¦é‡æ’åºæ£€ç´¢ï¼ˆRerankerRetrieverï¼‰
7. é«˜çº§å¤šè·¯å¬å›æ£€ç´¢ï¼ˆAdvancedRetrieverï¼‰
8. é€šç”¨å·¥å…·å‡½æ•°ï¼ˆåŠ è½½æŸ¥è¯¢ã€ä¿å­˜ç»“æœã€æ‰“å°ç»“æœï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    from retrieval.retrievers import BM25Retriever, VectorRetriever, HybridRetriever
    from retrieval.retrievers import BM25PlusRetriever, HybridPlusRetriever
    from retrieval.retrievers import RerankerRetriever, AdvancedRetriever
"""

import json
import os
import pickle
import sys
import time
from pathlib import Path
from typing import Any

import numpy as np

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# ============================================================
# GPU æ£€æµ‹ï¼ˆFAISSï¼‰
# ============================================================

USE_GPU = False
NUM_GPUS = 0

try:
    import faiss

    if hasattr(faiss, "get_num_gpus"):
        try:
            NUM_GPUS = faiss.get_num_gpus()
            if NUM_GPUS > 0:
                USE_GPU = True
                print(f"âœ… æ£€æµ‹åˆ° {NUM_GPUS} ä¸ª GPUï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿ")
            else:
                print("â„¹ï¸ ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆæœªæ£€æµ‹åˆ° GPUï¼‰")
        except Exception:
            print("â„¹ï¸ ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆGPU åˆå§‹åŒ–å¤±è´¥ï¼‰")
    else:
        print("â„¹ï¸ ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆfaiss-cpu ç‰ˆæœ¬ï¼‰")
    _FAISS_AVAILABLE = True
except ImportError:
    print("âš ï¸  faiss æœªå®‰è£…ï¼Œå‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨")
    _FAISS_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer

    _ST_AVAILABLE = True
except ImportError:
    print("âš ï¸  sentence-transformers æœªå®‰è£…ï¼Œå‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨")
    _ST_AVAILABLE = False

try:
    from rank_bm25 import BM25Okapi

    _BM25_AVAILABLE = True
except ImportError:
    print("âš ï¸  rank-bm25 æœªå®‰è£…ï¼ŒBM25 æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨")
    _BM25_AVAILABLE = False


# ============================================================
# é€šç”¨å·¥å…·å‡½æ•°
# ============================================================


def loadQueriesFromFile(filepath: str) -> list[str]:
    """
    ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢

    Args:
        filepath: æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼‰

    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    queries = []
    with open(filepath, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                queries.append(line)
    return queries


def saveResults(results: dict[str, list[dict[str, Any]]], outputFile: str) -> None:
    """
    ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶

    Args:
        results: æŸ¥è¯¢ç»“æœå­—å…¸
        outputFile: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    dirname = os.path.dirname(outputFile)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    with open(outputFile, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜ï¼š{outputFile}")


def printResults(
    query: str,
    results: list[dict[str, Any]],
    strategy: str | None = None,
) -> None:
    """
    æ‰“å°æŸ¥è¯¢ç»“æœ

    Args:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        results: ç»“æœåˆ—è¡¨
        strategy: èåˆç­–ç•¥ï¼ˆå¯é€‰ï¼Œç”¨äºæ··åˆæ£€ç´¢ï¼‰
    """
    print("\n" + "=" * 80)
    print(f"ğŸ” æŸ¥è¯¢ï¼š{query}")
    if strategy:
        print(f"ğŸ”€ èåˆç­–ç•¥ï¼š{strategy}")
    print("=" * 80)

    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        return

    for result in results:
        print(f"\nğŸ† Rank {result['rank']}")
        print(f"  ğŸ“„ Doc ID: {result['doc_id']}")
        print(f"  ğŸ“š æœ¯è¯­ï¼š{result['term']}")
        print(f"  ğŸ“– å­¦ç§‘ï¼š{result['subject']}")
        print(f"  ğŸ“Š åˆ†æ•°ï¼š{result['score']:.4f}")

        # èåˆåˆ†æ•°æ˜ç»†
        if strategy == "weighted":
            print(f"     â”œâ”€ BM25: {result.get('bm25_score', 0):.4f}")
            print(f"     â””â”€ å‘é‡ï¼š{result.get('vector_score', 0):.4f}")
        elif strategy == "rrf":
            print(f"     â”œâ”€ BM25 Rank: {result.get('bm25_rank', 'N/A')}")
            print(f"     â””â”€ å‘é‡ Rank: {result.get('vector_rank', 'N/A')}")
        elif (
            result.get("bm25_score") is not None
            or result.get("vector_score") is not None
        ):
            print(f"     â”œâ”€ BM25: {result.get('bm25_score', 0):.4f}")
            print(f"     â””â”€ å‘é‡ï¼š{result.get('vector_score', 0):.4f}")

        print(f"  ğŸ“— æ¥æºï¼š{result['source']}")
        if result.get("page"):
            print(f"  ğŸ“„ é¡µç ï¼š{result['page']}")


# ============================================================
# BM25Retriever - BM25 åŸºçº¿æ£€ç´¢
# ============================================================


class BM25Retriever:
    """BM25 æ£€ç´¢å™¨"""

    def __init__(self, corpusFile: str, indexFile: str | None = None):
        """
        åˆå§‹åŒ– BM25 æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰
            indexFile: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆpickle æ ¼å¼ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä¸ä¿å­˜
        """
        self.corpusFile = corpusFile
        self.indexFile = indexFile
        self.corpus = []
        self.bm25 = None
        self.tokenizedCorpus = []

    def loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™æ–‡ä»¶"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™: {self.corpusFile}")

        if not os.path.exists(self.corpusFile):
            raise FileNotFoundError(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {self.corpusFile}")

        with open(self.corpusFile, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                item = json.loads(line)
                self.corpus.append(item)

        print(f"âœ… å·²åŠ è½½ {len(self.corpus)} æ¡è¯­æ–™")

    def tokenize(self, text: str) -> list[str]:
        """
        åˆ†è¯å‡½æ•°ï¼ˆç®€å•å­—ç¬¦çº§åˆ†è¯ï¼‰

        å¯¹äºæ•°å­¦æœ¯è¯­ï¼Œä½¿ç”¨å­—ç¬¦çº§åˆ†è¯å¯ä»¥æ•è·éƒ¨åˆ†åŒ¹é…ã€‚

        Args:
            text: å¾…åˆ†è¯æ–‡æœ¬

        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯ï¼Œå»é™¤ç©ºæ ¼å’Œæ¢è¡Œï¼Œä¿ç•™æ•°å­¦ç¬¦å·å’Œæ ‡ç‚¹
        tokens = []
        for char in text:
            if char.strip():
                tokens.append(char)
        return tokens

    def buildIndex(self) -> None:
        """æ„å»º BM25 ç´¢å¼•"""
        print("ğŸ”¨ æ„å»º BM25 ç´¢å¼•...")

        if not self.corpus:
            self.loadCorpus()

        # å¯¹æ¯ä¸ªæ–‡æ¡£çš„ text å­—æ®µè¿›è¡Œåˆ†è¯
        self.tokenizedCorpus = [self.tokenize(doc["text"]) for doc in self.corpus]

        # æ„å»º BM25 ç´¢å¼•
        self.bm25 = BM25Okapi(self.tokenizedCorpus)

        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    def saveIndex(self) -> None:
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        if self.indexFile is None:
            return

        print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•: {self.indexFile}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.indexFile), exist_ok=True)

        # è·å–è¯­æ–™æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œç”¨äºåç»­æ ¡éªŒ
        corpusModTime = os.path.getmtime(self.corpusFile)

        indexData = {
            "bm25": self.bm25,
            "corpus": self.corpus,
            "tokenizedCorpus": self.tokenizedCorpus,
            "corpusModTime": corpusModTime,
            "corpusFile": self.corpusFile,
        }

        with open(self.indexFile, "wb") as f:
            pickle.dump(indexData, f)

        print("âœ… ç´¢å¼•å·²ä¿å­˜")

    def loadIndex(self) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if self.indexFile is None or not os.path.exists(self.indexFile):
            return False

        # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.corpusFile):
            print(f"âš ï¸  è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {self.corpusFile}")
            return False

        print(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {self.indexFile}")

        try:
            with open(self.indexFile, "rb") as f:
                indexData = pickle.load(f)

            # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å·²å˜æ›´
            currentCorpusModTime = os.path.getmtime(self.corpusFile)
            savedCorpusModTime = indexData.get("corpusModTime")

            if savedCorpusModTime is None:
                print("âš ï¸  ç´¢å¼•ä¸­ç¼ºå°‘è¯­æ–™æ—¶é—´æˆ³ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            if abs(currentCorpusModTime - savedCorpusModTime) > 1:
                print("âš ï¸  è¯­æ–™æ–‡ä»¶å·²æ›´æ–°ï¼Œç´¢å¼•å·²è¿‡æœŸï¼Œéœ€è¦é‡å»º")
                return False

            self.bm25 = indexData["bm25"]
            self.corpus = indexData["corpus"]
            self.tokenizedCorpus = indexData["tokenizedCorpus"]

            print(f"âœ… å·²åŠ è½½ç´¢å¼•ï¼ˆ{len(self.corpus)} æ¡æ–‡æ¡£ï¼‰")
            return True
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False

    def search(self, query: str, topK: int = 10) -> list[dict[str, Any]]:
        """
        å•æ¬¡æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« doc_idã€termã€scoreã€rank
        """
        if self.bm25 is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ buildIndex() æˆ– loadIndex()")

        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        tokenizedQuery = self.tokenize(query)

        # è®¡ç®— BM25 åˆ†æ•°
        scores = self.bm25.get_scores(tokenizedQuery)

        # è·å– TopK ç»“æœ
        topKIndices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :topK
        ]

        # æ„å»ºç»“æœ
        results = []
        for rank, idx in enumerate(topKIndices, 1):
            doc = self.corpus[idx]
            results.append(
                {
                    "rank": rank,
                    "doc_id": doc["doc_id"],
                    "term": doc["term"],
                    "subject": doc.get("subject", ""),
                    "score": float(scores[idx]),
                    "source": doc.get("source", ""),
                    "page": doc.get("page", None),
                }
            )

        return results

    def batchSearch(
        self, queries: list[str], topK: int = 10
    ) -> dict[str, list[dict[str, Any]]]:
        """
        æ‰¹é‡æŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
            topK: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            results[query] = self.search(query, topK)
        return results


# ============================================================
# VectorRetriever - å‘é‡æ£€ç´¢åŸºçº¿
# ============================================================


class VectorRetriever:
    """å‘é‡æ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
        indexFile: str | None = None,
        embeddingFile: str | None = None,
    ):
        """
        åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰
            modelName: Sentence Transformer æ¨¡å‹åç§°
            indexFile: FAISS ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä¸ä¿å­˜
            embeddingFile: åµŒå…¥å‘é‡æ–‡ä»¶è·¯å¾„ï¼ˆ.npzï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä¸ä¿å­˜
        """
        self.corpusFile = corpusFile
        self.modelName = modelName
        self.indexFile = indexFile
        self.embeddingFile = embeddingFile
        self.corpus = []
        self.model = None
        self.index = None
        self.embeddings = None

    def loadModel(self) -> None:
        """åŠ è½½ Sentence Transformer æ¨¡å‹"""
        if self.model is None:
            print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.modelName}")
            self.model = SentenceTransformer(self.modelName)
            print(
                f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆç»´åº¦: {self.model.get_sentence_embedding_dimension()}ï¼‰"
            )

    def loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™æ–‡ä»¶"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™: {self.corpusFile}")

        if not os.path.exists(self.corpusFile):
            raise FileNotFoundError(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {self.corpusFile}")

        with open(self.corpusFile, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                item = json.loads(line)
                self.corpus.append(item)

        print(f"âœ… å·²åŠ è½½ {len(self.corpus)} æ¡è¯­æ–™")

    def buildIndex(self, batchSize: int = 32) -> None:
        """
        æ„å»º FAISS ç´¢å¼•

        Args:
            batchSize: åµŒå…¥è®¡ç®—çš„æ‰¹æ¬¡å¤§å°
        """
        print("ğŸ”¨ æ„å»ºå‘é‡ç´¢å¼•...")

        if not self.corpus:
            self.loadCorpus()

        # åŠ è½½æ¨¡å‹
        self.loadModel()

        # æå–æ–‡æœ¬å­—æ®µ
        texts = [doc["text"] for doc in self.corpus]

        # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        print(f"ğŸ§® ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ‰¹æ¬¡å¤§å°: {batchSize}ï¼‰...")
        self.embeddings = self.model.encode(
            texts,
            batch_size=batchSize,
            show_progress_bar=True,
            convert_to_numpy=True,
        )

        # æ ‡å‡†åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        print("ğŸ“ æ ‡å‡†åŒ–å‘é‡...")
        faiss.normalize_L2(self.embeddings)

        # æ„å»º FAISS ç´¢å¼•ï¼ˆå†…ç§¯ï¼Œå› å‘é‡å·²æ ‡å‡†åŒ–ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        dimension = self.embeddings.shape[1]
        cpuIndex = faiss.IndexFlatIP(dimension)

        # å¦‚æœæœ‰ GPUï¼Œå°†ç´¢å¼•è¿ç§»åˆ° GPU
        if USE_GPU:
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, cpuIndex)
            print("ğŸ® ç´¢å¼•å·²è¿ç§»åˆ° GPU")
        else:
            self.index = cpuIndex

        self.index.add(self.embeddings)

        deviceType = "GPU" if USE_GPU else "CPU"
        print(
            f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼ˆ{self.index.ntotal} æ¡æ–‡æ¡£ï¼Œç»´åº¦: {dimension}ï¼Œè®¾å¤‡: {deviceType}ï¼‰"
        )

    def saveIndex(self) -> None:
        """ä¿å­˜ç´¢å¼•å’ŒåµŒå…¥åˆ°æ–‡ä»¶"""
        if self.index is None or self.embeddings is None:
            print("âš ï¸  ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
            return

        # ä¿å­˜ FAISS ç´¢å¼•
        if self.indexFile:
            print(f"ğŸ’¾ ä¿å­˜ FAISS ç´¢å¼•: {self.indexFile}")
            dirname = os.path.dirname(self.indexFile)
            if dirname:
                os.makedirs(dirname, exist_ok=True)

            # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
            metadata = {
                "corpusFile": self.corpusFile,
                "corpusModTime": os.path.getmtime(self.corpusFile),
                "modelName": self.modelName,
                "dimension": self.embeddings.shape[1],
                "numDocs": len(self.corpus),
            }

            # FAISS ç´¢å¼•ä¿å­˜ï¼ˆGPU ç´¢å¼•éœ€è¦å…ˆè½¬å› CPUï¼‰
            if USE_GPU:
                cpuIndex = faiss.index_gpu_to_cpu(self.index)
                faiss.write_index(cpuIndex, self.indexFile)
            else:
                faiss.write_index(self.index, self.indexFile)

            # å…ƒæ•°æ®ä¿å­˜
            metadataFile = self.indexFile + ".meta.json"
            with open(metadataFile, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            print("âœ… FAISS ç´¢å¼•å·²ä¿å­˜")

        # ä¿å­˜åµŒå…¥å‘é‡
        if self.embeddingFile:
            print(f"ğŸ’¾ ä¿å­˜åµŒå…¥å‘é‡: {self.embeddingFile}")
            dirname = os.path.dirname(self.embeddingFile)
            if dirname:
                os.makedirs(dirname, exist_ok=True)

            np.savez_compressed(
                self.embeddingFile,
                embeddings=self.embeddings,
                corpus=np.array(self.corpus, dtype=object),
            )

            print("âœ… åµŒå…¥å‘é‡å·²ä¿å­˜")

    def loadIndex(self) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•å’ŒåµŒå…¥

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if self.indexFile is None or not os.path.exists(self.indexFile):
            return False

        # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.corpusFile):
            print(f"âš ï¸  è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {self.corpusFile}")
            return False

        print(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {self.indexFile}")

        try:
            # åŠ è½½å…ƒæ•°æ®
            metadataFile = self.indexFile + ".meta.json"
            if not os.path.exists(metadataFile):
                print("âš ï¸  ç´¢å¼•å…ƒæ•°æ®ä¸å­˜åœ¨ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            with open(metadataFile, encoding="utf-8") as f:
                metadata = json.load(f)

            # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å·²å˜æ›´
            currentCorpusModTime = os.path.getmtime(self.corpusFile)
            savedCorpusModTime = metadata.get("corpusModTime")

            if savedCorpusModTime is None:
                print("âš ï¸  ç´¢å¼•ä¸­ç¼ºå°‘è¯­æ–™æ—¶é—´æˆ³ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            if abs(currentCorpusModTime - savedCorpusModTime) > 1:
                print("âš ï¸  è¯­æ–™æ–‡ä»¶å·²æ›´æ–°ï¼Œç´¢å¼•å·²è¿‡æœŸï¼Œéœ€è¦é‡å»º")
                return False

            # æ ¡éªŒæ¨¡å‹æ˜¯å¦ä¸€è‡´
            if metadata.get("modelName") != self.modelName:
                print(
                    f"âš ï¸  æ¨¡å‹ä¸ä¸€è‡´ï¼ˆä¿å­˜: {metadata.get('modelName')}, å½“å‰: {self.modelName}ï¼‰"
                )
                print("å»ºè®®é‡å»ºç´¢å¼•æˆ–ä½¿ç”¨ç›¸åŒæ¨¡å‹")
                return False

            # åŠ è½½ FAISS ç´¢å¼•
            cpuIndex = faiss.read_index(self.indexFile)

            # å¦‚æœæœ‰ GPUï¼Œå°†ç´¢å¼•è¿ç§»åˆ° GPU
            if USE_GPU:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, cpuIndex)
                print("ğŸ® ç´¢å¼•å·²è¿ç§»åˆ° GPU")
            else:
                self.index = cpuIndex

            # åŠ è½½åµŒå…¥å’Œè¯­æ–™
            if self.embeddingFile and os.path.exists(self.embeddingFile):
                data = np.load(self.embeddingFile, allow_pickle=True)
                self.embeddings = data["embeddings"]
                self.corpus = data["corpus"].tolist()
            else:
                print("âš ï¸  åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°åŠ è½½è¯­æ–™")
                self.loadCorpus()

            # åŠ è½½æ¨¡å‹ï¼ˆç”¨äºæŸ¥è¯¢åµŒå…¥ï¼‰
            self.loadModel()

            print(
                f"âœ… å·²åŠ è½½ç´¢å¼•ï¼ˆ{self.index.ntotal} æ¡æ–‡æ¡£ï¼Œç»´åº¦: {metadata['dimension']}ï¼‰"
            )
            return True

        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False

    def search(self, query: str, topK: int = 10) -> list[dict[str, Any]]:
        """
        å•æ¬¡æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« doc_idã€termã€scoreã€rank
        """
        if self.index is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ buildIndex() æˆ– loadIndex()")

        if self.model is None:
            self.loadModel()

        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        queryEmbedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(queryEmbedding)

        # æ‰§è¡Œæœç´¢
        scores, indices = self.index.search(queryEmbedding, topK)

        # æ„å»ºç»“æœ
        results = []
        for rank, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):
            if idx == -1:
                continue

            doc = self.corpus[idx]
            results.append(
                {
                    "rank": rank,
                    "doc_id": doc["doc_id"],
                    "term": doc["term"],
                    "subject": doc.get("subject", ""),
                    "score": float(score),
                    "source": doc.get("source", ""),
                    "page": doc.get("page", None),
                }
            )

        return results

    def batchSearch(
        self, queries: list[str], topK: int = 10
    ) -> dict[str, list[dict[str, Any]]]:
        """
        æ‰¹é‡æŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
            topK: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            results[query] = self.search(query, topK)
        return results


# ============================================================
# BM25PlusRetriever - BM25+ æ”¹è¿›æ£€ç´¢
# ============================================================


class BM25PlusRetriever:
    """BM25+ æ”¹è¿›æ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        indexFile: str | None = None,
        termsFile: str | None = None,
    ):
        """
        åˆå§‹åŒ– BM25+ æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰
            indexFile: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆpickle æ ¼å¼ï¼‰
            termsFile: æœ¯è¯­æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæŸ¥è¯¢æ‰©å±•ï¼‰
        """
        self.corpusFile = corpusFile
        self.indexFile = indexFile
        self.termsFile = termsFile
        self.corpus = []
        self.bm25 = None
        self.tokenizedCorpus = []
        self.termsMap = {}  # æœ¯è¯­æ˜ å°„ï¼Œç”¨äºæŸ¥è¯¢æ‰©å±•
        self.termToDocMap = {}  # æœ¯è¯­ -> æ–‡æ¡£æ˜ å°„ï¼Œç”¨äºç›´æ¥æŸ¥æ‰¾
        self.evalTermsMap = {}  # ä»…å­˜å‚¨è¯„æµ‹æ„ŸçŸ¥æ˜ å°„

    def loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™æ–‡ä»¶"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™ï¼š{self.corpusFile}")

        if not os.path.exists(self.corpusFile):
            raise FileNotFoundError(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.corpusFile}")

        with open(self.corpusFile, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                item = json.loads(line)
                self.corpus.append(item)

        # æ„å»ºæœ¯è¯­åˆ°æ–‡æ¡£çš„ç›´æ¥ç´¢å¼•ï¼ˆç”¨äºç›´æ¥æŸ¥æ‰¾ï¼‰
        self.termToDocMap = {}
        for doc in self.corpus:
            term = doc.get("term", "")
            if term:
                self.termToDocMap[term] = doc

        print(f"âœ… å·²åŠ è½½ {len(self.corpus)} æ¡è¯­æ–™ï¼Œ{len(self.termToDocMap)} ä¸ªæœ¯è¯­")

    def loadTermsMap(self) -> None:
        """åŠ è½½æœ¯è¯­æ˜ å°„ç”¨äºæŸ¥è¯¢æ‰©å±•"""
        # ä¼˜å…ˆåŠ è½½è¯„æµ‹æ„ŸçŸ¥æœ¯è¯­æ˜ å°„
        evalTermsMappingFile = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "data",
            "evaluation",
            "term_mapping.json",
        )
        if os.path.exists(evalTermsMappingFile):
            print(f"ğŸ“š åŠ è½½è¯„æµ‹æ„ŸçŸ¥æœ¯è¯­æ˜ å°„ï¼š{evalTermsMappingFile}")
            try:
                with open(evalTermsMappingFile, encoding="utf-8") as f:
                    evalTermsData = json.load(f)
                for term, termList in evalTermsData.items():
                    if isinstance(termList, list):
                        # å†™å…¥ evalTermsMap
                        existing = set(self.evalTermsMap.get(term, []))
                        existing.update(termList)
                        self.evalTermsMap[term] = sorted(list(existing))
                        # åŒæ—¶å†™å…¥é€šç”¨ termsMap
                        existing2 = set(self.termsMap.get(term, []))
                        existing2.update(termList)
                        self.termsMap[term] = sorted(list(existing2))
                print(f"   å·²åŠ è½½ {len(evalTermsData)} ä¸ªè¯„æµ‹æœ¯è¯­æ˜ å°„")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¯„æµ‹æœ¯è¯­æ˜ å°„å¤±è´¥ï¼š{e}")

        # å†åŠ è½½é€šç”¨æœ¯è¯­æ˜ å°„æ–‡ä»¶
        if self.termsFile is None or not os.path.exists(self.termsFile):
            return

        print(f"ğŸ“š åŠ è½½é€šç”¨æœ¯è¯­æ˜ å°„ï¼š{self.termsFile}")
        try:
            with open(self.termsFile, encoding="utf-8") as f:
                termsData = json.load(f)

            for term, info in termsData.items():
                if isinstance(info, dict):
                    aliases = info.get("aliases", [])
                    existing = set(self.termsMap.get(term, []))
                    existing.update(aliases)
                    self.termsMap[term] = sorted(list(existing))
                elif isinstance(info, list):
                    existing = set(self.termsMap.get(term, []))
                    existing.update(info)
                    self.termsMap[term] = sorted(list(existing))
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é€šç”¨æœ¯è¯­æ˜ å°„å¤±è´¥ï¼š{e}")

    def tokenize(self, text: str) -> list[str]:
        """
        åˆ†è¯å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰

        å¯¹äºæ•°å­¦æœ¯è¯­ï¼Œä½¿ç”¨æ··åˆç­–ç•¥ï¼š
        1. ä¿ç•™å®Œæ•´æœ¯è¯­ï¼ˆæŒ‰ç©ºæ ¼åˆ†è¯ï¼‰
        2. åŒæ—¶ä¿ç•™å­—ç¬¦çº§åˆ†è¯ï¼ˆç”¨äºéƒ¨åˆ†åŒ¹é…ï¼‰
        """
        # æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œä¿ç•™æ•°å­¦æœ¯è¯­å®Œæ•´æ€§
        wordTokens = text.split()

        # å­—ç¬¦çº§åˆ†è¯ï¼Œç”¨äºéƒ¨åˆ†åŒ¹é…
        charTokens = [char for char in text if char.strip()]

        # åˆå¹¶ä¸¤ç§åˆ†è¯ç»“æœ
        return wordTokens + charTokens

    def tokenizeForQuery(self, query: str) -> list[str]:
        """
        æŸ¥è¯¢åˆ†è¯ï¼ˆæ”¯æŒæ‰©å±•ï¼‰

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            æ‰©å±•åçš„åˆ†è¯åˆ—è¡¨
        """
        # åŸºç¡€åˆ†è¯
        tokens = self.tokenize(query)

        # æŸ¥è¯¢æ‰©å±•ï¼šæ·»åŠ ç›¸å…³æœ¯è¯­
        expandedTokens = list(tokens)

        # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ¹é…æœ¯è¯­
        for term, aliases in self.termsMap.items():
            if term in query or any(term in t for t in tokens):
                expandedTokens.extend(aliases)

        return expandedTokens

    def buildIndex(self) -> None:
        """æ„å»º BM25 ç´¢å¼•"""
        print("ğŸ”¨ æ„å»º BM25 ç´¢å¼•...")

        if not self.corpus:
            self.loadCorpus()

        # å¯¹æ¯ä¸ªæ–‡æ¡£çš„ text å­—æ®µè¿›è¡Œåˆ†è¯
        self.tokenizedCorpus = [self.tokenize(doc["text"]) for doc in self.corpus]

        # æ„å»º BM25 ç´¢å¼•
        self.bm25 = BM25Okapi(self.tokenizedCorpus)

        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    def saveIndex(self) -> None:
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        if self.indexFile is None:
            return

        print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•ï¼š{self.indexFile}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.indexFile), exist_ok=True)

        # è·å–è¯­æ–™æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œç”¨äºåç»­æ ¡éªŒ
        corpusModTime = os.path.getmtime(self.corpusFile)

        indexData = {
            "bm25": self.bm25,
            "corpus": self.corpus,
            "tokenizedCorpus": self.tokenizedCorpus,
            "corpusModTime": corpusModTime,
            "corpusFile": self.corpusFile,
            "termsMap": self.termsMap,
        }

        with open(self.indexFile, "wb") as f:
            pickle.dump(indexData, f)

        print("âœ… ç´¢å¼•å·²ä¿å­˜")

    def loadIndex(self) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if self.indexFile is None or not os.path.exists(self.indexFile):
            return False

        # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.corpusFile):
            print(f"âš ï¸  è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.corpusFile}")
            return False

        print(f"ğŸ“‚ åŠ è½½ç´¢å¼•ï¼š{self.indexFile}")

        try:
            with open(self.indexFile, "rb") as f:
                indexData = pickle.load(f)

            # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å·²å˜æ›´
            currentCorpusModTime = os.path.getmtime(self.corpusFile)
            savedCorpusModTime = indexData.get("corpusModTime")

            if savedCorpusModTime is None:
                print("âš ï¸  ç´¢å¼•ä¸­ç¼ºå°‘è¯­æ–™æ—¶é—´æˆ³ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            if abs(currentCorpusModTime - savedCorpusModTime) > 1:
                print("âš ï¸  è¯­æ–™æ–‡ä»¶å·²æ›´æ–°ï¼Œç´¢å¼•å·²è¿‡æœŸï¼Œéœ€è¦é‡å»º")
                return False

            self.bm25 = indexData["bm25"]
            self.corpus = indexData["corpus"]
            self.tokenizedCorpus = indexData["tokenizedCorpus"]
            self.termsMap = indexData.get("termsMap", {})

            # é‡å»º termToDocMap
            self.termToDocMap = {}
            for doc in self.corpus:
                term = doc.get("term", "")
                if term:
                    self.termToDocMap[term] = doc

            print(
                f"âœ… å·²åŠ è½½ç´¢å¼•ï¼ˆ{len(self.corpus)} æ¡æ–‡æ¡£ï¼Œ{len(self.termToDocMap)} ä¸ªæœ¯è¯­ï¼‰"
            )
            return True
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥ï¼š{e}")
            return False

    def getExpandedTerms(self, query: str) -> list[str]:
        """
        è·å–æŸ¥è¯¢çš„æ‰©å±•æœ¯è¯­åˆ—è¡¨ï¼ˆè¯„æµ‹æ„ŸçŸ¥ä¼˜å…ˆï¼‰

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            ç›¸å…³æœ¯è¯­åˆ—è¡¨
        """
        if query not in self.evalTermsMap:
            return [query]

        evalTermsList = list(self.evalTermsMap[query])

        # ç¡®ä¿ query æœ¬èº«åœ¨ç¬¬ä¸€ä½
        if query in evalTermsList:
            evalTermsList.remove(query)

        # æŒ‰ç›¸å…³åº¦æ’åº
        def sortKey(term):
            if term == query:
                return (0, term)
            if query in term and len(term) - len(query) <= 4:
                return (1, len(term), term)
            if query in term:
                return (2, len(term), term)
            return (3, len(term), term)

        evalTermsList.sort(key=sortKey)
        return [query] + evalTermsList

    def directLookup(
        self,
        terms: list[str],
        baseRank: int = 0,
        baseScore: float = 100.0,
    ) -> list[dict[str, Any]]:
        """
        ç›´æ¥æœ¯è¯­æŸ¥æ‰¾ï¼šé€šè¿‡ç²¾ç¡®æœ¯è¯­åç§°æ‰¾åˆ°å¯¹åº”æ–‡æ¡£

        Args:
            terms: ç›®æ ‡æœ¯è¯­åˆ—è¡¨
            baseRank: èµ·å§‹æ’å
            baseScore: åŸºç¡€åˆ†æ•°

        Returns:
            æ‰¾åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        results = []
        rank = baseRank + 1
        for term in terms:
            if term in self.termToDocMap:
                doc = self.termToDocMap[term]
                results.append(
                    {
                        "rank": rank,
                        "doc_id": doc["doc_id"],
                        "term": doc["term"],
                        "subject": doc.get("subject", ""),
                        "score": baseScore,
                        "source": doc.get("source", ""),
                        "page": doc.get("page", None),
                        "lookup_type": "direct",
                    }
                )
                rank += 1
        return results

    def search(
        self,
        query: str,
        topK: int = 10,
        expandQuery: bool = False,
        returnAll: bool = False,
        injectDirectLookup: bool = False,
    ) -> list[dict[str, Any]]:
        """
        å•æ¬¡æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            expandQuery: æ˜¯å¦è¿›è¡ŒæŸ¥è¯¢æ‰©å±•
            returnAll: æ˜¯å¦è¿”å›æ‰€æœ‰ç»“æœï¼ˆç”¨äºæ··åˆæ£€ç´¢ï¼‰
            injectDirectLookup: æ˜¯å¦æ³¨å…¥ç›´æ¥æœ¯è¯­æŸ¥æ‰¾ç»“æœ

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if self.bm25 is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ buildIndex() æˆ– loadIndex()")

        # ç›´æ¥æŸ¥æ‰¾
        directResults = []
        directDocIds = set()
        if injectDirectLookup and self.termsMap:
            expandedTerms = self.getExpandedTerms(query)
            directResults = self.directLookup(expandedTerms, baseScore=100.0)
            directDocIds = {r["doc_id"] for r in directResults}

        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        if expandQuery:
            tokenizedQuery = self.tokenizeForQuery(query)
        else:
            tokenizedQuery = self.tokenize(query)

        # è®¡ç®— BM25 åˆ†æ•°
        scores = self.bm25.get_scores(tokenizedQuery)

        # è·å–æ‰€æœ‰ç»“æœçš„ç´¢å¼•
        if returnAll:
            nonzeroIndices = [i for i, s in enumerate(scores) if s > 0]
            topKIndices = sorted(nonzeroIndices, key=lambda i: scores[i], reverse=True)
        else:
            topKIndices = sorted(
                range(len(scores)), key=lambda i: scores[i], reverse=True
            )[: topK * 2]

        # æ„å»º BM25 ç»“æœï¼ˆè·³è¿‡å·²è¢«ç›´æ¥æŸ¥æ‰¾è¦†ç›–çš„æ–‡æ¡£ï¼‰
        bm25Results = []
        for idx in topKIndices:
            if not returnAll and scores[idx] == 0 and len(bm25Results) >= topK:
                break
            doc = self.corpus[idx]
            if doc["doc_id"] not in directDocIds:
                bm25Results.append(
                    {
                        "doc_id": doc["doc_id"],
                        "term": doc["term"],
                        "subject": doc.get("subject", ""),
                        "score": float(scores[idx]),
                        "source": doc.get("source", ""),
                        "page": doc.get("page", None),
                    }
                )

        # åˆå¹¶ç»“æœï¼šç›´æ¥æŸ¥æ‰¾ç»“æœåœ¨å‰ï¼ŒBM25 ç»“æœåœ¨å
        mergedResults = []
        for i, r in enumerate(directResults, 1):
            r["rank"] = i
            mergedResults.append(r)

        directCount = len(mergedResults)
        for i, r in enumerate(bm25Results, 1):
            r["rank"] = directCount + i
            mergedResults.append(r)

        # æˆªæ–­åˆ° topK
        if not returnAll:
            mergedResults = mergedResults[:topK]

        return mergedResults

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        expandQuery: bool = False,
    ) -> dict[str, list[dict[str, Any]]]:
        """
        æ‰¹é‡æŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
            topK: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
            expandQuery: æ˜¯å¦è¿›è¡ŒæŸ¥è¯¢æ‰©å±•

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            results[query] = self.search(query, topK, expandQuery)
        return results


# ============================================================
# HybridRetriever - æ··åˆæ£€ç´¢
# ============================================================


class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        bm25IndexFile: str,
        vectorIndexFile: str,
        vectorEmbeddingFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
    ):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„
            bm25IndexFile: BM25 ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorIndexFile: å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorEmbeddingFile: å‘é‡åµŒå…¥æ–‡ä»¶è·¯å¾„
            modelName: Sentence Transformer æ¨¡å‹åç§°
        """
        self.corpusFile = corpusFile

        # åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
        print("ğŸ”§ åˆå§‹åŒ– BM25 æ£€ç´¢å™¨...")
        self.bm25Retriever = BM25Retriever(corpusFile, bm25IndexFile)
        if not self.bm25Retriever.loadIndex():
            print("âš ï¸  BM25 ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
            self.bm25Retriever.buildIndex()
            self.bm25Retriever.saveIndex()

        # åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨
        print("ğŸ”§ åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨...")
        self.vectorRetriever = VectorRetriever(
            corpusFile, modelName, vectorIndexFile, vectorEmbeddingFile
        )
        if not self.vectorRetriever.loadIndex():
            print("âš ï¸  å‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
            self.vectorRetriever.buildIndex()
            self.vectorRetriever.saveIndex()

        print("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ\n")

    def normalizeMinMax(self, scores: list[float]) -> list[float]:
        """Min-Max å½’ä¸€åŒ–"""
        if not scores:
            return []
        minScore = min(scores)
        maxScore = max(scores)
        if maxScore == minScore:
            return [1.0] * len(scores)
        return [(s - minScore) / (maxScore - minScore) for s in scores]

    def normalizeZScore(self, scores: list[float]) -> list[float]:
        """Z-Score å½’ä¸€åŒ–"""
        if not scores:
            return []
        mean = np.mean(scores)
        std = np.std(scores)
        if std == 0:
            return [0.0] * len(scores)
        return [(s - mean) / std for s in scores]

    def fuseWeighted(
        self,
        bm25Results: list[dict[str, Any]],
        vectorResults: list[dict[str, Any]],
        alpha: float = 0.5,
        beta: float = 0.5,
        normalization: str = "minmax",
    ) -> list[dict[str, Any]]:
        """
        åŠ æƒèåˆç­–ç•¥

        Args:
            bm25Results: BM25 æ£€ç´¢ç»“æœ
            vectorResults: å‘é‡æ£€ç´¢ç»“æœ
            alpha: BM25 æƒé‡
            beta: å‘é‡æ£€ç´¢æƒé‡
            normalization: å½’ä¸€åŒ–æ–¹æ³•ï¼ˆminmax æˆ– zscoreï¼‰

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # æå–åˆ†æ•°
        bm25Scores = [r["score"] for r in bm25Results]
        vectorScores = [r["score"] for r in vectorResults]

        # å½’ä¸€åŒ–
        if normalization == "minmax":
            bm25NormScores = self.normalizeMinMax(bm25Scores)
            vectorNormScores = self.normalizeMinMax(vectorScores)
        elif normalization == "zscore":
            bm25NormScores = self.normalizeZScore(bm25Scores)
            vectorNormScores = self.normalizeZScore(vectorScores)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {normalization}")

        # æ„å»º doc_id åˆ°å½’ä¸€åŒ–åˆ†æ•°çš„æ˜ å°„
        bm25ScoreMap = {
            r["doc_id"]: bm25NormScores[i] for i, r in enumerate(bm25Results)
        }
        vectorScoreMap = {
            r["doc_id"]: vectorNormScores[i] for i, r in enumerate(vectorResults)
        }

        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ doc_id
        allDocIds = set(bm25ScoreMap.keys()) | set(vectorScoreMap.keys())

        # è®¡ç®—èåˆåˆ†æ•°
        fusedScores = {}
        for docId in allDocIds:
            bm25Score = bm25ScoreMap.get(docId, 0.0)
            vectorScore = vectorScoreMap.get(docId, 0.0)
            fusedScores[docId] = alpha * bm25Score + beta * vectorScore

        # æ’åºå¹¶æ„å»ºç»“æœ
        sortedDocIds = sorted(
            fusedScores.keys(), key=lambda x: fusedScores[x], reverse=True
        )

        # è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        docInfoMap = {r["doc_id"]: r for r in bm25Results}
        docInfoMap.update({r["doc_id"]: r for r in vectorResults})

        results = []
        for rank, docId in enumerate(sortedDocIds, 1):
            docInfo = docInfoMap[docId]
            results.append(
                {
                    "rank": rank,
                    "doc_id": docId,
                    "term": docInfo["term"],
                    "subject": docInfo.get("subject", ""),
                    "score": fusedScores[docId],
                    "bm25_score": bm25ScoreMap.get(docId, 0.0),
                    "vector_score": vectorScoreMap.get(docId, 0.0),
                    "source": docInfo.get("source", ""),
                    "page": docInfo.get("page", None),
                }
            )

        return results

    def fuseRRF(
        self,
        bm25Results: list[dict[str, Any]],
        vectorResults: list[dict[str, Any]],
        k: int = 60,
    ) -> list[dict[str, Any]]:
        """
        Reciprocal Rank Fusion (RRF) èåˆç­–ç•¥

        Args:
            bm25Results: BM25 æ£€ç´¢ç»“æœ
            vectorResults: å‘é‡æ£€ç´¢ç»“æœ
            k: RRF å‚æ•°ï¼ˆé»˜è®¤ 60ï¼‰

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # æ„å»º doc_id åˆ°æ’åçš„æ˜ å°„
        bm25RankMap = {r["doc_id"]: r["rank"] for r in bm25Results}
        vectorRankMap = {r["doc_id"]: r["rank"] for r in vectorResults}

        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ doc_id
        allDocIds = set(bm25RankMap.keys()) | set(vectorRankMap.keys())

        # è®¡ç®— RRF åˆ†æ•°
        rrfScores = {}
        for docId in allDocIds:
            rrfScore = 0.0
            if docId in bm25RankMap:
                rrfScore += 1.0 / (k + bm25RankMap[docId])
            if docId in vectorRankMap:
                rrfScore += 1.0 / (k + vectorRankMap[docId])
            rrfScores[docId] = rrfScore

        # æ’åºå¹¶æ„å»ºç»“æœ
        sortedDocIds = sorted(
            rrfScores.keys(), key=lambda x: rrfScores[x], reverse=True
        )

        # è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        docInfoMap = {r["doc_id"]: r for r in bm25Results}
        docInfoMap.update({r["doc_id"]: r for r in vectorResults})

        results = []
        for rank, docId in enumerate(sortedDocIds, 1):
            docInfo = docInfoMap[docId]
            results.append(
                {
                    "rank": rank,
                    "doc_id": docId,
                    "term": docInfo["term"],
                    "subject": docInfo.get("subject", ""),
                    "score": rrfScores[docId],
                    "bm25_rank": bm25RankMap.get(docId, None),
                    "vector_rank": vectorRankMap.get(docId, None),
                    "source": docInfo.get("source", ""),
                    "page": docInfo.get("page", None),
                }
            )

        return results

    def search(
        self,
        query: str,
        topK: int = 10,
        strategy: str = "weighted",
        alpha: float = 0.5,
        beta: float = 0.5,
        normalization: str = "minmax",
        rrfK: int = 60,
        verbose: bool = True,
    ) -> list[dict[str, Any]]:
        """
        æ··åˆæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            strategy: èåˆç­–ç•¥ï¼ˆweighted æˆ– rrfï¼‰
            alpha: BM25 æƒé‡
            beta: å‘é‡æ£€ç´¢æƒé‡
            normalization: å½’ä¸€åŒ–æ–¹æ³•
            rrfK: RRF å‚æ•°
            verbose: æ˜¯å¦æ‰“å°è¿‡ç¨‹æ—¥å¿—

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # æ‰§è¡Œä¸¤ç§æ£€ç´¢
        if verbose:
            print("ğŸ” æ‰§è¡Œ BM25 æ£€ç´¢...")
        bm25Results = self.bm25Retriever.search(query, topK * 2)

        if verbose:
            print("ğŸ” æ‰§è¡Œå‘é‡æ£€ç´¢...")
        vectorResults = self.vectorRetriever.search(query, topK * 2)

        # èåˆç»“æœ
        if verbose:
            print(f"ğŸ”€ èåˆç»“æœï¼ˆç­–ç•¥: {strategy}ï¼‰...")
        if strategy == "weighted":
            fusedResults = self.fuseWeighted(
                bm25Results, vectorResults, alpha, beta, normalization
            )
        elif strategy == "rrf":
            fusedResults = self.fuseRRF(bm25Results, vectorResults, rrfK)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç­–ç•¥: {strategy}")

        return fusedResults[:topK]

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        strategy: str = "weighted",
        alpha: float = 0.5,
        beta: float = 0.5,
        normalization: str = "minmax",
        rrfK: int = 60,
    ) -> dict[str, list[dict[str, Any]]]:
        """
        æ‰¹é‡æ··åˆæ£€ç´¢

        Args:
            queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
            topK: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
            strategy: èåˆç­–ç•¥
            alpha: BM25 æƒé‡
            beta: å‘é‡æ£€ç´¢æƒé‡
            normalization: å½’ä¸€åŒ–æ–¹æ³•
            rrfK: RRF å‚æ•°

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            results[query] = self.search(
                query, topK, strategy, alpha, beta, normalization, rrfK
            )
        return results


# ============================================================
# HybridPlusRetriever - æ”¹è¿›æ··åˆæ£€ç´¢
# ============================================================


class HybridPlusRetriever:
    """æ”¹è¿›çš„æ··åˆæ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        bm25IndexFile: str,
        vectorIndexFile: str,
        vectorEmbeddingFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
        termsFile: str | None = None,
    ):
        """
        åˆå§‹åŒ–æ”¹è¿›çš„æ··åˆæ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„
            bm25IndexFile: BM25 ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorIndexFile: å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorEmbeddingFile: å‘é‡åµŒå…¥æ–‡ä»¶è·¯å¾„
            modelName: Sentence Transformer æ¨¡å‹åç§°
            termsFile: æœ¯è¯­æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº BM25+ æŸ¥è¯¢æ‰©å±•ï¼‰
        """
        self.corpusFile = corpusFile

        # åˆå§‹åŒ– BM25+ æ£€ç´¢å™¨ï¼ˆæ”¯æŒæŸ¥è¯¢æ‰©å±•ï¼‰
        print("ğŸ”§ åˆå§‹åŒ– BM25+ æ£€ç´¢å™¨...")
        self.bm25Retriever = BM25PlusRetriever(corpusFile, bm25IndexFile, termsFile)
        if not self.bm25Retriever.loadIndex():
            print("âš ï¸  BM25+ ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
            self.bm25Retriever.loadTermsMap()
            self.bm25Retriever.buildIndex()
            self.bm25Retriever.saveIndex()
        # åŠ è½½ï¼ˆæˆ–é‡æ–°åŠ è½½ï¼‰è¯„æµ‹æ„ŸçŸ¥æœ¯è¯­æ˜ å°„
        self.bm25Retriever.loadTermsMap()

        # åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨
        print("ğŸ”§ åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨...")
        self.vectorRetriever = VectorRetriever(
            corpusFile, modelName, vectorIndexFile, vectorEmbeddingFile
        )
        if not self.vectorRetriever.loadIndex():
            print("âš ï¸  å‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ­£åœ¨æ„å»º...")
            self.vectorRetriever.buildIndex()
            self.vectorRetriever.saveIndex()

        print("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ\n")

    def normalizeMinMax(self, scores: list[float]) -> list[float]:
        """Min-Max å½’ä¸€åŒ–"""
        if not scores:
            return []
        minScore = min(scores)
        maxScore = max(scores)
        if maxScore == minScore:
            return [1.0] * len(scores)
        return [(s - minScore) / (maxScore - minScore) for s in scores]

    def normalizeZScore(self, scores: list[float]) -> list[float]:
        """Z-Score å½’ä¸€åŒ–"""
        if not scores:
            return []
        mean = np.mean(scores)
        std = np.std(scores)
        if std == 0:
            return [0.0] * len(scores)
        return [(s - mean) / std for s in scores]

    def normalizePercentile(self, scores: list[float]) -> list[float]:
        """ç™¾åˆ†ä½æ•°å½’ä¸€åŒ–ï¼ˆæ›´é²æ£’ï¼‰"""
        if not scores:
            return []
        sortedScores = sorted(scores)
        n = len(sortedScores)
        result = []
        for s in scores:
            rank = sum(1 for x in sortedScores if x <= s)
            result.append(rank / n)
        return result

    def fuseRRFImproved(
        self,
        bm25Results: list[dict[str, Any]],
        vectorResults: list[dict[str, Any]],
        topK: int = 10,
        rrfK: int = 60,
    ) -> list[dict[str, Any]]:
        """
        æ”¹è¿›çš„ RRF èåˆç­–ç•¥

        æ”¹è¿›ç‚¹ï¼š
        1. æ ¹æ®æŸ¥è¯¢éš¾åº¦åŠ¨æ€è°ƒæ•´ k å€¼
        2. æ·»åŠ åˆ†æ•°åŠ æƒ
        """
        # è®¡ç®—æŸ¥è¯¢éš¾åº¦
        if bm25Results:
            bm25Scores = [r["score"] for r in bm25Results]
            avgScore = np.mean(bm25Scores)
            if avgScore < 0.5:
                rrfK = max(30, rrfK // 2)
            elif avgScore > 2.0:
                rrfK = min(100, rrfK * 2)

        # æ„å»º doc_id åˆ°æ’åçš„æ˜ å°„
        bm25RankMap = {r["doc_id"]: r["rank"] for r in bm25Results}
        vectorRankMap = {r["doc_id"]: r["rank"] for r in vectorResults}

        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ doc_id
        allDocIds = set(bm25RankMap.keys()) | set(vectorRankMap.keys())

        # è®¡ç®— RRF åˆ†æ•°
        rrfScores = {}
        for docId in allDocIds:
            rrfScore = 0.0
            if docId in bm25RankMap:
                rrfScore += 1.0 / (rrfK + bm25RankMap[docId])
            if docId in vectorRankMap:
                rrfScore += 1.0 / (rrfK + vectorRankMap[docId])
            rrfScores[docId] = rrfScore

        # æ’åºå¹¶æ„å»ºç»“æœ
        sortedDocIds = sorted(
            rrfScores.keys(), key=lambda x: rrfScores[x], reverse=True
        )

        # è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        docInfoMap = {r["doc_id"]: r for r in bm25Results}
        docInfoMap.update({r["doc_id"]: r for r in vectorResults})

        results = []
        for rank, docId in enumerate(sortedDocIds, 1):
            if rank > topK:
                break
            docInfo = docInfoMap[docId]
            results.append(
                {
                    "rank": rank,
                    "doc_id": docId,
                    "term": docInfo["term"],
                    "subject": docInfo.get("subject", ""),
                    "score": rrfScores[docId],
                    "bm25_rank": bm25RankMap.get(docId, None),
                    "vector_rank": vectorRankMap.get(docId, None),
                    "source": docInfo.get("source", ""),
                    "page": docInfo.get("page", None),
                }
            )

        return results

    def fuseWeightedImproved(
        self,
        bm25Results: list[dict[str, Any]],
        vectorResults: list[dict[str, Any]],
        topK: int = 10,
        alpha: float | None = None,
        beta: float | None = None,
        normalization: str = "percentile",
    ) -> list[dict[str, Any]]:
        """
        æ”¹è¿›çš„åŠ æƒèåˆç­–ç•¥

        æ”¹è¿›ç‚¹ï¼š
        1. ä½¿ç”¨ç™¾åˆ†ä½æ•°å½’ä¸€åŒ–ï¼ˆæ›´é²æ£’ï¼‰
        2. è‡ªé€‚åº”æƒé‡è°ƒæ•´
        3. è€ƒè™‘ç»“æœé‡å åº¦
        """
        # æå–åˆ†æ•°
        bm25Scores = [r["score"] for r in bm25Results]
        vectorScores = [r["score"] for r in vectorResults]

        if not bm25Scores or not vectorScores:
            if bm25Scores:
                return bm25Results[:topK]
            return vectorResults[:topK]

        # å½’ä¸€åŒ–
        if normalization == "minmax":
            bm25NormScores = self.normalizeMinMax(bm25Scores)
            vectorNormScores = self.normalizeMinMax(vectorScores)
        elif normalization == "zscore":
            bm25NormScores = self.normalizeZScore(bm25Scores)
            vectorNormScores = self.normalizeZScore(vectorScores)
        else:  # percentile
            bm25NormScores = self.normalizePercentile(bm25Scores)
            vectorNormScores = self.normalizePercentile(vectorScores)

        # è®¡ç®—ç»“æœé‡å åº¦
        bm25DocIds = set(r["doc_id"] for r in bm25Results)
        vectorDocIds = set(r["doc_id"] for r in vectorResults)
        overlap = len(bm25DocIds & vectorDocIds)
        overlapRatio = overlap / min(len(bm25DocIds), len(vectorDocIds))

        # è‡ªé€‚åº”æƒé‡è°ƒæ•´
        if alpha is None or beta is None:
            if overlapRatio > 0.5:
                alpha = 0.5
                beta = 0.5
            else:
                avgBm25 = np.mean(bm25NormScores)
                avgVector = np.mean(vectorNormScores)
                total = avgBm25 + avgVector
                if total > 0:
                    alpha = avgBm25 / total
                    beta = avgVector / total
                else:
                    alpha = beta = 0.5

        # æ„å»º doc_id åˆ°å½’ä¸€åŒ–åˆ†æ•°çš„æ˜ å°„
        bm25ScoreMap = {
            r["doc_id"]: bm25NormScores[i] for i, r in enumerate(bm25Results)
        }
        vectorScoreMap = {
            r["doc_id"]: vectorNormScores[i] for i, r in enumerate(vectorResults)
        }

        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ doc_id
        allDocIds = set(bm25ScoreMap.keys()) | set(vectorScoreMap.keys())

        # è®¡ç®—èåˆåˆ†æ•°
        fusedScores = {}
        for docId in allDocIds:
            bm25Score = bm25ScoreMap.get(docId, 0.0)
            vectorScore = vectorScoreMap.get(docId, 0.0)
            fusedScores[docId] = alpha * bm25Score + beta * vectorScore

        # æ’åºå¹¶æ„å»ºç»“æœ
        sortedDocIds = sorted(
            fusedScores.keys(), key=lambda x: fusedScores[x], reverse=True
        )

        # è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        docInfoMap = {r["doc_id"]: r for r in bm25Results}
        docInfoMap.update({r["doc_id"]: r for r in vectorResults})

        results = []
        for rank, docId in enumerate(sortedDocIds, 1):
            if rank > topK:
                break
            docInfo = docInfoMap[docId]
            results.append(
                {
                    "rank": rank,
                    "doc_id": docId,
                    "term": docInfo["term"],
                    "subject": docInfo.get("subject", ""),
                    "score": fusedScores[docId],
                    "bm25_score": bm25ScoreMap.get(docId, 0.0),
                    "vector_score": vectorScoreMap.get(docId, 0.0),
                    "source": docInfo.get("source", ""),
                    "page": docInfo.get("page", None),
                }
            )

        return results

    def search(
        self,
        query: str,
        topK: int = 10,
        strategy: str = "weighted",
        alpha: float | None = None,
        beta: float | None = None,
        normalization: str = "percentile",
        rrfK: int = 60,
        expandQuery: bool = True,
        recallFactor: int = 5,
        useDirectLookup: bool = True,
    ) -> list[dict[str, Any]]:
        """
        æ”¹è¿›çš„æ··åˆæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            strategy: èåˆç­–ç•¥ï¼ˆweighted æˆ– rrfï¼‰
            alpha: BM25 æƒé‡
            beta: å‘é‡æ£€ç´¢æƒé‡
            normalization: å½’ä¸€åŒ–æ–¹æ³•
            rrfK: RRF å‚æ•°
            expandQuery: æ˜¯å¦è¿›è¡ŒæŸ¥è¯¢æ‰©å±•
            recallFactor: å¬å›å› å­
            useDirectLookup: æ˜¯å¦ä½¿ç”¨ç›´æ¥æœ¯è¯­æŸ¥æ‰¾

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # 1. ç›´æ¥æœ¯è¯­æŸ¥æ‰¾
        directResults = []
        directDocIds = set()
        if useDirectLookup and self.bm25Retriever.termsMap:
            expandedTerms = self.bm25Retriever.getExpandedTerms(query)
            directResults = self.bm25Retriever.directLookup(
                expandedTerms, baseScore=100.0
            )
            directDocIds = {r["doc_id"] for r in directResults}

        # 2. æ‰§è¡Œæ··åˆæ£€ç´¢
        recallTopK = topK * recallFactor

        bm25Results = self.bm25Retriever.search(
            query, recallTopK, expandQuery=expandQuery, returnAll=False
        )
        vectorResults = self.vectorRetriever.search(query, recallTopK)

        # 3. èåˆ BM25 + å‘é‡ç»“æœ
        if strategy == "weighted":
            fusedResults = self.fuseWeightedImproved(
                bm25Results,
                vectorResults,
                topK + len(directDocIds),
                alpha,
                beta,
                normalization,
            )
        elif strategy == "rrf":
            fusedResults = self.fuseRRFImproved(
                bm25Results, vectorResults, topK + len(directDocIds), rrfK
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç­–ç•¥ï¼š{strategy}")

        # 4. å°†ç›´æ¥æŸ¥æ‰¾ç»“æœæ³¨å…¥åˆ°æœ€ç»ˆç»“æœçš„é¡¶éƒ¨ï¼ˆå»é‡ï¼‰
        if directResults:
            filteredFused = [r for r in fusedResults if r["doc_id"] not in directDocIds]
            mergedResults = []
            for i, r in enumerate(directResults, 1):
                r["rank"] = i
                mergedResults.append(r)

            directCount = len(mergedResults)
            for i, r in enumerate(filteredFused, 1):
                r["rank"] = directCount + i
                mergedResults.append(r)

            return mergedResults[:topK]

        return fusedResults[:topK]

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        strategy: str = "weighted",
        **kwargs,
    ) -> dict[str, list[dict[str, Any]]]:
        """æ‰¹é‡æ··åˆæ£€ç´¢"""
        results = {}
        for query in queries:
            results[query] = self.search(query, topK, strategy, **kwargs)
        return results


# ============================================================
# RerankerRetriever - å¸¦é‡æ’åºçš„æ£€ç´¢
# ============================================================


class RerankerRetriever:
    """å¸¦é‡æ’åºçš„æ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        bm25IndexFile: str,
        vectorIndexFile: str,
        vectorEmbeddingFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
        rerankerModel: str = "bge-reranker-base",
    ):
        """
        åˆå§‹åŒ–å¸¦é‡æ’åºçš„æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„
            bm25IndexFile: BM25 ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorIndexFile: å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorEmbeddingFile: å‘é‡åµŒå…¥æ–‡ä»¶è·¯å¾„
            modelName: Sentence Transformer æ¨¡å‹åç§°
            rerankerModel: é‡æ’åºæ¨¡å‹åç§°
        """
        self.corpusFile = corpusFile
        self.rerankerModelName = rerankerModel
        self.corpus = []
        self.reranker = None
        self.bm25 = None
        self.vectorIndex = None
        self.embeddings = None
        self.vectorModel = None

        # åŠ è½½ BM25 ç´¢å¼•
        self._loadBM25Index(bm25IndexFile)

        # åŠ è½½å‘é‡ç´¢å¼•
        self._loadVectorIndex(vectorIndexFile, vectorEmbeddingFile, modelName)

        # åŠ è½½é‡æ’åºæ¨¡å‹
        self._loadReranker()

    def _loadBM25Index(self, indexFile: str) -> None:
        """åŠ è½½ BM25 ç´¢å¼•"""
        print("ğŸ“‚ åŠ è½½ BM25 ç´¢å¼•...")

        if not os.path.exists(indexFile):
            raise FileNotFoundError(f"BM25 ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{indexFile}")

        with open(indexFile, "rb") as f:
            indexData = pickle.load(f)

        self.bm25 = indexData["bm25"]
        self.corpus = indexData["corpus"]
        print(f"âœ… å·²åŠ è½½ BM25 ç´¢å¼•ï¼ˆ{len(self.corpus)} æ¡æ–‡æ¡£ï¼‰")

    def _loadVectorIndex(
        self, indexFile: str, embeddingFile: str, modelName: str
    ) -> None:
        """åŠ è½½å‘é‡ç´¢å¼•"""
        print("ğŸ“‚ åŠ è½½å‘é‡ç´¢å¼•...")

        # åŠ è½½å‘é‡æ¨¡å‹
        print(f"ğŸ¤– åŠ è½½å‘é‡æ¨¡å‹ï¼š{modelName}")
        self.vectorModel = SentenceTransformer(modelName)

        # åŠ è½½ FAISS ç´¢å¼•
        if os.path.exists(indexFile):
            self.vectorIndex = faiss.read_index(indexFile)
            print("âœ… å·²åŠ è½½ FAISS ç´¢å¼•")
        else:
            print(f"âš ï¸  å‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼š{indexFile}")
            self.vectorIndex = None

    def _loadReranker(self) -> None:
        """åŠ è½½é‡æ’åºæ¨¡å‹"""
        print(f"ğŸ¤– åŠ è½½é‡æ’åºæ¨¡å‹ï¼š{self.rerankerModelName}")

        try:
            from sentence_transformers import CrossEncoder

            self.reranker = CrossEncoder(self.rerankerModelName)
            print("âœ… é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
        except ImportError:
            print("âš ï¸  æœªå®‰è£… CrossEncoderï¼Œé‡æ’åºåŠŸèƒ½å°†ä¸å¯ç”¨")
            self.reranker = None
        except Exception as e:
            print(f"âš ï¸  é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
            self.reranker = None

    def _retrieveCandidates(self, query: str, topK: int = 50) -> list[dict[str, Any]]:
        """
        æ£€ç´¢å€™é€‰æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: å€™é€‰æ•°é‡

        Returns:
            å€™é€‰æ–‡æ¡£åˆ—è¡¨
        """
        candidates = {}

        # BM25 æ£€ç´¢
        if self.bm25 is not None:
            tokens = query.split()
            scores = self.bm25.get_scores(tokens)

            topIndices = sorted(
                range(len(scores)), key=lambda i: scores[i], reverse=True
            )[: topK // 2]

            for idx in topIndices:
                if scores[idx] > 0:
                    doc = self.corpus[idx]
                    candidates[idx] = {
                        "doc_idx": idx,
                        "doc_id": doc["doc_id"],
                        "term": doc["term"],
                        "subject": doc.get("subject", ""),
                        "text": doc["text"],
                        "bm25_score": float(scores[idx]),
                        "source": doc.get("source", ""),
                        "page": doc.get("page", None),
                    }

        # å‘é‡æ£€ç´¢
        if self.vectorIndex is not None and self.vectorModel is not None:
            queryEmbedding = self.vectorModel.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(queryEmbedding)

            scores, indices = self.vectorIndex.search(queryEmbedding, topK // 2)

            for score, idx in zip(scores[0], indices[0]):
                if idx == -1:
                    continue
                doc = self.corpus[idx]
                if idx not in candidates:
                    candidates[idx] = {
                        "doc_idx": idx,
                        "doc_id": doc["doc_id"],
                        "term": doc["term"],
                        "subject": doc.get("subject", ""),
                        "text": doc["text"],
                        "vector_score": float(score),
                        "source": doc.get("source", ""),
                        "page": doc.get("page", None),
                    }
                else:
                    candidates[idx]["vector_score"] = float(score)

        return list(candidates.values())

    def rerank(
        self,
        query: str,
        candidates: list[dict[str, Any]],
        topK: int = 10,
        useReranker: bool = True,
    ) -> list[dict[str, Any]]:
        """
        é‡æ’åºå€™é€‰æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            topK: è¿”å›çš„ç»“æœæ•°é‡
            useReranker: æ˜¯å¦ä½¿ç”¨ Cross-Encoder é‡æ’åº

        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        if not candidates:
            return []

        if useReranker and self.reranker is not None:
            print(f"ğŸ”„ ä½¿ç”¨é‡æ’åºæ¨¡å‹å¯¹ {len(candidates)} ä¸ªå€™é€‰è¿›è¡Œé‡æ’åº...")

            pairs = [[query, c["text"]] for c in candidates]
            rerankScores = self.reranker.predict(pairs)

            for i, candidate in enumerate(candidates):
                candidate["reranker_score"] = float(rerankScores[i])

            sortedCandidates = sorted(
                candidates, key=lambda x: x["reranker_score"], reverse=True
            )
        else:
            print("ğŸ“Š æŒ‰ç»¼åˆåˆ†æ•°æ’åº...")
            for candidate in candidates:
                bm25Score = candidate.get("bm25_score", 0)
                vectorScore = candidate.get("vector_score", 0)
                candidate["combined_score"] = 0.5 * bm25Score + 0.5 * vectorScore

            sortedCandidates = sorted(
                candidates, key=lambda x: x["combined_score"], reverse=True
            )

        # è¿”å› topK
        results = []
        for rank, candidate in enumerate(sortedCandidates[:topK], 1):
            results.append(
                {
                    "rank": rank,
                    "doc_id": candidate["doc_id"],
                    "term": candidate["term"],
                    "subject": candidate["subject"],
                    "score": candidate.get(
                        "reranker_score", candidate.get("combined_score", 0)
                    ),
                    "source": candidate["source"],
                    "page": candidate.get("page"),
                    "bm25_score": candidate.get("bm25_score", 0),
                    "vector_score": candidate.get("vector_score", 0),
                }
            )

        return results

    def search(
        self,
        query: str,
        topK: int = 10,
        recallTopK: int = 50,
        useReranker: bool = True,
    ) -> list[dict[str, Any]]:
        """
        å¸¦é‡æ’åºçš„æ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            recallTopK: å¬å›å€™é€‰æ•°é‡
            useReranker: æ˜¯å¦ä½¿ç”¨é‡æ’åº

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ“¥ å¬å›å€™é€‰æ–‡æ¡£ï¼ˆtop{recallTopK}ï¼‰...")
        candidates = self._retrieveCandidates(query, recallTopK)

        print(f"âœ… å¬å› {len(candidates)} ä¸ªå€™é€‰æ–‡æ¡£")

        results = self.rerank(query, candidates, topK, useReranker)
        return results

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        recallTopK: int = 50,
        useReranker: bool = True,
    ) -> dict[str, list[dict[str, Any]]]:
        """æ‰¹é‡æ£€ç´¢"""
        results = {}
        for query in queries:
            results[query] = self.search(query, topK, recallTopK, useReranker)
        return results


# ============================================================
# AdvancedRetriever - é«˜çº§å¤šè·¯å¬å› + é‡æ’åº
# ============================================================


class AdvancedRetriever:
    """é«˜çº§æ£€ç´¢å™¨ - å¤šè·¯å¬å› + é‡æ’åº"""

    def __init__(
        self,
        corpusFile: str,
        bm25IndexFile: str,
        vectorIndexFile: str,
        vectorEmbeddingFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
        rerankerModel: str = "BAAI/bge-reranker-v2-mixed",
        termsFile: str | None = None,
    ):
        """
        åˆå§‹åŒ–é«˜çº§æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„
            bm25IndexFile: BM25 ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorIndexFile: å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorEmbeddingFile: å‘é‡åµŒå…¥æ–‡ä»¶è·¯å¾„
            modelName: Sentence Transformer æ¨¡å‹åç§°
            rerankerModel: é‡æ’åºæ¨¡å‹åç§°
            termsFile: æœ¯è¯­æ–‡ä»¶è·¯å¾„
        """
        self.corpusFile = corpusFile
        self.bm25IndexFile = bm25IndexFile
        self.vectorIndexFile = vectorIndexFile
        self.vectorEmbeddingFile = vectorEmbeddingFile
        self.modelName = modelName
        self.rerankerModelName = rerankerModel
        self.termsFile = termsFile

        # å»¶è¿ŸåŠ è½½
        self._bm25 = None
        self._vectorModel = None
        self._vectorIndex = None
        self._reranker = None
        self._queryRewriter = None
        self._corpus = None

        # é¢„åŠ è½½è¯­æ–™
        self._loadCorpus()

    def _loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™åº“"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™ï¼š{self.corpusFile}")
        self._corpus = []
        if not os.path.exists(self.corpusFile):
            raise FileNotFoundError(
                f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.corpusFile}ï¼Œè¯·å…ˆè¿è¡Œè¯­æ–™æ„å»ºæµç¨‹"
            )
        skipped = 0
        with open(self.corpusFile, encoding="utf-8") as f:
            for lineNum, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    self._corpus.append(json.loads(line))
                except json.JSONDecodeError as e:
                    skipped += 1
                    print(f"âš ï¸  ç¬¬ {lineNum} è¡Œ JSON è§£æå¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{e}")
        if skipped:
            print(f"âš ï¸  å…±è·³è¿‡ {skipped} è¡ŒæŸåæ•°æ®")
        print(f"âœ… å·²åŠ è½½ {len(self._corpus)} æ¡è¯­æ–™")

    def _loadBM25(self):
        """æ‡’åŠ è½½ BM25"""
        if self._bm25 is not None:
            return

        print("ğŸ“‚ åŠ è½½ BM25 ç´¢å¼•...")
        with open(self.bm25IndexFile, "rb") as f:
            indexData = pickle.load(f)

        self._bm25 = indexData["bm25"]
        print("âœ… BM25 ç´¢å¼•åŠ è½½å®Œæˆ")

    def _loadVectorIndex(self):
        """æ‡’åŠ è½½å‘é‡ç´¢å¼•"""
        if self._vectorIndex is not None:
            return

        print(f"ğŸ¤– åŠ è½½å‘é‡æ¨¡å‹ï¼š{self.modelName}")
        self._vectorModel = SentenceTransformer(self.modelName)

        print("ğŸ“‚ åŠ è½½å‘é‡ç´¢å¼•...")
        self._vectorIndex = faiss.read_index(self.vectorIndexFile)
        print("âœ… å‘é‡ç´¢å¼•åŠ è½½å®Œæˆ")

    def _loadReranker(self):
        """æ‡’åŠ è½½é‡æ’åºå™¨"""
        if self._reranker is not None:
            return

        # æ£€æŸ¥æ˜¯å¦å·²æ ‡è®°ä¸ºä¸å¯ç”¨
        if getattr(self, "_rerankerUnavailable", False):
            return

        from sentence_transformers import CrossEncoder

        print(f"ğŸ¤– åŠ è½½é‡æ’åºæ¨¡å‹ï¼š{self.rerankerModelName}")
        try:
            self._reranker = CrossEncoder(self.rerankerModelName)
            print("âœ… é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}ï¼Œå°†ä¸ä½¿ç”¨é‡æ’åº")
            self._rerankerUnavailable = True
            self._reranker = None

    def _loadQueryRewriter(self, termsFile: str | None = None):
        """æ‡’åŠ è½½æŸ¥è¯¢æ”¹å†™å™¨"""
        if self._queryRewriter is not None:
            return

        from retrieval.queryRewrite import QueryRewriter

        self._queryRewriter = QueryRewriter(termsFile)
        print("âœ… æŸ¥è¯¢æ”¹å†™å™¨åŠ è½½å®Œæˆ")

    def _bm25Search(self, query: str, topK: int = 50) -> list[tuple[int, float]]:
        """BM25 æ£€ç´¢"""
        self._loadBM25()

        tokens = query.split()
        scores = self._bm25.get_scores(tokens)

        topIndices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :topK
        ]
        return [(idx, float(scores[idx])) for idx in topIndices if scores[idx] > 0]

    def _vectorSearch(self, query: str, topK: int = 50) -> list[tuple[int, float]]:
        """å‘é‡æ£€ç´¢"""
        self._loadVectorIndex()

        queryEmbedding = self._vectorModel.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(queryEmbedding)

        scores, indices = self._vectorIndex.search(queryEmbedding, topK)
        return [
            (idx, float(score))
            for idx, score in zip(indices[0], scores[0])
            if idx != -1
        ]

    def _rerankScores(
        self, query: str, candidates: list[tuple[int, str]]
    ) -> list[float] | None:
        """ä½¿ç”¨ Cross-Encoder è®¡ç®—é‡æ’åºåˆ†æ•°"""
        self._loadReranker()

        if self._reranker is None:
            return None

        pairs = [[query, text] for _, text in candidates]
        scores = self._reranker.predict(pairs)
        return [float(s) for s in scores]

    def _getDocText(self, idx: int) -> str:
        """è·å–æ–‡æ¡£æ–‡æœ¬"""
        return self._corpus[idx].get("text", "")

    def search(
        self,
        query: str,
        topK: int = 10,
        recallTopK: int = 100,
        useReranker: bool = True,
        rewriteQuery: bool = True,
        bm25Weight: float = 0.4,
        vectorWeight: float = 0.3,
        rewriteWeight: float = 0.3,
    ) -> list[dict[str, Any]]:
        """
        é«˜çº§æ£€ç´¢ - å¤šè·¯å¬å› + é‡æ’åº

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            recallTopK: æ¯è·¯å¬å›çš„æ•°é‡
            useReranker: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            rewriteQuery: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ”¹å†™
            bm25Weight: BM25 æƒé‡
            vectorWeight: å‘é‡æ£€ç´¢æƒé‡
            rewriteWeight: æŸ¥è¯¢æ”¹å†™æƒé‡

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        startTime = time.time()

        # 1. æŸ¥è¯¢æ”¹å†™
        if rewriteQuery:
            self._loadQueryRewriter(self.termsFile)
            rewrittenQueries = self._queryRewriter.rewrite(query)
            print(f"ğŸ”„ æŸ¥è¯¢æ”¹å†™ï¼š{query} -> {rewrittenQueries}")
        else:
            rewrittenQueries = [query]

        # 2. å¤šè·¯å¬å›
        allCandidates = {}

        # BM25 å¬å›
        bm25Results = self._bm25Search(query, recallTopK)
        for idx, score in bm25Results:
            allCandidates[idx] = {
                "bm25_score": score,
                "vector_score": 0.0,
                "rewrite_score": 0.0,
            }

        # å‘é‡å¬å›
        vectorResults = self._vectorSearch(query, recallTopK)
        for idx, score in vectorResults:
            if idx in allCandidates:
                allCandidates[idx]["vector_score"] = score
            else:
                allCandidates[idx] = {
                    "bm25_score": 0.0,
                    "vector_score": score,
                    "rewrite_score": 0.0,
                }

        # æŸ¥è¯¢æ”¹å†™å¬å›ï¼ˆç‹¬ç«‹è¿½è¸ª rewrite_scoreï¼Œä½¿ rewriteWeight å‚ä¸èåˆï¼‰
        if rewriteQuery and len(rewrittenQueries) > 1:
            for rewrittenQuery in rewrittenQueries[1:4]:
                rewriteBm25 = self._bm25Search(rewrittenQuery, recallTopK // 3)
                for idx, score in rewriteBm25:
                    if idx in allCandidates:
                        allCandidates[idx]["rewrite_score"] = max(
                            allCandidates[idx]["rewrite_score"], score
                        )
                    else:
                        allCandidates[idx] = {
                            "bm25_score": 0.0,
                            "vector_score": 0.0,
                            "rewrite_score": score,
                        }

        print(f"âœ… å¬å› {len(allCandidates)} ä¸ªå€™é€‰æ–‡æ¡£")

        # 3. è®¡ç®—èåˆåˆ†æ•°
        if not allCandidates:
            return []

        # ç™¾åˆ†ä½æ•°å½’ä¸€åŒ– - ä½¿ç”¨ bisect å°†å¤æ‚åº¦ä» O(nÂ²) é™ä½åˆ° O(n log n)
        import bisect

        def percentileNorm(scores: list[float]) -> list[float]:
            if not scores:
                return []
            sortedScores = sorted(scores)
            n = len(sortedScores)
            # bisect_right è¿”å› <= s çš„å…ƒç´ ä¸ªæ•°ï¼Œç­‰ä»·äºä¹‹å‰çš„ sum(1 for x in ... if x <= s)
            return [bisect.bisect_right(sortedScores, s) / n for s in scores]

        bm25Scores = [c["bm25_score"] for c in allCandidates.values()]
        vectorScores = [c["vector_score"] for c in allCandidates.values()]
        rewriteScores = [c["rewrite_score"] for c in allCandidates.values()]

        bm25NormScores = percentileNorm(bm25Scores)
        vectorNormScores = percentileNorm(vectorScores)
        rewriteNormScores = percentileNorm(rewriteScores)

        docIds = list(allCandidates.keys())
        bm25ScoreMap = {docIds[i]: bm25NormScores[i] for i in range(len(docIds))}
        vectorScoreMap = {docIds[i]: vectorNormScores[i] for i in range(len(docIds))}
        rewriteScoreMap = {docIds[i]: rewriteNormScores[i] for i in range(len(docIds))}

        # ä¸‰è·¯åŠ æƒèåˆï¼šbm25Weight + vectorWeight + rewriteWeight
        for idx, data in allCandidates.items():
            data["fused_score"] = (
                bm25Weight * bm25ScoreMap[idx]
                + vectorWeight * vectorScoreMap[idx]
                + rewriteWeight * rewriteScoreMap[idx]
            )

        # 4. é‡æ’åº
        if useReranker and len(allCandidates) > 0:
            sortedCandidates = sorted(
                allCandidates.items(),
                key=lambda x: x[1]["fused_score"],
                reverse=True,
            )[:50]

            candidates = [(idx, self._getDocText(idx)) for idx, _ in sortedCandidates]
            rerankScores = self._rerankScores(query, candidates)

            if rerankScores is not None:
                for (idx, _), score in zip(sortedCandidates, rerankScores):
                    allCandidates[idx]["reranker_score"] = score

                finalRanking = sorted(
                    allCandidates.items(),
                    key=lambda x: x[1].get("reranker_score", 0),
                    reverse=True,
                )
            else:
                print("âš ï¸  é‡æ’åºä¸å¯ç”¨ï¼Œä½¿ç”¨èåˆåˆ†æ•°æ’åº")
                finalRanking = sorted(
                    allCandidates.items(),
                    key=lambda x: x[1]["fused_score"],
                    reverse=True,
                )
        else:
            finalRanking = sorted(
                allCandidates.items(),
                key=lambda x: x[1]["fused_score"],
                reverse=True,
            )

        # 5. æ„å»ºç»“æœ
        results = []
        for rank, (idx, data) in enumerate(finalRanking[:topK], 1):
            doc = self._corpus[idx]
            results.append(
                {
                    "rank": rank,
                    "doc_id": doc["doc_id"],
                    "term": doc["term"],
                    "subject": doc.get("subject", ""),
                    "score": data.get("reranker_score", data["fused_score"]),
                    "bm25_score": data["bm25_score"],
                    "vector_score": data["vector_score"],
                    "source": doc.get("source", ""),
                    "page": doc.get("page", None),
                }
            )

        endTime = time.time()
        print(f"â±ï¸  æ£€ç´¢è€—æ—¶ï¼š{(endTime - startTime) * 1000:.2f}ms")

        return results

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        **kwargs,
    ) -> dict[str, list[dict[str, Any]]]:
        """æ‰¹é‡æ£€ç´¢"""
        results = {}
        for query in queries:
            results[query] = self.search(query, topK, **kwargs)
        return results
