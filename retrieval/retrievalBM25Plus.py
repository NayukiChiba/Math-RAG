"""
BM25+ æ”¹è¿›æ£€ç´¢

åŠŸèƒ½ï¼š
1. åœ¨ BM25 åŸºç¡€ä¸Šå¢åŠ æŸ¥è¯¢æ‰©å±•
2. æ”¯æŒåŒä¹‰è¯æ‰©å±•ï¼ˆæ•°å­¦æœ¯è¯­ï¼‰
3. æ”¯æŒå­—æ®µåŠ æƒï¼ˆterm å­—æ®µæƒé‡æ›´é«˜ï¼‰
4. å¢åŠ å¬å›æ•°é‡

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•æ¬¡æŸ¥è¯¢
    python retrieval/retrievalBM25Plus.py --query "æ³°å‹’å±•å¼€" --topk 10

    # å¸¦æŸ¥è¯¢æ‰©å±•
    python retrieval/retrievalBM25Plus.py --query "æ³°å‹’å±•å¼€" --topk 10 --expand-query
"""

import argparse
import json
import os
import pickle
import sys
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config


class BM25PlusRetriever:
    """BM25+ æ”¹è¿›æ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        indexFile: str | None = None,
        termsFile: str | None = None,
    ):
        """
        åˆå§‹åŒ– BM25+ æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰
            indexFile: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆpickle æ ¼å¼ï¼‰
            termsFile: æœ¯è¯­æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæŸ¥è¯¢æ‰©å±•ï¼‰
        """
        self.corpusFile = corpusFile
        self.indexFile = indexFile
        self.termsFile = termsFile
        self.corpus = []
        self.bm25 = None
        self.tokenizedCorpus = []
        self.termsMap = {}  # æœ¯è¯­æ˜ å°„ï¼Œç”¨äºæŸ¥è¯¢æ‰©å±•

    def loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™æ–‡ä»¶"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™ï¼š{self.corpusFile}")

        if not os.path.exists(self.corpusFile):
            raise FileNotFoundError(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.corpusFile}")

        with open(self.corpusFile, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                item = json.loads(line)
                self.corpus.append(item)

        print(f"âœ… å·²åŠ è½½ {len(self.corpus)} æ¡è¯­æ–™")

    def loadTermsMap(self) -> None:
        """åŠ è½½æœ¯è¯­æ˜ å°„ç”¨äºæŸ¥è¯¢æ‰©å±•"""
        # ä¼˜å…ˆåŠ è½½è¯„æµ‹æœ¯è¯­æ˜ å°„
        eval_terms_file = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            "data",
            "evaluation",
            "term_mapping.json",
        )
        if os.path.exists(eval_terms_file):
            print(f"ğŸ“š åŠ è½½è¯„æµ‹æœ¯è¯­æ˜ å°„ï¼š{eval_terms_file}")
            try:
                with open(eval_terms_file, encoding="utf-8") as f:
                    eval_terms = json.load(f)
                self.termsMap.update(eval_terms)
                print(f"âœ… å·²åŠ è½½ {len(eval_terms)} ä¸ªè¯„æµ‹æœ¯è¯­æ˜ å°„")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¯„æµ‹æœ¯è¯­æ˜ å°„å¤±è´¥ï¼š{e}")

        # å†åŠ è½½é€šç”¨æœ¯è¯­æ˜ å°„
        if self.termsFile is None or not os.path.exists(self.termsFile):
            return

        print(f"ğŸ“š åŠ è½½é€šç”¨æœ¯è¯­æ˜ å°„ï¼š{self.termsFile}")
        try:
            with open(self.termsFile, encoding="utf-8") as f:
                termsData = json.load(f)

            # æ„å»ºæœ¯è¯­æ˜ å°„ï¼šæœ¯è¯­ -> ç›¸å…³æœ¯è¯­åˆ—è¡¨
            for term, info in termsData.items():
                if isinstance(info, dict):
                    aliases = info.get("aliases", [])
                    self.termsMap[term] = aliases
                elif isinstance(info, list):
                    self.termsMap[term] = info
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æœ¯è¯­æ˜ å°„å¤±è´¥ï¼š{e}")

    def tokenize(self, text: str) -> list[str]:
        """
        åˆ†è¯å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰

        å¯¹äºæ•°å­¦æœ¯è¯­ï¼Œä½¿ç”¨æ··åˆç­–ç•¥ï¼š
        1. ä¿ç•™å®Œæ•´æœ¯è¯­ï¼ˆæŒ‰ç©ºæ ¼åˆ†è¯ï¼‰
        2. åŒæ—¶ä¿ç•™å­—ç¬¦çº§åˆ†è¯ï¼ˆç”¨äºéƒ¨åˆ†åŒ¹é…ï¼‰
        """
        # æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œä¿ç•™æ•°å­¦æœ¯è¯­å®Œæ•´æ€§
        wordTokens = text.split()

        # å­—ç¬¦çº§åˆ†è¯ï¼Œç”¨äºéƒ¨åˆ†åŒ¹é…
        charTokens = [char for char in text if char.strip()]

        # åˆå¹¶ä¸¤ç§åˆ†è¯ç»“æœ
        return wordTokens + charTokens

    def tokenizeForQuery(self, query: str) -> list[str]:
        """
        æŸ¥è¯¢åˆ†è¯ï¼ˆæ”¯æŒæ‰©å±•ï¼‰

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            æ‰©å±•åçš„åˆ†è¯åˆ—è¡¨
        """
        # åŸºç¡€åˆ†è¯
        tokens = self.tokenize(query)

        # æŸ¥è¯¢æ‰©å±•ï¼šæ·»åŠ ç›¸å…³æœ¯è¯­
        expandedTokens = list(tokens)

        # åªåœ¨æŸ¥è¯¢å®Œå…¨åŒ¹é…æœ¯è¯­æ—¶æ‰æ‰©å±•
        if query in self.termsMap:
            # æ·»åŠ ç›¸å…³æœ¯è¯­ï¼Œä½†åªæ·»åŠ å‰ 5 ä¸ªæœ€ç›¸å…³çš„ï¼ˆé¿å…å¼•å…¥è¿‡å¤šå™ªå£°ï¼‰
            aliases = self.termsMap[query][:5]
            expandedTokens.extend(aliases)

        return expandedTokens

    def buildIndex(self) -> None:
        """æ„å»º BM25 ç´¢å¼•"""
        print("ğŸ”¨ æ„å»º BM25 ç´¢å¼•...")

        if not self.corpus:
            self.loadCorpus()

        # å¯¹æ¯ä¸ªæ–‡æ¡£çš„ text å­—æ®µè¿›è¡Œåˆ†è¯
        self.tokenizedCorpus = [self.tokenize(doc["text"]) for doc in self.corpus]

        # æ„å»º BM25 ç´¢å¼•
        try:
            from rank_bm25 import BM25Okapi

            self.bm25 = BM25Okapi(self.tokenizedCorpus)
        except ImportError:
            print("âŒ ç¼ºå°‘ä¾èµ–åº“ rank-bm25")
            print("è¯·å®‰è£…ï¼špip install rank-bm25")
            sys.exit(1)

        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    def saveIndex(self) -> None:
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        if self.indexFile is None:
            return

        print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•ï¼š{self.indexFile}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.indexFile), exist_ok=True)

        # è·å–è¯­æ–™æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œç”¨äºåç»­æ ¡éªŒ
        corpusModTime = os.path.getmtime(self.corpusFile)

        indexData = {
            "bm25": self.bm25,
            "corpus": self.corpus,
            "tokenizedCorpus": self.tokenizedCorpus,
            "corpusModTime": corpusModTime,
            "corpusFile": self.corpusFile,
            "termsMap": self.termsMap,
        }

        with open(self.indexFile, "wb") as f:
            pickle.dump(indexData, f)

        print("âœ… ç´¢å¼•å·²ä¿å­˜")

    def loadIndex(self) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if self.indexFile is None or not os.path.exists(self.indexFile):
            return False

        # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.corpusFile):
            print(f"âš ï¸  è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.corpusFile}")
            return False

        print(f"ğŸ“‚ åŠ è½½ç´¢å¼•ï¼š{self.indexFile}")

        try:
            with open(self.indexFile, "rb") as f:
                indexData = pickle.load(f)

            # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å·²å˜æ›´
            currentCorpusModTime = os.path.getmtime(self.corpusFile)
            savedCorpusModTime = indexData.get("corpusModTime")

            if savedCorpusModTime is None:
                print("âš ï¸  ç´¢å¼•ä¸­ç¼ºå°‘è¯­æ–™æ—¶é—´æˆ³ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            if abs(currentCorpusModTime - savedCorpusModTime) > 1:  # å…è®¸ 1 ç§’è¯¯å·®
                print("âš ï¸  è¯­æ–™æ–‡ä»¶å·²æ›´æ–°ï¼Œç´¢å¼•å·²è¿‡æœŸï¼Œéœ€è¦é‡å»º")
                return False

            self.bm25 = indexData["bm25"]
            self.corpus = indexData["corpus"]
            self.tokenizedCorpus = indexData["tokenizedCorpus"]
            self.termsMap = indexData.get("termsMap", {})

            print(f"âœ… å·²åŠ è½½ç´¢å¼•ï¼ˆ{len(self.corpus)} æ¡æ–‡æ¡£ï¼‰")
            return True
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥ï¼š{e}")
            return False

    def search(
        self,
        query: str,
        topK: int = 10,
        expandQuery: bool = False,
        returnAll: bool = False,
    ) -> list[dict[str, Any]]:
        """
        å•æ¬¡æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            expandQuery: æ˜¯å¦è¿›è¡ŒæŸ¥è¯¢æ‰©å±•
            returnAll: æ˜¯å¦è¿”å›æ‰€æœ‰ç»“æœï¼ˆç”¨äºæ··åˆæ£€ç´¢ï¼‰

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if self.bm25 is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ buildIndex() æˆ– loadIndex()")

        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        if expandQuery:
            tokenizedQuery = self.tokenizeForQuery(query)
        else:
            tokenizedQuery = self.tokenize(query)

        # è®¡ç®— BM25 åˆ†æ•°
        scores = self.bm25.get_scores(tokenizedQuery)

        # è·å–æ‰€æœ‰ç»“æœçš„ç´¢å¼•ï¼ˆæŒ‰åˆ†æ•°æ’åºï¼‰
        if returnAll:
            # è¿”å›æ‰€æœ‰éé›¶åˆ†æ•°çš„ç»“æœï¼šå…ˆè¿‡æ»¤ä¸ºéé›¶åˆ†æ•°ï¼Œå†æŒ‰åˆ†æ•°æ’åº
            nonzero_indices = [i for i, s in enumerate(scores) if s != 0]
            topKIndices = sorted(nonzero_indices, key=lambda i: scores[i], reverse=True)
        else:
            topKIndices = sorted(
                range(len(scores)), key=lambda i: scores[i], reverse=True
            )[:topK]

        # æ„å»ºç»“æœ
        results = []
        for rank, idx in enumerate(topKIndices, 1):
            # åœ¨ returnAll æ¨¡å¼ä¸‹ä¸è¿‡æ»¤é›¶åˆ†ï¼ˆå·²åœ¨ä¸Šé¢è¿‡æ»¤ï¼‰
            if not returnAll and rank > topK:
                break

            doc = self.corpus[idx]
            results.append(
                {
                    "rank": rank,
                    "doc_id": doc["doc_id"],
                    "term": doc["term"],
                    "subject": doc.get("subject", ""),
                    "score": float(scores[idx]),
                    "source": doc.get("source", ""),
                    "page": doc.get("page", None),
                }
            )

        return results

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        expandQuery: bool = False,
    ) -> dict[str, list[dict[str, Any]]]:
        """
        æ‰¹é‡æŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
            topK: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
            expandQuery: æ˜¯å¦è¿›è¡ŒæŸ¥è¯¢æ‰©å±•

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            results[query] = self.search(query, topK, expandQuery)
        return results


def loadQueriesFromFile(filepath: str) -> list[str]:
    """ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢"""
    queries = []
    with open(filepath, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                queries.append(line)
    return queries


def saveResults(results: dict[str, list[dict[str, Any]]], outputFile: str) -> None:
    """ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶"""
    dirname = os.path.dirname(outputFile)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    with open(outputFile, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜ï¼š{outputFile}")


def printResults(query: str, results: list[dict[str, Any]]) -> None:
    """æ‰“å°æŸ¥è¯¢ç»“æœ"""
    print("\n" + "=" * 80)
    print(f"ğŸ” æŸ¥è¯¢ï¼š{query}")
    print("=" * 80)

    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        return

    for result in results:
        print(f"\nğŸ† Rank {result['rank']}")
        print(f"  ğŸ“„ Doc ID: {result['doc_id']}")
        print(f"  ğŸ“š æœ¯è¯­ï¼š{result['term']}")
        print(f"  ğŸ“– å­¦ç§‘ï¼š{result['subject']}")
        print(f"  ğŸ“Š åˆ†æ•°ï¼š{result['score']:.4f}")
        print(f"  ğŸ“— æ¥æºï¼š{result['source']}")
        if result.get("page"):
            print(f"  ğŸ“„ é¡µç ï¼š{result['page']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="BM25+ æ”¹è¿›æ£€ç´¢")
    parser.add_argument("--query", type=str, help="å•æ¬¡æŸ¥è¯¢å­—ç¬¦ä¸²")
    parser.add_argument("--query-file", type=str, help="æ‰¹é‡æŸ¥è¯¢æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--topk", type=int, default=10, help="è¿”å›çš„ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰"
    )
    parser.add_argument("--output", type=str, help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--rebuild-index", action="store_true", help="å¼ºåˆ¶é‡æ–°æ„å»ºç´¢å¼•")
    parser.add_argument("--corpus", type=str, help="è¯­æ–™æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--index", type=str, help="ç´¢å¼•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--terms", type=str, help="æœ¯è¯­æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--expand-query", action="store_true", help="å¯ç”¨æŸ¥è¯¢æ‰©å±•")
    parser.add_argument(
        "--return-all", action="store_true", help="è¿”å›æ‰€æœ‰ç»“æœï¼ˆç”¨äºæ··åˆæ£€ç´¢ï¼‰"
    )

    args = parser.parse_args()

    # é»˜è®¤è·¯å¾„
    corpusFile = args.corpus or os.path.join(
        config.PROCESSED_DIR, "retrieval", "corpus.jsonl"
    )
    indexFile = args.index or os.path.join(
        config.PROCESSED_DIR, "retrieval", "bm25plus_index.pkl"
    )
    termsFile = args.terms or os.path.join(
        config.PROCESSED_DIR, "terms", "all_terms.json"
    )

    print("=" * 80)
    print("ğŸ” BM25+ æ”¹è¿›æ£€ç´¢")
    print("=" * 80)
    print(f"ğŸ“‚ è¯­æ–™æ–‡ä»¶ï¼š{corpusFile}")
    print(f"ğŸ“‚ ç´¢å¼•æ–‡ä»¶ï¼š{indexFile}")
    print(f"ğŸ” æŸ¥è¯¢æ‰©å±•ï¼š{'å¯ç”¨' if args.expand_query else 'ç¦ç”¨'}")
    print()

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = BM25PlusRetriever(corpusFile, indexFile, termsFile)

    # åŠ è½½æœ¯è¯­æ˜ å°„
    retriever.loadTermsMap()

    # åŠ è½½æˆ–æ„å»ºç´¢å¼•
    if args.rebuild_index or not retriever.loadIndex():
        retriever.buildIndex()
        retriever.saveIndex()

    # æ‰§è¡ŒæŸ¥è¯¢
    if args.query:
        results = retriever.search(
            args.query, args.topk, args.expand_query, args.return_all
        )
        printResults(args.query, results)

        if args.output:
            saveResults({args.query: results}, args.output)

    elif args.query_file:
        print(f"ğŸ“‚ åŠ è½½æŸ¥è¯¢ï¼š{args.query_file}")
        queries = loadQueriesFromFile(args.query_file)
        print(f"âœ… å·²åŠ è½½ {len(queries)} ä¸ªæŸ¥è¯¢\n")

        results = retriever.batchSearch(queries, args.topk, args.expand_query)

        for query, queryResults in results.items():
            printResults(query, queryResults)

        if args.output:
            saveResults(results, args.output)
        else:
            defaultOutput = os.path.join(
                config.PROJECT_ROOT, "outputs", "bm25plus_results.json"
            )
            saveResults(results, defaultOutput)

    else:
        print("âš ï¸  è¯·æä¾›æŸ¥è¯¢å‚æ•°")
        parser.print_help()


if __name__ == "__main__":
    main()
