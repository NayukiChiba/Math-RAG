"""
å‘é‡æ£€ç´¢åŸºçº¿

åŠŸèƒ½ï¼š
1. ä»è¯­æ–™æ–‡ä»¶æ„å»ºå‘é‡ç´¢å¼•ï¼ˆFAISSï¼‰
2. ä½¿ç”¨ Sentence Transformers è¿›è¡Œæ–‡æœ¬åµŒå…¥
3. æ”¯æŒå•æ¬¡æŸ¥è¯¢å’Œæ‰¹é‡æŸ¥è¯¢
4. è¾“å‡º TopK ç»“æœï¼ˆdoc_idã€termã€scoreã€rankï¼‰
5. æ”¯æŒç´¢å¼•å’ŒåµŒå…¥ä¿å­˜åŠ è½½

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•æ¬¡æŸ¥è¯¢
    python retrieval/retrievalVector.py --query "æ³°å‹’å±•å¼€" --topk 10

    # æ‰¹é‡æŸ¥è¯¢
    python retrieval/retrievalVector.py --query-file queries.txt --output results.json

    # é‡æ–°æ„å»ºç´¢å¼•
    python retrieval/retrievalVector.py --rebuild-index

    # æŒ‡å®šæ¨¡å‹
    python retrieval/retrievalVector.py --model paraphrase-multilingual-MiniLM-L12-v2
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Any

import numpy as np

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config

# å…¨å±€å˜é‡ï¼šGPU å¯ç”¨æ€§
USE_GPU = False
NUM_GPUS = 0

try:
    import faiss

    # å°è¯•æ£€æµ‹ GPUï¼ˆfaiss-gpu æ‰æœ‰æ­¤æ–¹æ³•ï¼‰
    if hasattr(faiss, "get_num_gpus"):
        try:
            NUM_GPUS = faiss.get_num_gpus()
            if NUM_GPUS > 0:
                USE_GPU = True
                print(f"ğŸ® æ£€æµ‹åˆ° {NUM_GPUS} ä¸ª GPUï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿ")
            else:
                print("ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆæœªæ£€æµ‹åˆ° GPUï¼‰")
        except Exception:
            print("ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆGPU åˆå§‹åŒ–å¤±è´¥ï¼‰")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆfaiss-cpu ç‰ˆæœ¬ï¼‰")
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–åº“ faiss")
    print("è¯·å®‰è£…:")
    print("  CPU ç‰ˆæœ¬: pip install faiss-cpu")
    print("  GPU ç‰ˆæœ¬: conda install -c pytorch -c nvidia faiss-gpu")
    sys.exit(1)

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–åº“ sentence-transformers")
    print("è¯·å®‰è£…: pip install sentence-transformers")
    sys.exit(1)


class VectorRetriever:
    """å‘é‡æ£€ç´¢å™¨"""

    def __init__(
        self,
        corpusFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
        indexFile: str | None = None,
        embeddingFile: str | None = None,
    ):
        """
        åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰
            modelName: Sentence Transformer æ¨¡å‹åç§°
            indexFile: FAISS ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä¸ä¿å­˜
            embeddingFile: åµŒå…¥å‘é‡æ–‡ä»¶è·¯å¾„ï¼ˆ.npyï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä¸ä¿å­˜
        """
        self.corpusFile = corpusFile
        self.modelName = modelName
        self.indexFile = indexFile
        self.embeddingFile = embeddingFile
        self.corpus = []
        self.model = None
        self.index = None
        self.embeddings = None

    def loadModel(self) -> None:
        """åŠ è½½ Sentence Transformer æ¨¡å‹"""
        if self.model is None:
            print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.modelName}")
            self.model = SentenceTransformer(self.modelName)
            print(
                f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆç»´åº¦: {self.model.get_sentence_embedding_dimension()}ï¼‰"
            )

    def loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™æ–‡ä»¶"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™: {self.corpusFile}")

        if not os.path.exists(self.corpusFile):
            raise FileNotFoundError(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {self.corpusFile}")

        with open(self.corpusFile, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                item = json.loads(line)
                self.corpus.append(item)

        print(f"âœ… å·²åŠ è½½ {len(self.corpus)} æ¡è¯­æ–™")

    def buildIndex(self, batchSize: int = 32) -> None:
        """
        æ„å»º FAISS ç´¢å¼•

        Args:
            batchSize: åµŒå…¥è®¡ç®—çš„æ‰¹æ¬¡å¤§å°
        """
        print("ğŸ”¨ æ„å»ºå‘é‡ç´¢å¼•...")

        if not self.corpus:
            self.loadCorpus()

        # åŠ è½½æ¨¡å‹
        self.loadModel()

        # æå–æ–‡æœ¬å­—æ®µ
        texts = [doc["text"] for doc in self.corpus]

        # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        print(f"ğŸ§® ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ‰¹æ¬¡å¤§å°: {batchSize}ï¼‰...")
        self.embeddings = self.model.encode(
            texts,
            batch_size=batchSize,
            show_progress_bar=True,
            convert_to_numpy=True,
        )

        # æ ‡å‡†åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        print("ğŸ“ æ ‡å‡†åŒ–å‘é‡...")
        faiss.normalize_L2(self.embeddings)

        # æ„å»º FAISS ç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ï¼Œå› ä¸ºå‘é‡å·²æ ‡å‡†åŒ–ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        dimension = self.embeddings.shape[1]
        cpuIndex = faiss.IndexFlatIP(dimension)  # Inner Product (ä½™å¼¦ç›¸ä¼¼åº¦)

        # å¦‚æœæœ‰ GPUï¼Œå°†ç´¢å¼•è¿ç§»åˆ° GPU
        if USE_GPU:
            res = faiss.StandardGpuResources()  # ä½¿ç”¨é»˜è®¤ GPU èµ„æº
            self.index = faiss.index_cpu_to_gpu(res, 0, cpuIndex)  # è¿ç§»åˆ° GPU 0
            print("ğŸ® ç´¢å¼•å·²è¿ç§»åˆ° GPU")
        else:
            self.index = cpuIndex

        self.index.add(self.embeddings)

        deviceType = "GPU" if USE_GPU else "CPU"
        print(
            f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼ˆ{self.index.ntotal} æ¡æ–‡æ¡£ï¼Œç»´åº¦: {dimension}ï¼Œè®¾å¤‡: {deviceType}ï¼‰"
        )

    def saveIndex(self) -> None:
        """ä¿å­˜ç´¢å¼•å’ŒåµŒå…¥åˆ°æ–‡ä»¶"""
        if self.index is None or self.embeddings is None:
            print("âš ï¸  ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
            return

        # ä¿å­˜ FAISS ç´¢å¼•
        if self.indexFile:
            print(f"ğŸ’¾ ä¿å­˜ FAISS ç´¢å¼•: {self.indexFile}")
            dirname = os.path.dirname(self.indexFile)
            if dirname:  # åªæœ‰å½“ç›®å½•åéç©ºæ—¶æ‰åˆ›å»ºç›®å½•
                os.makedirs(dirname, exist_ok=True)

            # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
            metadata = {
                "corpusFile": self.corpusFile,
                "corpusModTime": os.path.getmtime(self.corpusFile),
                "modelName": self.modelName,
                "dimension": self.embeddings.shape[1],
                "numDocs": len(self.corpus),
            }

            # FAISS ç´¢å¼•ä¿å­˜ï¼ˆGPU ç´¢å¼•éœ€è¦å…ˆè½¬å› CPUï¼‰
            if USE_GPU:
                cpuIndex = faiss.index_gpu_to_cpu(self.index)
                faiss.write_index(cpuIndex, self.indexFile)
            else:
                faiss.write_index(self.index, self.indexFile)

            # å…ƒæ•°æ®ä¿å­˜
            metadataFile = self.indexFile + ".meta.json"
            with open(metadataFile, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            print("âœ… FAISS ç´¢å¼•å·²ä¿å­˜")

        # ä¿å­˜åµŒå…¥å‘é‡
        if self.embeddingFile:
            print(f"ğŸ’¾ ä¿å­˜åµŒå…¥å‘é‡: {self.embeddingFile}")
            dirname = os.path.dirname(self.embeddingFile)
            if dirname:  # åªæœ‰å½“ç›®å½•åéç©ºæ—¶æ‰åˆ›å»ºç›®å½•
                os.makedirs(dirname, exist_ok=True)

            # ä¿å­˜åµŒå…¥å’Œè¯­æ–™
            np.savez_compressed(
                self.embeddingFile,
                embeddings=self.embeddings,
                corpus=np.array(self.corpus, dtype=object),
            )

            print("âœ… åµŒå…¥å‘é‡å·²ä¿å­˜")

    def loadIndex(self) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•å’ŒåµŒå…¥

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if self.indexFile is None or not os.path.exists(self.indexFile):
            return False

        # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.corpusFile):
            print(f"âš ï¸  è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {self.corpusFile}")
            return False

        print(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {self.indexFile}")

        try:
            # åŠ è½½å…ƒæ•°æ®
            metadataFile = self.indexFile + ".meta.json"
            if not os.path.exists(metadataFile):
                print("âš ï¸  ç´¢å¼•å…ƒæ•°æ®ä¸å­˜åœ¨ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            with open(metadataFile, encoding="utf-8") as f:
                metadata = json.load(f)

            # æ ¡éªŒè¯­æ–™æ–‡ä»¶æ˜¯å¦å·²å˜æ›´
            currentCorpusModTime = os.path.getmtime(self.corpusFile)
            savedCorpusModTime = metadata.get("corpusModTime")

            if savedCorpusModTime is None:
                print("âš ï¸  ç´¢å¼•ä¸­ç¼ºå°‘è¯­æ–™æ—¶é—´æˆ³ï¼Œå»ºè®®é‡å»ºç´¢å¼•")
                return False

            if abs(currentCorpusModTime - savedCorpusModTime) > 1:  # å…è®¸1ç§’è¯¯å·®
                print("âš ï¸  è¯­æ–™æ–‡ä»¶å·²æ›´æ–°ï¼Œç´¢å¼•å·²è¿‡æœŸï¼Œéœ€è¦é‡å»º")
                return False

            # æ ¡éªŒæ¨¡å‹æ˜¯å¦ä¸€è‡´
            if metadata.get("modelName") != self.modelName:
                print(
                    f"âš ï¸  æ¨¡å‹ä¸ä¸€è‡´ï¼ˆä¿å­˜: {metadata.get('modelName')}, å½“å‰: {self.modelName}ï¼‰"
                )
                print("å»ºè®®é‡å»ºç´¢å¼•æˆ–ä½¿ç”¨ç›¸åŒæ¨¡å‹")
                return False

            # åŠ è½½ FAISS ç´¢å¼•
            cpuIndex = faiss.read_index(self.indexFile)

            # å¦‚æœæœ‰ GPUï¼Œå°†ç´¢å¼•è¿ç§»åˆ° GPU
            if USE_GPU:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, cpuIndex)
                print("ğŸ® ç´¢å¼•å·²è¿ç§»åˆ° GPU")
            else:
                self.index = cpuIndex

            # åŠ è½½åµŒå…¥å’Œè¯­æ–™
            if self.embeddingFile and os.path.exists(self.embeddingFile):
                data = np.load(self.embeddingFile, allow_pickle=True)
                self.embeddings = data["embeddings"]
                self.corpus = data["corpus"].tolist()
            else:
                # å¦‚æœåµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°åŠ è½½è¯­æ–™
                print("âš ï¸  åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°åŠ è½½è¯­æ–™")
                self.loadCorpus()

            # åŠ è½½æ¨¡å‹ï¼ˆç”¨äºæŸ¥è¯¢åµŒå…¥ï¼‰
            self.loadModel()

            print(
                f"âœ… å·²åŠ è½½ç´¢å¼•ï¼ˆ{self.index.ntotal} æ¡æ–‡æ¡£ï¼Œç»´åº¦: {metadata['dimension']}ï¼‰"
            )
            return True

        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False

    def search(self, query: str, topK: int = 10) -> list[dict[str, Any]]:
        """
        å•æ¬¡æŸ¥è¯¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« doc_idã€termã€scoreã€rank
        """
        if self.index is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ buildIndex() æˆ– loadIndex()")

        if self.model is None:
            self.loadModel()

        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        queryEmbedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(queryEmbedding)  # æ ‡å‡†åŒ–

        # æ‰§è¡Œæœç´¢
        scores, indices = self.index.search(queryEmbedding, topK)

        # æ„å»ºç»“æœ
        results = []
        for rank, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):
            if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— æ•ˆç»“æœ
                continue

            doc = self.corpus[idx]
            results.append(
                {
                    "rank": rank,
                    "doc_id": doc["doc_id"],
                    "term": doc["term"],
                    "subject": doc.get("subject", ""),
                    "score": float(score),
                    "source": doc.get("source", ""),
                    "page": doc.get("page", None),
                }
            )

        return results

    def batchSearch(
        self, queries: list[str], topK: int = 10
    ) -> dict[str, list[dict[str, Any]]]:
        """
        æ‰¹é‡æŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
            topK: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå€¼ä¸ºç»“æœåˆ—è¡¨
        """
        results = {}
        for query in queries:
            results[query] = self.search(query, topK)
        return results


def loadQueriesFromFile(filepath: str) -> list[str]:
    """
    ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢

    Args:
        filepath: æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼‰

    Returns:
        æŸ¥è¯¢åˆ—è¡¨
    """
    queries = []
    with open(filepath, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                queries.append(line)
    return queries


def saveResults(results: dict[str, list[dict[str, Any]]], outputFile: str) -> None:
    """
    ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶

    Args:
        results: æŸ¥è¯¢ç»“æœå­—å…¸
        outputFile: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    dirname = os.path.dirname(outputFile)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    with open(outputFile, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {outputFile}")


def printResults(query: str, results: list[dict[str, Any]]) -> None:
    """
    æ‰“å°æŸ¥è¯¢ç»“æœ

    Args:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        results: ç»“æœåˆ—è¡¨
    """
    print("\n" + "=" * 80)
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print("=" * 80)

    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        return

    for result in results:
        print(f"\nğŸ† Rank {result['rank']}")
        print(f"  ğŸ“„ Doc ID: {result['doc_id']}")
        print(f"  ğŸ“š æœ¯è¯­: {result['term']}")
        print(f"  ğŸ“– å­¦ç§‘: {result['subject']}")
        print(f"  ğŸ“Š åˆ†æ•°: {result['score']:.4f}")
        print(f"  ğŸ“— æ¥æº: {result['source']}")
        if result.get("page"):
            print(f"  ğŸ“„ é¡µç : {result['page']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å‘é‡æ£€ç´¢åŸºçº¿")
    parser.add_argument("--query", type=str, help="å•æ¬¡æŸ¥è¯¢å­—ç¬¦ä¸²")
    parser.add_argument("--query-file", type=str, help="æ‰¹é‡æŸ¥è¯¢æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--topk", type=int, default=10, help="è¿”å›çš„ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰"
    )
    parser.add_argument("--output", type=str, help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆJSON æ ¼å¼ï¼‰")
    parser.add_argument("--rebuild-index", action="store_true", help="å¼ºåˆ¶é‡æ–°æ„å»ºç´¢å¼•")
    parser.add_argument("--corpus", type=str, help="è¯­æ–™æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--index", type=str, help="FAISS ç´¢å¼•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--embedding", type=str, help="åµŒå…¥å‘é‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--model",
        type=str,
        default="paraphrase-multilingual-MiniLM-L12-v2",
        help="Sentence Transformer æ¨¡å‹åç§°",
    )
    parser.add_argument(
        "--batch-size", type=int, default=32, help="åµŒå…¥è®¡ç®—çš„æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 32ï¼‰"
    )

    args = parser.parse_args()

    # é»˜è®¤è·¯å¾„
    corpusFile = args.corpus or os.path.join(
        config.PROCESSED_DIR, "retrieval", "corpus.jsonl"
    )
    indexFile = args.index or os.path.join(
        config.PROCESSED_DIR, "retrieval", "vector_index.faiss"
    )
    embeddingFile = args.embedding or os.path.join(
        config.PROCESSED_DIR, "retrieval", "vector_embeddings.npz"
    )

    print("=" * 80)
    print("ğŸ” å‘é‡æ£€ç´¢åŸºçº¿")
    print("=" * 80)
    print(f"ğŸ“‚ è¯­æ–™æ–‡ä»¶: {corpusFile}")
    print(f"ğŸ“‚ ç´¢å¼•æ–‡ä»¶: {indexFile}")
    print(f"ğŸ“‚ åµŒå…¥æ–‡ä»¶: {embeddingFile}")
    print(f"ğŸ¤– æ¨¡å‹: {args.model}")
    print()

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = VectorRetriever(corpusFile, args.model, indexFile, embeddingFile)

    # åŠ è½½æˆ–æ„å»ºç´¢å¼•
    if args.rebuild_index or not retriever.loadIndex():
        retriever.buildIndex(batchSize=args.batch_size)
        retriever.saveIndex()

    # æ‰§è¡ŒæŸ¥è¯¢
    if args.query:
        # å•æ¬¡æŸ¥è¯¢
        results = retriever.search(args.query, args.topk)
        printResults(args.query, results)

        if args.output:
            saveResults({args.query: results}, args.output)

    elif args.query_file:
        # æ‰¹é‡æŸ¥è¯¢
        print(f"ğŸ“‚ åŠ è½½æŸ¥è¯¢: {args.query_file}")
        queries = loadQueriesFromFile(args.query_file)
        print(f"âœ… å·²åŠ è½½ {len(queries)} ä¸ªæŸ¥è¯¢\n")

        results = retriever.batchSearch(queries, args.topk)

        # æ‰“å°æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœ
        for query, queryResults in results.items():
            printResults(query, queryResults)

        # ä¿å­˜ç»“æœ
        if args.output:
            saveResults(results, args.output)
        else:
            # é»˜è®¤è¾“å‡ºæ–‡ä»¶
            defaultOutput = os.path.join(
                config.PROJECT_ROOT, "outputs", "vector_results.json"
            )
            saveResults(results, defaultOutput)

    else:
        print("âš ï¸  è¯·æä¾›æŸ¥è¯¢å‚æ•°ï¼š")
        print("  --query 'your query'  # å•æ¬¡æŸ¥è¯¢")
        print("  --query-file queries.txt  # æ‰¹é‡æŸ¥è¯¢")
        parser.print_help()


if __name__ == "__main__":
    main()
