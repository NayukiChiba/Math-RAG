"""
æ„å»ºæ£€ç´¢è¯­æ–™

åŠŸèƒ½ï¼š
1. ä» chunk JSON æ–‡ä»¶ä¸­æå–æœ¯è¯­æ•°æ®
2. æŒ‰è§„åˆ™æ‹¼æ¥æ–‡æœ¬å­—æ®µ
3. ç”Ÿæˆ JSONL æ ¼å¼çš„æ£€ç´¢è¯­æ–™

æ–‡æœ¬æ‹¼æ¥é¡ºåºï¼š
term â†’ aliases â†’ definitions.text â†’ formula â†’ usage â†’ applications â†’ disambiguation â†’ related_terms

ä½¿ç”¨æ–¹æ³•ï¼š
    python retrieval/buildCorpus.py
"""

import json
import os
import sys
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config


def loadJsonFile(filepath: str) -> dict[str, Any] | None:
    """
    åŠ è½½ JSON æ–‡ä»¶

    Args:
        filepath: JSON æ–‡ä»¶è·¯å¾„

    Returns:
        è§£æåçš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        with open(filepath, encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {filepath}, é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {e}")
        return None


def buildTextFromTerm(termData: dict[str, Any]) -> str:
    """
    ä»æœ¯è¯­æ•°æ®æ„å»ºæ‹¼æ¥æ–‡æœ¬

    æ‹¼æ¥é¡ºåºï¼šterm â†’ aliases â†’ definitions.text â†’ formula â†’ usage â†’ applications â†’ disambiguation â†’ related_terms

    Args:
        termData: æœ¯è¯­æ•°æ®å­—å…¸

    Returns:
        æ‹¼æ¥åçš„æ–‡æœ¬å­—ç¬¦ä¸²
    """
    textParts = []

    # 1. termï¼ˆæœ¯è¯­åç§°ï¼‰
    term = termData.get("term", "").strip()
    if term:
        textParts.append(f"æœ¯è¯­: {term}")

    # 2. aliasesï¼ˆåˆ«åï¼‰
    aliases = termData.get("aliases", [])
    if aliases and isinstance(aliases, list):
        aliasesText = "ã€".join([a.strip() for a in aliases if a])
        if aliasesText:
            textParts.append(f"åˆ«å: {aliasesText}")

    # 3. definitionsï¼ˆå®šä¹‰ï¼‰
    definitions = termData.get("definitions", [])
    if definitions and isinstance(definitions, list):
        for idx, defItem in enumerate(definitions, 1):
            if isinstance(defItem, dict):
                defText = defItem.get("text", "").strip()
                if defText:
                    defType = defItem.get("type", "")
                    typeLabel = f"[{defType}]" if defType else ""
                    textParts.append(f"å®šä¹‰{idx}{typeLabel}: {defText}")

                    # æ·»åŠ æ¡ä»¶å’Œè®°å·ï¼ˆå¦‚æœæœ‰ï¼‰
                    conditions = defItem.get("conditions", "").strip()
                    if conditions:
                        textParts.append(f"  æ¡ä»¶: {conditions}")

                    notation = defItem.get("notation", "").strip()
                    if notation:
                        textParts.append(f"  è®°å·: {notation}")

    # 4. notationï¼ˆç¬¦å·ï¼‰
    notation = termData.get("notation", "")
    if notation:
        if isinstance(notation, str):
            notationText = notation.strip()
            if notationText:
                textParts.append(f"ç¬¦å·: {notationText}")
        elif isinstance(notation, list):
            notationText = "ã€".join([n.strip() for n in notation if n])
            if notationText:
                textParts.append(f"ç¬¦å·: {notationText}")

    # 5. formulaï¼ˆå…¬å¼ï¼‰
    formulas = termData.get("formula", [])
    if formulas and isinstance(formulas, list):
        for idx, formula in enumerate(formulas, 1):
            if formula and isinstance(formula, str):
                formulaText = formula.strip()
                if formulaText:
                    textParts.append(f"å…¬å¼{idx}: {formulaText}")

    # 6. usageï¼ˆç”¨æ³•ï¼‰
    usage = termData.get("usage", "")
    if usage:
        if isinstance(usage, str):
            usageText = usage.strip()
            if usageText:
                textParts.append(f"ç”¨æ³•: {usageText}")
        elif isinstance(usage, list):
            usageText = " ".join([u.strip() for u in usage if u])
            if usageText:
                textParts.append(f"ç”¨æ³•: {usageText}")

    # 7. applicationsï¼ˆåº”ç”¨ï¼‰
    applications = termData.get("applications", "")
    if applications:
        if isinstance(applications, str):
            appText = applications.strip()
            if appText:
                textParts.append(f"åº”ç”¨: {appText}")
        elif isinstance(applications, list):
            appText = " ".join([a.strip() for a in applications if a])
            if appText:
                textParts.append(f"åº”ç”¨: {appText}")

    # 8. disambiguationï¼ˆæ¶ˆæ­§ï¼‰
    disambiguation = termData.get("disambiguation", "")
    if disambiguation:
        if isinstance(disambiguation, str):
            disambigText = disambiguation.strip()
            if disambigText:
                textParts.append(f"åŒºåˆ†: {disambigText}")
        elif isinstance(disambiguation, list):
            disambigText = " ".join([d.strip() for d in disambiguation if d])
            if disambigText:
                textParts.append(f"åŒºåˆ†: {disambigText}")

    # 9. related_termsï¼ˆç›¸å…³æœ¯è¯­ï¼‰
    relatedTerms = termData.get("related_terms", [])
    if relatedTerms and isinstance(relatedTerms, list):
        relatedText = "ã€".join([t.strip() for t in relatedTerms if t])
        if relatedText:
            textParts.append(f"ç›¸å…³æœ¯è¯­: {relatedText}")

    # ç”¨æ¢è¡Œç¬¦æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†
    return "\n".join(textParts)


def extractCorpusItem(termData: dict[str, Any], bookName: str) -> dict[str, Any] | None:
    """
    ä»æœ¯è¯­æ•°æ®æå–è¯­æ–™é¡¹

    Args:
        termData: æœ¯è¯­æ•°æ®å­—å…¸
        bookName: ä¹¦ç±åç§°

    Returns:
        è¯­æ–™é¡¹å­—å…¸ï¼ˆåŒ…å« doc_id, term, subject, text, source, pageï¼‰ï¼Œå¤±è´¥è¿”å› None
    """
    # å¿…éœ€å­—æ®µæ£€æŸ¥
    docId = termData.get("id", "").strip()
    term = termData.get("term", "").strip()
    subject = termData.get("subject", "").strip()

    if not docId or not term:
        return None

    # æ„å»ºæ‹¼æ¥æ–‡æœ¬
    text = buildTextFromTerm(termData)

    if not text:
        return None

    # æå–æ¥æºä¿¡æ¯å’Œé¡µç 
    sources = termData.get("sources", [])
    page = None
    if sources and isinstance(sources, list) and len(sources) > 0:
        firstSource = sources[0]
        # sources æ˜¯å­—ç¬¦ä¸²æ•°ç»„ï¼Œå¦‚ "æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡æ•™ç¨‹ç¬¬ä¸‰ç‰ˆ(èŒ†è¯—æ¾) ç¬¬104é¡µ"
        if isinstance(firstSource, str):
            # ä½¿ç”¨æ­£åˆ™æå–é¡µç ï¼ŒåŒ¹é… "ç¬¬Xé¡µ" æˆ– "p.X" æˆ– "pp.X" ç­‰æ ¼å¼
            import re

            pageMatch = re.search(r"ç¬¬(\d+)é¡µ|p\.?\s*(\d+)|pp\.?\s*(\d+)", firstSource)
            if pageMatch:
                # è·å–ç¬¬ä¸€ä¸ªéç©ºåŒ¹é…ç»„
                page = next((int(g) for g in pageMatch.groups() if g), None)

    # æ„å»ºè¯­æ–™é¡¹
    corpusItem = {
        "doc_id": docId,
        "term": term,
        "subject": subject if subject else "æœªåˆ†ç±»",
        "text": text,
        "source": bookName,
    }

    # æ·»åŠ é¡µç ï¼ˆå¦‚æœæœ‰ï¼‰
    if page is not None:
        corpusItem["page"] = page

    return corpusItem


def buildCorpus(chunkDir: str, outputFile: str) -> dict[str, Any]:
    """
    æ„å»ºæ£€ç´¢è¯­æ–™

    Args:
        chunkDir: æœ¯è¯­æ•°æ®ç›®å½•
        outputFile: è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {
        "totalFiles": 0,
        "validFiles": 0,
        "skippedFiles": 0,
        "corpusItems": 0,
        "bookStats": {},
    }

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(outputFile), exist_ok=True)

    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶
    with open(outputFile, "w", encoding="utf-8") as outFile:
        # éå†æ‰€æœ‰ä¹¦ç±ç›®å½•
        for bookName in os.listdir(chunkDir):
            bookPath = os.path.join(chunkDir, bookName)

            # è·³è¿‡éç›®å½•
            if not os.path.isdir(bookPath):
                continue

            print(f"ğŸ“– å¤„ç†ä¹¦ç±: {bookName}")

            stats["bookStats"][bookName] = {
                "totalFiles": 0,
                "validItems": 0,
                "skippedItems": 0,
            }

            # éå†è¯¥ä¹¦ç±ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶
            jsonFiles = [f for f in os.listdir(bookPath) if f.endswith(".json")]

            for jsonFile in jsonFiles:
                filepath = os.path.join(bookPath, jsonFile)
                stats["totalFiles"] += 1
                stats["bookStats"][bookName]["totalFiles"] += 1

                # åŠ è½½ JSON æ•°æ®
                termData = loadJsonFile(filepath)

                if termData is None:
                    stats["skippedFiles"] += 1
                    stats["bookStats"][bookName]["skippedItems"] += 1
                    continue

                stats["validFiles"] += 1

                # æå–è¯­æ–™é¡¹
                corpusItem = extractCorpusItem(termData, bookName)

                if corpusItem is None:
                    stats["skippedFiles"] += 1
                    stats["bookStats"][bookName]["skippedItems"] += 1
                    continue

                # å†™å…¥ JSONLï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
                outFile.write(json.dumps(corpusItem, ensure_ascii=False) + "\n")

                stats["corpusItems"] += 1
                stats["bookStats"][bookName]["validItems"] += 1

            print(f"  âœ… ç”Ÿæˆ {stats['bookStats'][bookName]['validItems']} æ¡è¯­æ–™é¡¹")

    return stats


def validateCorpusFile(corpusFile: str) -> dict[str, Any]:
    """
    éªŒè¯è¯­æ–™æ–‡ä»¶æ ¼å¼

    Args:
        corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„

    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    result = {
        "valid": True,
        "totalLines": 0,
        "validLines": 0,
        "errorLines": [],
        "sampleItems": [],
    }

    try:
        with open(corpusFile, encoding="utf-8") as f:
            for lineNum, line in enumerate(f, 1):
                result["totalLines"] += 1
                line = line.strip()

                if not line:
                    continue

                try:
                    item = json.loads(line)

                    # æ£€æŸ¥å¿…éœ€å­—æ®µï¼ˆåŒ…æ‹¬ pageï¼Œç¬¦åˆä»»åŠ¡éªŒæ”¶æ ‡å‡†ï¼‰
                    requiredFields = [
                        "doc_id",
                        "term",
                        "subject",
                        "text",
                        "source",
                        "page",
                    ]
                    missingFields = [
                        field for field in requiredFields if field not in item
                    ]

                    if missingFields:
                        result["errorLines"].append(
                            {
                                "line": lineNum,
                                "error": f"ç¼ºå°‘å­—æ®µ: {', '.join(missingFields)}",
                            }
                        )
                        result["valid"] = False
                    else:
                        result["validLines"] += 1

                        # ä¿å­˜å‰3æ¡ä½œä¸ºæ ·æœ¬
                        if len(result["sampleItems"]) < 3:
                            result["sampleItems"].append(item)

                except json.JSONDecodeError as e:
                    result["errorLines"].append(
                        {"line": lineNum, "error": f"JSON è§£æé”™è¯¯: {e}"}
                    )
                    result["valid"] = False

    except Exception as e:
        result["valid"] = False
        result["error"] = str(e)

    return result


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ” æ„å»ºæ£€ç´¢è¯­æ–™")
    print("=" * 60)

    # è¾“å…¥è¾“å‡ºè·¯å¾„
    chunkDir = config.CHUNK_DIR
    retrievalDir = os.path.join(config.PROCESSED_DIR, "retrieval")
    outputFile = os.path.join(retrievalDir, "corpus.jsonl")

    print(f"\nğŸ“‚ è¾“å…¥ç›®å½•: {chunkDir}")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {outputFile}\n")

    # æ„å»ºè¯­æ–™
    print("ğŸ”„ å¼€å§‹æ„å»ºè¯­æ–™...\n")
    stats = buildCorpus(chunkDir, outputFile)

    print("\n" + "=" * 60)
    print("ğŸ“Š æ„å»ºç»Ÿè®¡")
    print("=" * 60)
    print(f"æ€»æ–‡ä»¶æ•°: {stats['totalFiles']}")
    print(f"æœ‰æ•ˆæ–‡ä»¶: {stats['validFiles']}")
    print(f"è·³è¿‡æ–‡ä»¶: {stats['skippedFiles']}")
    print(f"è¯­æ–™é¡¹æ•°: {stats['corpusItems']}")

    print("\nğŸ“š å„ä¹¦ç±ç»Ÿè®¡:")
    for bookName, bookStat in stats["bookStats"].items():
        print(f"  - {bookName}:")
        print(f"    æ–‡ä»¶æ•°: {bookStat['totalFiles']}")
        print(f"    æœ‰æ•ˆé¡¹: {bookStat['validItems']}")
        print(f"    è·³è¿‡é¡¹: {bookStat['skippedItems']}")

    # éªŒè¯è¾“å‡ºæ–‡ä»¶
    print("\n" + "=" * 60)
    print("âœ… éªŒè¯è¯­æ–™æ–‡ä»¶")
    print("=" * 60)
    validation = validateCorpusFile(outputFile)

    print(f"æ€»è¡Œæ•°: {validation['totalLines']}")
    print(f"æœ‰æ•ˆè¡Œæ•°: {validation['validLines']}")

    if validation["errorLines"]:
        print(f"\nâš ï¸  å‘ç° {len(validation['errorLines'])} ä¸ªé”™è¯¯:")
        for error in validation["errorLines"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  è¡Œ {error['line']}: {error['error']}")
    else:
        print("âœ… æ‰€æœ‰è¡Œæ ¼å¼æ­£ç¡®ï¼")

    # æ˜¾ç¤ºæ ·æœ¬
    if validation["sampleItems"]:
        print("\nğŸ“ æ ·æœ¬æ•°æ®ï¼ˆå‰3æ¡ï¼‰:")
        for idx, item in enumerate(validation["sampleItems"], 1):
            print(f"\næ ·æœ¬ {idx}:")
            print(f"  doc_id: {item['doc_id']}")
            print(f"  term: {item['term']}")
            print(f"  subject: {item['subject']}")
            print(f"  source: {item['source']}")
            if "page" in item:
                print(f"  page: {item['page']}")
            # æ˜¾ç¤ºæ–‡æœ¬çš„å‰200ä¸ªå­—ç¬¦
            textPreview = (
                item["text"][:200] + "..." if len(item["text"]) > 200 else item["text"]
            )
            print(f"  text: {textPreview}")

    print("\n" + "=" * 60)
    print("âœ… è¯­æ–™æ„å»ºå®Œæˆï¼")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {outputFile}")
    print("=" * 60)


if __name__ == "__main__":
    main()
