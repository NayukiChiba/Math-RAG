"""
é«˜çº§æ£€ç´¢ - å¤šè·¯å¬å› + é‡æ’åº

åŠŸèƒ½ï¼š
1. å¤šè·¯å¬å›ï¼ˆBM25 + å‘é‡ + æŸ¥è¯¢æ”¹å†™ï¼‰
2. Cross-Encoder é‡æ’åº
3. æŸ¥è¯¢æ”¹å†™æ‰©å±•
4. æ”¯æŒé…ç½®åŒ–ç­–ç•¥

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•æ¬¡æŸ¥è¯¢
    python retrieval/retrievalAdvanced.py --query "æ³°å‹’å±•å¼€" --topk 10

    # å¯ç”¨é‡æ’åº
    python retrieval/retrievalAdvanced.py --query "æ³°å‹’å±•å¼€" --topk 10 --use-reranker

    # å¯ç”¨æŸ¥è¯¢æ”¹å†™
    python retrieval/retrievalAdvanced.py --query "æ³°å‹’å±•å¼€" --topk 10 --rewrite-query
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Any

# è·¯å¾„è°ƒæ•´
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config
from retrieval.queryRewrite import QueryRewriter


class AdvancedRetriever:
    """é«˜çº§æ£€ç´¢å™¨ - å¤šè·¯å¬å› + é‡æ’åº"""

    def __init__(
        self,
        corpusFile: str,
        bm25IndexFile: str,
        vectorIndexFile: str,
        vectorEmbeddingFile: str,
        modelName: str = "paraphrase-multilingual-MiniLM-L12-v2",
        rerankerModel: str = "BAAI/bge-reranker-v2-mixed",
        termsFile: str | None = None,
    ):
        """
        åˆå§‹åŒ–é«˜çº§æ£€ç´¢å™¨

        Args:
            corpusFile: è¯­æ–™æ–‡ä»¶è·¯å¾„
            bm25IndexFile: BM25 ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorIndexFile: å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
            vectorEmbeddingFile: å‘é‡åµŒå…¥æ–‡ä»¶è·¯å¾„
            modelName: Sentence Transformer æ¨¡å‹åç§°
            rerankerModel: é‡æ’åºæ¨¡å‹åç§°
            termsFile: æœ¯è¯­æ–‡ä»¶è·¯å¾„
        """
        self.corpusFile = corpusFile
        self.bm25IndexFile = bm25IndexFile
        self.vectorIndexFile = vectorIndexFile
        self.vectorEmbeddingFile = vectorEmbeddingFile
        self.modelName = modelName
        self.rerankerModelName = rerankerModel

        # å»¶è¿ŸåŠ è½½ï¼Œé¿å…ä¸å¿…è¦çš„å¯¼å…¥
        self._bm25 = None
        self._vectorModel = None
        self._vectorIndex = None
        self._reranker = None
        self._queryRewriter = None
        self._corpus = None

        # é¢„åŠ è½½è¯­æ–™
        self._loadCorpus()

    def _loadCorpus(self) -> None:
        """åŠ è½½è¯­æ–™åº“"""
        print(f"ğŸ“‚ åŠ è½½è¯­æ–™ï¼š{self.corpusFile}")
        self._corpus = []
        with open(self.corpusFile, encoding="utf-8") as f:
            for line in f:
                self._corpus.append(json.loads(line.strip()))
        print(f"âœ… å·²åŠ è½½ {len(self._corpus)} æ¡è¯­æ–™")

    def _loadBM25(self):
        """æ‡’åŠ è½½ BM25"""
        if self._bm25 is not None:
            return

        import pickle

        print("ğŸ“‚ åŠ è½½ BM25 ç´¢å¼•...")
        with open(self.bm25IndexFile, "rb") as f:
            indexData = pickle.load(f)

        self._bm25 = indexData["bm25"]
        print("âœ… BM25 ç´¢å¼•åŠ è½½å®Œæˆ")

    def _loadVectorIndex(self):
        """æ‡’åŠ è½½å‘é‡ç´¢å¼•"""
        if self._vectorIndex is not None:
            return

        import faiss
        from sentence_transformers import SentenceTransformer

        print(f"ğŸ¤– åŠ è½½å‘é‡æ¨¡å‹ï¼š{self.modelName}")
        self._vectorModel = SentenceTransformer(self.modelName)

        print("ğŸ“‚ åŠ è½½å‘é‡ç´¢å¼•...")
        self._vectorIndex = faiss.read_index(self.vectorIndexFile)
        print("âœ… å‘é‡ç´¢å¼•åŠ è½½å®Œæˆ")

    def _loadReranker(self):
        """æ‡’åŠ è½½é‡æ’åºå™¨"""
        if self._reranker is not None:
            return

        # æ£€æŸ¥æ˜¯å¦å·²æ ‡è®°ä¸ºä¸å¯ç”¨
        if getattr(self, "_rerankerUnavailable", False):
            return

        from sentence_transformers import CrossEncoder

        print(f"ğŸ¤– åŠ è½½é‡æ’åºæ¨¡å‹ï¼š{self.rerankerModelName}")
        try:
            self._reranker = CrossEncoder(self.rerankerModelName)
            print("âœ… é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}ï¼Œå°†ä¸ä½¿ç”¨é‡æ’åº")
            self._rerankerUnavailable = True
            self._reranker = None

    def _loadQueryRewriter(self, termsFile: str | None = None):
        """æ‡’åŠ è½½æŸ¥è¯¢æ”¹å†™å™¨"""
        if self._queryRewriter is not None:
            return

        self._queryRewriter = QueryRewriter(termsFile)
        print("âœ… æŸ¥è¯¢æ”¹å†™å™¨åŠ è½½å®Œæˆ")

    def _bm25Search(self, query: str, topK: int = 50) -> list[tuple[int, float]]:
        """BM25 æ£€ç´¢"""
        self._loadBM25()

        tokens = query.split()
        scores = self._bm25.get_scores(tokens)

        topIndices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :topK
        ]
        return [(idx, float(scores[idx])) for idx in topIndices if scores[idx] > 0]

    def _vectorSearch(self, query: str, topK: int = 50) -> list[tuple[int, float]]:
        """å‘é‡æ£€ç´¢"""
        self._loadVectorIndex()

        queryEmbedding = self._vectorModel.encode([query], convert_to_numpy=True)
        import faiss

        faiss.normalize_L2(queryEmbedding)

        scores, indices = self._vectorIndex.search(queryEmbedding, topK)
        return [
            (idx, float(score))
            for idx, score in zip(indices[0], scores[0])
            if idx != -1
        ]

    def _rerankScores(
        self, query: str, candidates: list[tuple[int, str]]
    ) -> list[float] | None:
        """ä½¿ç”¨ Cross-Encoder è®¡ç®—é‡æ’åºåˆ†æ•°"""
        self._loadReranker()

        if self._reranker is None:
            return None

        pairs = [[query, text] for _, text in candidates]
        scores = self._reranker.predict(pairs)
        return [float(s) for s in scores]

    def _getDocText(self, idx: int) -> str:
        """è·å–æ–‡æ¡£æ–‡æœ¬"""
        return self._corpus[idx].get("text", "")

    def search(
        self,
        query: str,
        topK: int = 10,
        recallTopK: int = 100,
        useReranker: bool = True,
        rewriteQuery: bool = True,
        bm25Weight: float = 0.4,
        vectorWeight: float = 0.3,
        rewriteWeight: float = 0.3,
    ) -> list[dict[str, Any]]:
        """
        é«˜çº§æ£€ç´¢ - å¤šè·¯å¬å› + é‡æ’åº

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            topK: è¿”å›çš„ç»“æœæ•°é‡
            recallTopK: æ¯è·¯å¬å›çš„æ•°é‡
            useReranker: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            rewriteQuery: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ”¹å†™
            bm25Weight: BM25 æƒé‡
            vectorWeight: å‘é‡æ£€ç´¢æƒé‡
            rewriteWeight: æŸ¥è¯¢æ”¹å†™æƒé‡

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        startTime = time.time()

        # 1. æŸ¥è¯¢æ”¹å†™
        if rewriteQuery:
            self._loadQueryRewriter()
            rewrittenQueries = self._queryRewriter.rewrite(query)
            print(f"ğŸ”„ æŸ¥è¯¢æ”¹å†™ï¼š{query} -> {rewrittenQueries}")
        else:
            rewrittenQueries = [query]

        # 2. å¤šè·¯å¬å›
        allCandidates = {}  # doc_idx -> (doc_idx, max_score, text)

        # BM25 å¬å›
        bm25Results = self._bm25Search(query, recallTopK)
        for idx, score in bm25Results:
            allCandidates[idx] = {"bm25_score": score, "vector_score": 0.0}

        # å‘é‡å¬å›
        vectorResults = self._vectorSearch(query, recallTopK)
        for idx, score in vectorResults:
            if idx in allCandidates:
                allCandidates[idx]["vector_score"] = score
            else:
                allCandidates[idx] = {"bm25_score": 0.0, "vector_score": score}

        # æŸ¥è¯¢æ”¹å†™å¬å›
        if rewriteQuery and len(rewrittenQueries) > 1:
            for rewrittenQuery in rewrittenQueries[1:4]:  # ç”¨å‰ 3 ä¸ªæ”¹å†™æŸ¥è¯¢
                rewriteBm25 = self._bm25Search(rewrittenQuery, recallTopK // 3)
                for idx, score in rewriteBm25:
                    if idx in allCandidates:
                        allCandidates[idx]["bm25_score"] = max(
                            allCandidates[idx]["bm25_score"], score
                        )
                    else:
                        allCandidates[idx] = {"bm25_score": score, "vector_score": 0.0}

        print(f"âœ… å¬å› {len(allCandidates)} ä¸ªå€™é€‰æ–‡æ¡£")

        # 3. è®¡ç®—èåˆåˆ†æ•°
        if not allCandidates:
            return []

        # ä½¿ç”¨ç™¾åˆ†ä½æ•°å½’ä¸€åŒ–ï¼ˆæ›´é²æ£’ï¼Œä¸ Hybrid+ ä¸€è‡´ï¼‰
        def percentileNorm(scores: list[float]) -> list[float]:
            if not scores:
                return []
            sortedScores = sorted(scores)
            n = len(sortedScores)
            result = []
            for s in scores:
                rank = sum(1 for x in sortedScores if x <= s)
                result.append(rank / n)
            return result

        bm25Scores = [c["bm25_score"] for c in allCandidates.values()]
        vectorScores = [c["vector_score"] for c in allCandidates.values()]

        bm25NormScores = percentileNorm(bm25Scores)
        vectorNormScores = percentileNorm(vectorScores)

        # æ„å»º doc_id åˆ°å½’ä¸€åŒ–åˆ†æ•°çš„æ˜ å°„
        docIds = list(allCandidates.keys())
        bm25ScoreMap = {docIds[i]: bm25NormScores[i] for i in range(len(docIds))}
        vectorScoreMap = {docIds[i]: vectorNormScores[i] for i in range(len(docIds))}

        # è‡ªé€‚åº”æƒé‡è°ƒæ•´ï¼ˆä¸ Hybrid+ ä¸€è‡´ï¼‰
        import numpy as np

        avgBm25 = np.mean(bm25NormScores) if bm25NormScores else 0
        avgVector = np.mean(vectorNormScores) if vectorNormScores else 0
        total = avgBm25 + avgVector
        if total > 0:
            adaptiveAlpha = avgBm25 / total
            adaptiveBeta = avgVector / total
        else:
            adaptiveAlpha = adaptiveBeta = 0.5

        # ä½¿ç”¨è‡ªé€‚åº”æƒé‡è®¡ç®—èåˆåˆ†æ•°
        for idx, data in allCandidates.items():
            data["fused_score"] = (
                adaptiveAlpha * bm25ScoreMap[idx] + adaptiveBeta * vectorScoreMap[idx]
            )

        # 4. é‡æ’åº
        if useReranker and len(allCandidates) > 0:
            # å…ˆæŒ‰èåˆåˆ†æ•°æ’åºï¼Œå–å‰ 50 ä¸ªè¿›è¡Œé‡æ’åº
            sortedCandidates = sorted(
                allCandidates.items(),
                key=lambda x: x[1]["fused_score"],
                reverse=True,
            )[:50]

            candidates = [(idx, self._getDocText(idx)) for idx, _ in sortedCandidates]
            rerankScores = self._rerankScores(query, candidates)

            if rerankScores is not None:
                # é‡æ’åºæˆåŠŸ
                for (idx, _), score in zip(sortedCandidates, rerankScores):
                    allCandidates[idx]["reranker_score"] = score

                # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
                finalRanking = sorted(
                    allCandidates.items(),
                    key=lambda x: x[1].get("reranker_score", 0),
                    reverse=True,
                )
            else:
                # é‡æ’åºä¸å¯ç”¨ï¼ŒæŒ‰èåˆåˆ†æ•°æ’åº
                print("âš ï¸  é‡æ’åºä¸å¯ç”¨ï¼Œä½¿ç”¨èåˆåˆ†æ•°æ’åº")
                finalRanking = sorted(
                    allCandidates.items(),
                    key=lambda x: x[1]["fused_score"],
                    reverse=True,
                )
        else:
            # æŒ‰èåˆåˆ†æ•°æ’åº
            finalRanking = sorted(
                allCandidates.items(),
                key=lambda x: x[1]["fused_score"],
                reverse=True,
            )

        # 5. æ„å»ºç»“æœ
        results = []
        for rank, (idx, data) in enumerate(finalRanking[:topK], 1):
            doc = self._corpus[idx]
            results.append(
                {
                    "rank": rank,
                    "doc_id": doc["doc_id"],
                    "term": doc["term"],
                    "subject": doc.get("subject", ""),
                    "score": data.get("reranker_score", data["fused_score"]),
                    "bm25_score": data["bm25_score"],
                    "vector_score": data["vector_score"],
                    "source": doc.get("source", ""),
                    "page": doc.get("page", None),
                }
            )

        endTime = time.time()
        print(f"â±ï¸  æ£€ç´¢è€—æ—¶ï¼š{(endTime - startTime) * 1000:.2f}ms")

        return results

    def batchSearch(
        self,
        queries: list[str],
        topK: int = 10,
        **kwargs,
    ) -> dict[str, list[dict[str, Any]]]:
        """æ‰¹é‡æ£€ç´¢"""
        results = {}
        for query in queries:
            results[query] = self.search(query, topK, **kwargs)
        return results


def loadQueriesFromFile(filepath: str) -> list[str]:
    """ä»æ–‡ä»¶åŠ è½½æŸ¥è¯¢"""
    queries = []
    with open(filepath, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                queries.append(line)
    return queries


def saveResults(results: dict[str, list[dict[str, Any]]], outputFile: str) -> None:
    """ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶"""
    dirname = os.path.dirname(outputFile)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    with open(outputFile, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜ï¼š{outputFile}")


def printResults(query: str, results: list[dict[str, Any]]) -> None:
    """æ‰“å°æŸ¥è¯¢ç»“æœ"""
    print("\n" + "=" * 80)
    print(f"ğŸ” æŸ¥è¯¢ï¼š{query}")
    print("=" * 80)

    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        return

    for result in results:
        print(f"\nğŸ† Rank {result['rank']}")
        print(f"  ğŸ“„ Doc ID: {result['doc_id']}")
        print(f"  ğŸ“š æœ¯è¯­ï¼š{result['term']}")
        print(f"  ğŸ“– å­¦ç§‘ï¼š{result['subject']}")
        print(f"  ğŸ“Š åˆ†æ•°ï¼š{result['score']:.4f}")
        print(f"     â”œâ”€ BM25: {result.get('bm25_score', 0):.4f}")
        print(f"     â””â”€ å‘é‡ï¼š{result.get('vector_score', 0):.4f}")
        print(f"  ğŸ“— æ¥æºï¼š{result['source']}")
        if result.get("page"):
            print(f"  ğŸ“„ é¡µç ï¼š{result['page']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é«˜çº§æ£€ç´¢ - å¤šè·¯å¬å› + é‡æ’åº")
    parser.add_argument("--query", type=str, help="å•æ¬¡æŸ¥è¯¢å­—ç¬¦ä¸²")
    parser.add_argument("--query-file", type=str, help="æ‰¹é‡æŸ¥è¯¢æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--topk", type=int, default=10, help="è¿”å›çš„ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰"
    )
    parser.add_argument(
        "--recall-topk", type=int, default=100, help="å¬å›å€™é€‰æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰"
    )
    parser.add_argument("--output", type=str, help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--corpus", type=str, help="è¯­æ–™æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--bm25-index", type=str, help="BM25 ç´¢å¼•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--vector-index", type=str, help="å‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--vector-embedding", type=str, help="å‘é‡åµŒå…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--model",
        type=str,
        default="paraphrase-multilingual-MiniLM-L12-v2",
        help="Sentence Transformer æ¨¡å‹åç§°",
    )
    parser.add_argument(
        "--reranker-model",
        type=str,
        default="BAAI/bge-reranker-v2-mixed",
        help="é‡æ’åºæ¨¡å‹åç§°",
    )
    parser.add_argument("--terms", type=str, help="æœ¯è¯­æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--no-rerank", action="store_true", help="ç¦ç”¨é‡æ’åº")
    parser.add_argument("--no-rewrite", action="store_true", help="ç¦ç”¨æŸ¥è¯¢æ”¹å†™")
    parser.add_argument(
        "--bm25-weight", type=float, default=0.4, help="BM25 æƒé‡ï¼ˆé»˜è®¤ 0.4ï¼‰"
    )
    parser.add_argument(
        "--vector-weight", type=float, default=0.3, help="å‘é‡æƒé‡ï¼ˆé»˜è®¤ 0.3ï¼‰"
    )

    args = parser.parse_args()

    # é»˜è®¤è·¯å¾„
    corpusFile = args.corpus or os.path.join(
        config.PROCESSED_DIR, "retrieval", "corpus.jsonl"
    )
    bm25IndexFile = args.bm25_index or os.path.join(
        config.PROCESSED_DIR, "retrieval", "bm25_index.pkl"
    )
    vectorIndexFile = args.vector_index or os.path.join(
        config.PROCESSED_DIR, "retrieval", "vector_index.faiss"
    )
    vectorEmbeddingFile = args.vector_embedding or os.path.join(
        config.PROCESSED_DIR, "retrieval", "vector_embeddings.npz"
    )
    termsFile = args.terms or os.path.join(
        config.PROCESSED_DIR, "terms", "all_terms.json"
    )

    print("=" * 80)
    print("ğŸ” é«˜çº§æ£€ç´¢ - å¤šè·¯å¬å› + é‡æ’åº")
    print("=" * 80)
    print(f"ğŸ“‚ è¯­æ–™æ–‡ä»¶ï¼š{corpusFile}")
    print(f"ğŸ¤– æ£€ç´¢æ¨¡å‹ï¼š{args.model}")
    print(f"ğŸ¤– é‡æ’åºæ¨¡å‹ï¼š{args.reranker_model}")
    print(f"ğŸ”€ é‡æ’åºï¼š{'ç¦ç”¨' if args.no_rerank else 'å¯ç”¨'}")
    print(f"ğŸ”€ æŸ¥è¯¢æ”¹å†™ï¼š{'ç¦ç”¨' if args.no_rewrite else 'å¯ç”¨'}")
    print()

    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = AdvancedRetriever(
        corpusFile,
        bm25IndexFile,
        vectorIndexFile,
        vectorEmbeddingFile,
        args.model,
        args.reranker_model,
        termsFile,
    )

    # æ‰§è¡ŒæŸ¥è¯¢
    if args.query:
        results = retriever.search(
            args.query,
            args.topk,
            args.recall_topk,
            not args.no_rerank,
            not args.no_rewrite,
            args.bm25_weight,
            args.vector_weight,
        )
        printResults(args.query, results)

        if args.output:
            saveResults({args.query: results}, args.output)

    elif args.query_file:
        print(f"ğŸ“‚ åŠ è½½æŸ¥è¯¢ï¼š{args.query_file}")
        queries = loadQueriesFromFile(args.query_file)
        print(f"âœ… å·²åŠ è½½ {len(queries)} ä¸ªæŸ¥è¯¢\n")

        results = retriever.batchSearch(
            queries,
            args.topk,
            recallTopK=args.recall_topk,
            useReranker=not args.no_rerank,
            rewriteQuery=not args.no_rewrite,
            bm25Weight=args.bm25_weight,
            vectorWeight=args.vector_weight,
        )

        for query, queryResults in results.items():
            printResults(query, queryResults)

        if args.output:
            saveResults(results, args.output)
        else:
            defaultOutput = os.path.join(
                config.PROJECT_ROOT, "outputs", "advanced_results.json"
            )
            saveResults(results, defaultOutput)

    else:
        print("âš ï¸  è¯·æä¾›æŸ¥è¯¢å‚æ•°")
        parser.print_help()


if __name__ == "__main__":
    main()
