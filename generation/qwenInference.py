"""
Qwen2.5-Math æœ¬åœ°æ¨ç†å°è£…æ¨¡å—

åŠŸèƒ½ï¼š
1. ä»æœ¬åœ°è·¯å¾„åŠ è½½ Qwen2.5-Math-1.5B-Instruct æ¨¡å‹
2. å°è£… generate() æ¥å£ï¼Œæ”¯æŒå•æ¡å’Œæ‰¹é‡æ¨ç†
3. æ”¯æŒ GPU åŠ é€Ÿï¼ˆdevice_map=autoï¼‰å’Œ CPU fallback
4. æ¨ç†å‚æ•°ä» config.toml è¯»å–

ä½¿ç”¨æ–¹æ³•ï¼š
    from generation.qwenInference import QwenInference

    # åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼‰
    qwen = QwenInference()

    # å•æ¡æ¨ç†
    response = qwen.generate("ä»€ä¹ˆæ˜¯ä¸€è‡´æ”¶æ•›ï¼Ÿ")

    # æ‰¹é‡æ¨ç†
    responses = qwen.generateBatch(["é—®é¢˜1", "é—®é¢˜2"])

    # ä½¿ç”¨ messages æ ¼å¼ï¼ˆå…¼å®¹ promptTemplatesï¼‰
    messages = [
        {"role": "system", "content": "ä½ æ˜¯æ•°å­¦åŠ©æ‰‹"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æé™ï¼Ÿ"}
    ]
    response = qwen.generateFromMessages(messages)
"""

import os
import sys
from pathlib import Path

# è·¯å¾„è°ƒæ•´ï¼Œæ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—å¯¼å…¥ä¸¤ç§æ–¹å¼
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import config


class QwenInference:
    """
    Qwen2.5-Math æœ¬åœ°æ¨ç†å°è£…ç±»

    Attributes:
        modelDir: æ¨¡å‹ç›®å½•è·¯å¾„
        device: æ¨ç†è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        temperature: é‡‡æ ·æ¸©åº¦
        topP: top-p é‡‡æ ·å‚æ•°
        maxNewTokens: æœ€å¤§ç”Ÿæˆ token æ•°
    """

    def __init__(
        self,
        modelDir: str | None = None,
        temperature: float | None = None,
        topP: float | None = None,
        maxNewTokens: int | None = None,
        deviceMap: str = "auto",
    ):
        """
        åˆå§‹åŒ– Qwen æ¨ç†å®ä¾‹

        Args:
            modelDir: æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä» config.QWEN_MODEL_DIR è¯»å–
            temperature: é‡‡æ ·æ¸©åº¦ï¼Œé»˜è®¤ä» config.toml è¯»å–
            topP: top-p é‡‡æ ·å‚æ•°ï¼Œé»˜è®¤ä» config.toml è¯»å–
            maxNewTokens: æœ€å¤§ç”Ÿæˆ token æ•°ï¼Œé»˜è®¤ä» config.toml è¯»å–
            deviceMap: è®¾å¤‡æ˜ å°„ç­–ç•¥ï¼Œé»˜è®¤ "auto"ï¼ˆä¼˜å…ˆ GPUï¼‰
        """
        # åŠ è½½é…ç½®
        genCfg = config.getGenerationConfig()

        self.modelDir = modelDir or config.QWEN_MODEL_DIR
        self.temperature = (
            temperature if temperature is not None else genCfg["temperature"]
        )
        self.topP = topP if topP is not None else genCfg["top_p"]
        self.maxNewTokens = (
            maxNewTokens if maxNewTokens is not None else genCfg["max_new_tokens"]
        )
        self.deviceMap = deviceMap

        # å»¶è¿ŸåŠ è½½æ¨¡å‹å’Œ tokenizer
        self._model = None
        self._tokenizer = None
        self._device = None

    def _loadModel(self) -> None:
        """
        åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        """
        if self._model is not None:
            return

        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.modelDir}")

        if not os.path.isdir(self.modelDir):
            raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.modelDir}")

        # æ£€æµ‹è®¾å¤‡
        if torch.cuda.is_available():
            print("âœ… æ£€æµ‹åˆ° CUDAï¼Œä½¿ç”¨ GPU åŠ é€Ÿ")
            torchDtype = torch.float16
            deviceMap = self.deviceMap
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ° CUDAï¼Œä½¿ç”¨ CPU æ¨ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            torchDtype = torch.float32
            deviceMap = "cpu"

        # åŠ è½½ tokenizer
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.modelDir,
            trust_remote_code=True,
        )

        # åŠ è½½æ¨¡å‹
        self._model = AutoModelForCausalLM.from_pretrained(
            self.modelDir,
            dtype=torchDtype,
            device_map=deviceMap,
            trust_remote_code=True,
        )

        # è®°å½•å®é™…è®¾å¤‡
        if hasattr(self._model, "device"):
            self._device = str(self._model.device)
        else:
            self._device = "cuda" if torch.cuda.is_available() else "cpu"

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self._device}")

    @property
    def model(self):
        """è·å–æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        self._loadModel()
        return self._model

    @property
    def tokenizer(self):
        """è·å– tokenizer å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        self._loadModel()
        return self._tokenizer

    @property
    def device(self) -> str:
        """è·å–å½“å‰è®¾å¤‡"""
        self._loadModel()
        return self._device

    def generate(
        self,
        prompt: str,
        maxNewTokens: int | None = None,
        temperature: float | None = None,
        topP: float | None = None,
    ) -> str:
        """
        å•æ¡æ¨ç†

        Args:
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            maxNewTokens: æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            topP: top-p é‡‡æ ·å‚æ•°ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆä¸å«è¾“å…¥ promptï¼‰
        """
        import torch

        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self._loadModel()

        # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–é»˜è®¤å€¼
        maxNewTokens = maxNewTokens if maxNewTokens is not None else self.maxNewTokens
        temperature = temperature if temperature is not None else self.temperature
        topP = topP if topP is not None else self.topP

        # tokenize è¾“å…¥
        inputs = self._tokenizer(prompt, return_tensors="pt")

        # ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
        if torch.cuda.is_available():
            inputs = {k: v.to(self._model.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_new_tokens=maxNewTokens,
                temperature=temperature,
                top_p=topP,
                do_sample=temperature > 0,
                pad_token_id=self._tokenizer.eos_token_id,
            )

        # è§£ç è¾“å‡ºï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        inputLen = inputs["input_ids"].shape[1]
        generatedTokens = outputs[0][inputLen:]
        response = self._tokenizer.decode(generatedTokens, skip_special_tokens=True)

        return response.strip()

    def generateFromMessages(
        self,
        messages: list[dict[str, str]],
        maxNewTokens: int | None = None,
        temperature: float | None = None,
        topP: float | None = None,
    ) -> str:
        """
        ä» messages æ ¼å¼ç”Ÿæˆå›å¤ï¼ˆå…¼å®¹ OpenAI/HuggingFace Chat æ ¼å¼ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "system/user/assistant", "content": "..."}]
            maxNewTokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            topP: top-p é‡‡æ ·å‚æ•°

        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self._loadModel()

        # ä½¿ç”¨ tokenizer çš„ apply_chat_template æ–¹æ³•
        prompt = self._tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        return self.generate(
            prompt=prompt,
            maxNewTokens=maxNewTokens,
            temperature=temperature,
            topP=topP,
        )

    def generateBatch(
        self,
        prompts: list[str],
        maxNewTokens: int | None = None,
        temperature: float | None = None,
        topP: float | None = None,
    ) -> list[str]:
        """
        æ‰¹é‡æ¨ç†

        Args:
            prompts: è¾“å…¥æç¤ºæ–‡æœ¬åˆ—è¡¨
            maxNewTokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            topP: top-p é‡‡æ ·å‚æ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # ç®€å•å®ç°ï¼šé€æ¡æ¨ç†
        # åç»­å¯ä¼˜åŒ–ä¸ºçœŸæ­£çš„æ‰¹é‡æ¨ç†
        results = []
        for i, prompt in enumerate(prompts):
            print(f"ğŸ”„ æ¨ç†è¿›åº¦: {i + 1}/{len(prompts)}")
            response = self.generate(
                prompt=prompt,
                maxNewTokens=maxNewTokens,
                temperature=temperature,
                topP=topP,
            )
            results.append(response)
        return results


# ---- å‘½ä»¤è¡Œæµ‹è¯• ----


def _testInference() -> None:
    """
    æ¨¡å—çº§æµ‹è¯•ï¼šè¾“å…¥ä¸€æ¡æ•°å­¦é—®é¢˜ï¼Œè¾“å‡ºéç©ºå›ç­”
    """
    print("=" * 60)
    print("Qwen2.5-Math æ¨ç†æ¨¡å—æµ‹è¯•")
    print("=" * 60)

    # åˆå§‹åŒ–
    qwen = QwenInference()

    # æµ‹è¯•é—®é¢˜
    testQuery = "ä»€ä¹ˆæ˜¯æé™ï¼Ÿè¯·ç”¨æ•°å­¦è¯­è¨€ç»™å‡ºå®šä¹‰ã€‚"

    print(f"\næµ‹è¯•é—®é¢˜: {testQuery}")
    print("-" * 40)

    # å•æ¡æ¨ç†æµ‹è¯•
    response = qwen.generate(testQuery)

    print(f"\næ¨¡å‹å›å¤:\n{response}")
    print("-" * 40)

    # éªŒè¯è¾“å‡ºéç©º
    if response and len(response) > 0:
        print("âœ… æµ‹è¯•é€šè¿‡ï¼šè¾“å‡ºéç©º")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼šè¾“å‡ºä¸ºç©º")

    # æµ‹è¯• messages æ ¼å¼
    print("\n" + "=" * 60)
    print("æµ‹è¯• messages æ ¼å¼")
    print("=" * 60)

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°å­¦æ•™å­¦åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯å¯¼æ•°ï¼Ÿ"},
    ]

    response2 = qwen.generateFromMessages(messages)
    print(f"\næ¨¡å‹å›å¤:\n{response2}")

    if response2 and len(response2) > 0:
        print("âœ… messages æ ¼å¼æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ messages æ ¼å¼æµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    _testInference()
